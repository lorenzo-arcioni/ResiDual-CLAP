{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "189dca85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import from your codebase\n",
    "from CLAPWrapper import CLAPWrapper\n",
    "from datasets.esc50 import ESC50\n",
    "from datasets.tinysol import TinySOL\n",
    "\n",
    "DATASET = TinySOL\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e48fa266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Loading TinySOL Dataset\n",
      "================================================================================\n",
      "Dataset giÃ  presente in ../data/TinySOL\n",
      "Loading audio files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2913it [00:00, 20453.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset loaded: 2913 samples\n",
      "   Classes: 14 categories\n",
      "   Sample classes: ['Accordion', 'Bass Tuba', 'Bassoon', 'Clarinet Bb', 'Contrabass']\n",
      "\n",
      "ðŸ“ Text prompts: 14 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Loading {DATASET.__name__} Dataset\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "root_path = \"../data\"\n",
    "dataset = DATASET(root=root_path, download=False)\n",
    "print(f\"âœ… Dataset loaded: {len(dataset)} samples\")\n",
    "print(f\"   Classes: {len(dataset.classes)} categories\")\n",
    "print(f\"   Sample classes: {dataset.classes[:5]}\")\n",
    "\n",
    "# Prepare text prompts\n",
    "prompt = 'this is the sound of '\n",
    "text_labels = [prompt + x for x in dataset.classes]\n",
    "print(f\"\\nðŸ“ Text prompts: {len(text_labels)} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebc7cf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Initializing Models\n",
      "================================================================================\n",
      "\n",
      "ðŸ”§ Loading CLAP Standard...OK\n",
      "\n",
      "ðŸ”§ Loading ResiDualCLAP...\n",
      "\n",
      "================================================================================\n",
      "ðŸ”§ Setup ResiDual HTSAT\n",
      "================================================================================\n",
      "ModalitÃ : ATTENTION\n",
      "Target layers: [0, 1, 2, 3]\n",
      "PCA components ratio: 1.0\n",
      "Reweight factor: 0.0\n",
      "\n",
      "âœ“ layer_0:\n",
      "  ModalitÃ : PER-HEAD reweighting\n",
      "  Num blocks: 2\n",
      "  Heads per block: 4\n",
      "  Total heads: 8\n",
      "  Head dim: 24D â†’ 24 PCs\n",
      "\n",
      "âœ“ layer_1:\n",
      "  ModalitÃ : PER-HEAD reweighting\n",
      "  Num blocks: 2\n",
      "  Heads per block: 8\n",
      "  Total heads: 16\n",
      "  Head dim: 24D â†’ 24 PCs\n",
      "\n",
      "âœ“ layer_2:\n",
      "  ModalitÃ : PER-HEAD reweighting\n",
      "  Num blocks: 6\n",
      "  Heads per block: 16\n",
      "  Total heads: 96\n",
      "  Head dim: 24D â†’ 24 PCs\n",
      "\n",
      "âœ“ layer_3:\n",
      "  ModalitÃ : PER-HEAD reweighting\n",
      "  Num blocks: 2\n",
      "  Heads per block: 32\n",
      "  Total heads: 64\n",
      "  Head dim: 24D â†’ 24 PCs\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResiDualCLAP(\n",
       "  (audio_encoder): AudioEncoder(\n",
       "    (base): ResiDualHTSATWrapper(\n",
       "      (htsat): ResiDualHTSAT(\n",
       "        (spectrogram_extractor): Spectrogram(\n",
       "          (stft): STFT(\n",
       "            (conv_real): Conv1d(1, 513, kernel_size=(1024,), stride=(320,), bias=False)\n",
       "            (conv_imag): Conv1d(1, 513, kernel_size=(1024,), stride=(320,), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (logmel_extractor): LogmelFilterBank()\n",
       "        (spec_augmenter): SpecAugmentation(\n",
       "          (time_dropper): DropStripes()\n",
       "          (freq_dropper): DropStripes()\n",
       "        )\n",
       "        (bn0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (patch_embed): PatchEmbed(\n",
       "          (proj): Conv2d(1, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "          (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0): BasicLayer(\n",
       "            dim=96, input_resolution=(64, 64), depth=2\n",
       "            (blocks): ModuleList(\n",
       "              (0): SwinTransformerBlock(\n",
       "                dim=96, input_resolution=(64, 64), num_heads=4, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttentionReweighting(\n",
       "                  dim=96, window_size=(8, 8), num_heads=4\n",
       "                  (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                  (spectral_layers): ModuleList(\n",
       "                    (0-3): 4 x SpectralReweightingLayer()\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): Identity()\n",
       "                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (1): SwinTransformerBlock(\n",
       "                dim=96, input_resolution=(64, 64), num_heads=4, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "                (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttentionReweighting(\n",
       "                  dim=96, window_size=(8, 8), num_heads=4\n",
       "                  (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                  (spectral_layers): ModuleList(\n",
       "                    (0-3): 4 x SpectralReweightingLayer()\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): DropPath()\n",
       "                (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (downsample): PatchMerging(\n",
       "              input_resolution=(64, 64), dim=96\n",
       "              (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (1): BasicLayer(\n",
       "            dim=192, input_resolution=(32, 32), depth=2\n",
       "            (blocks): ModuleList(\n",
       "              (0): SwinTransformerBlock(\n",
       "                dim=192, input_resolution=(32, 32), num_heads=8, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "                (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttentionReweighting(\n",
       "                  dim=192, window_size=(8, 8), num_heads=8\n",
       "                  (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                  (spectral_layers): ModuleList(\n",
       "                    (0-7): 8 x SpectralReweightingLayer()\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): DropPath()\n",
       "                (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (1): SwinTransformerBlock(\n",
       "                dim=192, input_resolution=(32, 32), num_heads=8, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "                (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttentionReweighting(\n",
       "                  dim=192, window_size=(8, 8), num_heads=8\n",
       "                  (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                  (spectral_layers): ModuleList(\n",
       "                    (0-7): 8 x SpectralReweightingLayer()\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): DropPath()\n",
       "                (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (downsample): PatchMerging(\n",
       "              input_resolution=(32, 32), dim=192\n",
       "              (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (2): BasicLayer(\n",
       "            dim=384, input_resolution=(16, 16), depth=6\n",
       "            (blocks): ModuleList(\n",
       "              (0): SwinTransformerBlock(\n",
       "                dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "                (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttentionReweighting(\n",
       "                  dim=384, window_size=(8, 8), num_heads=16\n",
       "                  (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                  (spectral_layers): ModuleList(\n",
       "                    (0-15): 16 x SpectralReweightingLayer()\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): DropPath()\n",
       "                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (1): SwinTransformerBlock(\n",
       "                dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "                (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttentionReweighting(\n",
       "                  dim=384, window_size=(8, 8), num_heads=16\n",
       "                  (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                  (spectral_layers): ModuleList(\n",
       "                    (0-15): 16 x SpectralReweightingLayer()\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): DropPath()\n",
       "                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (2): SwinTransformerBlock(\n",
       "                dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "                (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttentionReweighting(\n",
       "                  dim=384, window_size=(8, 8), num_heads=16\n",
       "                  (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                  (spectral_layers): ModuleList(\n",
       "                    (0-15): 16 x SpectralReweightingLayer()\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): DropPath()\n",
       "                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (3): SwinTransformerBlock(\n",
       "                dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "                (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttentionReweighting(\n",
       "                  dim=384, window_size=(8, 8), num_heads=16\n",
       "                  (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                  (spectral_layers): ModuleList(\n",
       "                    (0-15): 16 x SpectralReweightingLayer()\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): DropPath()\n",
       "                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (4): SwinTransformerBlock(\n",
       "                dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "                (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttentionReweighting(\n",
       "                  dim=384, window_size=(8, 8), num_heads=16\n",
       "                  (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                  (spectral_layers): ModuleList(\n",
       "                    (0-15): 16 x SpectralReweightingLayer()\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): DropPath()\n",
       "                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (5): SwinTransformerBlock(\n",
       "                dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "                (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttentionReweighting(\n",
       "                  dim=384, window_size=(8, 8), num_heads=16\n",
       "                  (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                  (spectral_layers): ModuleList(\n",
       "                    (0-15): 16 x SpectralReweightingLayer()\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): DropPath()\n",
       "                (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (downsample): PatchMerging(\n",
       "              input_resolution=(16, 16), dim=384\n",
       "              (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "              (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (3): BasicLayer(\n",
       "            dim=768, input_resolution=(8, 8), depth=2\n",
       "            (blocks): ModuleList(\n",
       "              (0-1): 2 x SwinTransformerBlock(\n",
       "                dim=768, input_resolution=(8, 8), num_heads=32, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "                (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn): WindowAttentionReweighting(\n",
       "                  dim=768, window_size=(8, 8), num_heads=32\n",
       "                  (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                  (softmax): Softmax(dim=-1)\n",
       "                  (spectral_layers): ModuleList(\n",
       "                    (0-31): 32 x SpectralReweightingLayer()\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): DropPath()\n",
       "                (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): Mlp(\n",
       "                  (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (drop): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "        (maxpool): AdaptiveMaxPool1d(output_size=1)\n",
       "        (tscam_conv): Conv2d(768, 527, kernel_size=(2, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (head): Linear(in_features=527, out_features=527, bias=True)\n",
       "        (spectral_layers): ModuleDict(\n",
       "          (layer_0): ModuleList(\n",
       "            (0-1): 2 x ModuleList(\n",
       "              (0-3): 4 x SpectralReweightingLayer()\n",
       "            )\n",
       "          )\n",
       "          (layer_1): ModuleList(\n",
       "            (0-1): 2 x ModuleList(\n",
       "              (0-7): 8 x SpectralReweightingLayer()\n",
       "            )\n",
       "          )\n",
       "          (layer_2): ModuleList(\n",
       "            (0-5): 6 x ModuleList(\n",
       "              (0-15): 16 x SpectralReweightingLayer()\n",
       "            )\n",
       "          )\n",
       "          (layer_3): ModuleList(\n",
       "            (0-1): 2 x ModuleList(\n",
       "              (0-31): 32 x SpectralReweightingLayer()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (projection): Projection(\n",
       "      (linear1): Linear(in_features=768, out_features=1024, bias=False)\n",
       "      (linear2): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (caption_encoder): TextEncoder(\n",
       "    (base): GPT2Model(\n",
       "      (wte): Embedding(50257, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-11): 12 x GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D(nf=2304, nx=768)\n",
       "            (c_proj): Conv1D(nf=768, nx=768)\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D(nf=3072, nx=768)\n",
       "            (c_proj): Conv1D(nf=768, nx=3072)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (projection): Projection(\n",
       "      (linear1): Linear(in_features=768, out_features=1024, bias=False)\n",
       "      (linear2): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: Initialize Models\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Initializing Models\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Residual config con pc_weights = 1.0 (identitÃ )\n",
    "residual_config = {\n",
    "    'mode': 'attention',\n",
    "    'n_components_ratio': 1.0,\n",
    "    'reweight_factor': 0.0,\n",
    "    'target_layers': [0, 1, 2, 3],  # Layers dove applicare reweighting\n",
    "}\n",
    "\n",
    "print(\"\\nðŸ”§ Loading CLAP Standard...\", end='')\n",
    "clap_standard = CLAPWrapper(\n",
    "    version='2023',  # or '2022'\n",
    "    use_cuda=torch.cuda.is_available(),\n",
    "    type='classic'\n",
    ")\n",
    "clap_standard.clap.eval()\n",
    "print(\"OK\")\n",
    "\n",
    "print(\"\\nðŸ”§ Loading ResiDualCLAP...\")\n",
    "clap_residual = CLAPWrapper(\n",
    "    version='2023',\n",
    "    use_cuda=torch.cuda.is_available(),\n",
    "    type='residual',\n",
    "    residual_config=residual_config\n",
    ")\n",
    "clap_residual.clap.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e07ed051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting samples for PCA fitting (200 samples)...OK\n"
     ]
    }
   ],
   "source": [
    "PCA_SAMPLES = 200\n",
    "\n",
    "# Prepare audio samples for PCA fitting\n",
    "print(f\"Collecting samples for PCA fitting ({PCA_SAMPLES} samples)...\", end='')\n",
    "\n",
    "# Create a simple dataloader wrapper per PCA fitting\n",
    "class SimpleAudioDataset:\n",
    "    def __init__(self, wrapper, esc50_dataset, max_samples=1000):\n",
    "        self.wrapper = wrapper\n",
    "        self.audio_paths = []\n",
    "        for i in range(min(max_samples, len(esc50_dataset))):\n",
    "            audio_path, _, _ = esc50_dataset[i]\n",
    "            self.audio_paths.append(audio_path)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_tensor = self.wrapper.load_audio_into_tensor(\n",
    "            self.audio_paths[idx],\n",
    "            self.wrapper.args.duration,\n",
    "            resample=True\n",
    "        )\n",
    "        # âœ… Assicurati sia 1D\n",
    "        if audio_tensor.dim() > 1:\n",
    "            audio_tensor = audio_tensor.squeeze()\n",
    "        \n",
    "        return audio_tensor\n",
    "\n",
    "# Create dataset and loader\n",
    "pca_dataset = SimpleAudioDataset(clap_residual, dataset, max_samples=PCA_SAMPLES)\n",
    "pca_loader = DataLoader(\n",
    "    pca_dataset, \n",
    "    batch_size=25, \n",
    "    shuffle=False,\n",
    "    num_workers=0,  # Start with 0 for debugging\n",
    "    pin_memory=False\n",
    ")\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4e2ba8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([308700])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea6b11aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpca_loader\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m].shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documenti/GitHub/ResiDual-CLAP/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documenti/GitHub/ResiDual-CLAP/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documenti/GitHub/ResiDual-CLAP/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documenti/GitHub/ResiDual-CLAP/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mSimpleAudioDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     audio_tensor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_audio_into_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maudio_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     \u001b[38;5;66;03m# âœ… Assicurati sia 1D\u001b[39;00m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m audio_tensor.dim() > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documenti/GitHub/ResiDual-CLAP/src/CLAPWrapper.py:296\u001b[39m, in \u001b[36mCLAPWrapper.load_audio_into_tensor\u001b[39m\u001b[34m(self, audio_path, audio_duration, resample)\u001b[39m\n\u001b[32m    294\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Loads audio file and returns raw audio.\"\"\"\u001b[39;00m\n\u001b[32m    295\u001b[39m \u001b[38;5;66;03m# Randomly sample a segment of audio_duration from the clip or pad to match duration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m audio_time_series, sample_rate = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    297\u001b[39m audio_time_series = audio_time_series.reshape(-\u001b[32m1\u001b[39m)\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# audio_time_series is shorter than predefined audio duration,\u001b[39;00m\n\u001b[32m    300\u001b[39m \u001b[38;5;66;03m# so audio_time_series is extended\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documenti/GitHub/ResiDual-CLAP/src/CLAPWrapper.py:285\u001b[39m, in \u001b[36mCLAPWrapper.read_audio\u001b[39m\u001b[34m(self, audio_path, resample)\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Loads audio file or array and returns a torch tensor\"\"\"\u001b[39;00m\n\u001b[32m    284\u001b[39m \u001b[38;5;66;03m# Randomly sample a segment of audio_duration from the clip or pad to match duration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m audio_time_series, sample_rate = \u001b[43mtorchaudio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m resample_rate = \u001b[38;5;28mself\u001b[39m.args.sampling_rate\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resample \u001b[38;5;129;01mand\u001b[39;00m resample_rate != sample_rate:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documenti/GitHub/ResiDual-CLAP/.venv/lib/python3.11/site-packages/torchaudio/__init__.py:86\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m     19\u001b[39m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m     20\u001b[39m     frame_offset: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     backend: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     27\u001b[39m ) -> Tuple[torch.Tensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m     28\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load audio data from source using TorchCodec's AudioDecoder.\u001b[39;00m\n\u001b[32m     29\u001b[39m \n\u001b[32m     30\u001b[39m \u001b[33;03m    .. note::\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     84\u001b[39m \u001b[33;03m        by TorchCodec.\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_with_torchcodec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documenti/GitHub/ResiDual-CLAP/.venv/lib/python3.11/site-packages/torchaudio/_torchcodec.py:128\u001b[39m, in \u001b[36mload_with_torchcodec\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# Decode the entire file first, then subsample manually\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# This is the simplest approach since torchcodec uses time-based indexing\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     audio_samples = \u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_all_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to decode audio samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documenti/GitHub/ResiDual-CLAP/.venv/lib/python3.11/site-packages/torchcodec/decoders/_audio_decoder.py:100\u001b[39m, in \u001b[36mAudioDecoder.get_all_samples\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_all_samples\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> AudioSamples:\n\u001b[32m     92\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns all the audio samples from the source.\u001b[39;00m\n\u001b[32m     93\u001b[39m \n\u001b[32m     94\u001b[39m \u001b[33;03m    To decode samples in a specific range, use\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     98\u001b[39m \u001b[33;03m        AudioSamples: The samples within the file.\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_samples_played_in_range\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documenti/GitHub/ResiDual-CLAP/.venv/lib/python3.11/site-packages/torchcodec/decoders/_audio_decoder.py:129\u001b[39m, in \u001b[36mAudioDecoder.get_samples_played_in_range\u001b[39m\u001b[34m(self, start_seconds, stop_seconds)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stop_seconds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m start_seconds <= stop_seconds:\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    127\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid start seconds: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_seconds\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. It must be less than or equal to stop seconds (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstop_seconds\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m frames, first_pts = \u001b[43mcore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_frames_by_pts_in_range_audio\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_seconds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstop_seconds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m first_pts = first_pts.item()\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# x = frame boundaries\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[38;5;66;03m#            first_pts                                    last_pts\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    147\u001b[39m \u001b[38;5;66;03m# So we do some basic math to figure out the position of the view that\u001b[39;00m\n\u001b[32m    148\u001b[39m \u001b[38;5;66;03m# we'll return.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documenti/GitHub/ResiDual-CLAP/.venv/lib/python3.11/site-packages/torch/_ops.py:841\u001b[39m, in \u001b[36mOpOverload.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, /, *args: _P.args, **kwargs: _P.kwargs) -> _T:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "list(pca_loader)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ef5dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2913"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f5a8ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Fitting PCA Components\n",
      "================================================================================\n",
      "Fitting PCA on 200 samples...\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š Fitting Spectral Layers\n",
      "================================================================================\n",
      "ModalitÃ : ATTENTION\n",
      "Target layers: [0, 1, 2, 3]\n",
      "Max samples: 200\n",
      "================================================================================\n",
      "\n",
      "ðŸ“¦ Fase 1: Raccolta hidden states...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Raccolta: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:32<00:00,  4.08s/it, samples=200]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Raccolti 200 samples totali\n",
      "\n",
      "================================================================================\n",
      "ðŸ”¬ Fase 2: Calcolo PCA e inizializzazione pesi...\n",
      "================================================================================\n",
      "\n",
      "\n",
      "layer_0:\n",
      "\n",
      "  Block 0:\n",
      "    âœ“ head_0: shape=torch.Size([819200, 24]), var=['0.879', '0.061', '0.051'], cum=0.991\n",
      "    âœ“ head_1: shape=torch.Size([819200, 24]), var=['0.782', '0.090', '0.076'], cum=0.948\n",
      "    âœ“ head_2: shape=torch.Size([819200, 24]), var=['0.883', '0.072', '0.035'], cum=0.989\n",
      "    âœ“ head_3: shape=torch.Size([819200, 24]), var=['0.836', '0.070', '0.037'], cum=0.943\n",
      "\n",
      "  Block 1:\n",
      "    âœ“ head_0: shape=torch.Size([819200, 24]), var=['0.345', '0.237', '0.149'], cum=0.730\n",
      "    âœ“ head_1: shape=torch.Size([819200, 24]), var=['0.750', '0.115', '0.042'], cum=0.907\n",
      "    âœ“ head_2: shape=torch.Size([819200, 24]), var=['0.620', '0.139', '0.116'], cum=0.875\n",
      "    âœ“ head_3: shape=torch.Size([819200, 24]), var=['0.538', '0.225', '0.105'], cum=0.868\n",
      "\n",
      "layer_1:\n",
      "\n",
      "  Block 0:\n",
      "    âœ“ head_0: shape=torch.Size([204800, 24]), var=['0.434', '0.158', '0.113'], cum=0.706\n",
      "    âœ“ head_1: shape=torch.Size([204800, 24]), var=['0.241', '0.158', '0.117'], cum=0.515\n",
      "    âœ“ head_2: shape=torch.Size([204800, 24]), var=['0.243', '0.146', '0.121'], cum=0.511\n",
      "    âœ“ head_3: shape=torch.Size([204800, 24]), var=['0.620', '0.144', '0.089'], cum=0.854\n",
      "    âœ“ head_4: shape=torch.Size([204800, 24]), var=['0.576', '0.113', '0.062'], cum=0.751\n",
      "    âœ“ head_5: shape=torch.Size([204800, 24]), var=['0.254', '0.140', '0.112'], cum=0.506\n",
      "    âœ“ head_6: shape=torch.Size([204800, 24]), var=['0.495', '0.246', '0.119'], cum=0.860\n",
      "    âœ“ head_7: shape=torch.Size([204800, 24]), var=['0.279', '0.186', '0.091'], cum=0.557\n",
      "\n",
      "  Block 1:\n",
      "    âœ“ head_0: shape=torch.Size([204800, 24]), var=['0.374', '0.278', '0.082'], cum=0.735\n",
      "    âœ“ head_1: shape=torch.Size([204800, 24]), var=['0.275', '0.225', '0.108'], cum=0.608\n",
      "    âœ“ head_2: shape=torch.Size([204800, 24]), var=['0.268', '0.154', '0.141'], cum=0.564\n",
      "    âœ“ head_3: shape=torch.Size([204800, 24]), var=['0.333', '0.198', '0.131'], cum=0.662\n",
      "    âœ“ head_4: shape=torch.Size([204800, 24]), var=['0.461', '0.152', '0.076'], cum=0.690\n",
      "    âœ“ head_5: shape=torch.Size([204800, 24]), var=['0.251', '0.165', '0.128'], cum=0.544\n",
      "    âœ“ head_6: shape=torch.Size([204800, 24]), var=['0.391', '0.188', '0.121'], cum=0.700\n",
      "    âœ“ head_7: shape=torch.Size([204800, 24]), var=['0.275', '0.250', '0.141'], cum=0.666\n",
      "\n",
      "layer_2:\n",
      "\n",
      "  Block 0:\n",
      "    âœ“ head_0: shape=torch.Size([51200, 24]), var=['0.338', '0.208', '0.110'], cum=0.655\n",
      "    âœ“ head_1: shape=torch.Size([51200, 24]), var=['0.368', '0.145', '0.124'], cum=0.637\n",
      "    âœ“ head_2: shape=torch.Size([51200, 24]), var=['0.383', '0.156', '0.071'], cum=0.611\n",
      "    âœ“ head_3: shape=torch.Size([51200, 24]), var=['0.401', '0.160', '0.143'], cum=0.704\n",
      "    âœ“ head_4: shape=torch.Size([51200, 24]), var=['0.292', '0.230', '0.170'], cum=0.692\n",
      "    âœ“ head_5: shape=torch.Size([51200, 24]), var=['0.391', '0.229', '0.156'], cum=0.775\n",
      "    âœ“ head_6: shape=torch.Size([51200, 24]), var=['0.379', '0.243', '0.128'], cum=0.750\n",
      "    âœ“ head_7: shape=torch.Size([51200, 24]), var=['0.444', '0.267', '0.078'], cum=0.790\n",
      "    âœ“ head_8: shape=torch.Size([51200, 24]), var=['0.295', '0.151', '0.130'], cum=0.576\n",
      "    âœ“ head_9: shape=torch.Size([51200, 24]), var=['0.306', '0.147', '0.118'], cum=0.570\n",
      "    âœ“ head_10: shape=torch.Size([51200, 24]), var=['0.338', '0.190', '0.152'], cum=0.680\n",
      "    âœ“ head_11: shape=torch.Size([51200, 24]), var=['0.406', '0.178', '0.137'], cum=0.720\n",
      "    âœ“ head_12: shape=torch.Size([51200, 24]), var=['0.374', '0.178', '0.143'], cum=0.695\n",
      "    âœ“ head_13: shape=torch.Size([51200, 24]), var=['0.282', '0.238', '0.130'], cum=0.650\n",
      "    âœ“ head_14: shape=torch.Size([51200, 24]), var=['0.358', '0.142', '0.091'], cum=0.591\n",
      "    âœ“ head_15: shape=torch.Size([51200, 24]), var=['0.429', '0.140', '0.097'], cum=0.666\n",
      "\n",
      "  Block 1:\n",
      "    âœ“ head_0: shape=torch.Size([51200, 24]), var=['0.327', '0.193', '0.116'], cum=0.636\n",
      "    âœ“ head_1: shape=torch.Size([51200, 24]), var=['0.328', '0.208', '0.083'], cum=0.619\n",
      "    âœ“ head_2: shape=torch.Size([51200, 24]), var=['0.178', '0.150', '0.106'], cum=0.434\n",
      "    âœ“ head_3: shape=torch.Size([51200, 24]), var=['0.258', '0.175', '0.133'], cum=0.566\n",
      "    âœ“ head_4: shape=torch.Size([51200, 24]), var=['0.357', '0.163', '0.123'], cum=0.643\n",
      "    âœ“ head_5: shape=torch.Size([51200, 24]), var=['0.316', '0.156', '0.141'], cum=0.613\n",
      "    âœ“ head_6: shape=torch.Size([51200, 24]), var=['0.269', '0.220', '0.111'], cum=0.599\n",
      "    âœ“ head_7: shape=torch.Size([51200, 24]), var=['0.372', '0.143', '0.085'], cum=0.600\n",
      "    âœ“ head_8: shape=torch.Size([51200, 24]), var=['0.240', '0.131', '0.100'], cum=0.470\n",
      "    âœ“ head_9: shape=torch.Size([51200, 24]), var=['0.462', '0.147', '0.090'], cum=0.699\n",
      "    âœ“ head_10: shape=torch.Size([51200, 24]), var=['0.278', '0.154', '0.123'], cum=0.555\n",
      "    âœ“ head_11: shape=torch.Size([51200, 24]), var=['0.258', '0.206', '0.148'], cum=0.612\n",
      "    âœ“ head_12: shape=torch.Size([51200, 24]), var=['0.275', '0.214', '0.143'], cum=0.631\n",
      "    âœ“ head_13: shape=torch.Size([51200, 24]), var=['0.322', '0.191', '0.101'], cum=0.614\n",
      "    âœ“ head_14: shape=torch.Size([51200, 24]), var=['0.286', '0.185', '0.141'], cum=0.612\n",
      "    âœ“ head_15: shape=torch.Size([51200, 24]), var=['0.270', '0.255', '0.154'], cum=0.679\n",
      "\n",
      "  Block 2:\n",
      "    âœ“ head_0: shape=torch.Size([51200, 24]), var=['0.298', '0.207', '0.121'], cum=0.626\n",
      "    âœ“ head_1: shape=torch.Size([51200, 24]), var=['0.297', '0.182', '0.110'], cum=0.589\n",
      "    âœ“ head_2: shape=torch.Size([51200, 24]), var=['0.326', '0.111', '0.096'], cum=0.533\n",
      "    âœ“ head_3: shape=torch.Size([51200, 24]), var=['0.334', '0.175', '0.089'], cum=0.599\n",
      "    âœ“ head_4: shape=torch.Size([51200, 24]), var=['0.247', '0.133', '0.105'], cum=0.485\n",
      "    âœ“ head_5: shape=torch.Size([51200, 24]), var=['0.307', '0.242', '0.117'], cum=0.666\n",
      "    âœ“ head_6: shape=torch.Size([51200, 24]), var=['0.338', '0.307', '0.087'], cum=0.732\n",
      "    âœ“ head_7: shape=torch.Size([51200, 24]), var=['0.324', '0.126', '0.120'], cum=0.570\n",
      "    âœ“ head_8: shape=torch.Size([51200, 24]), var=['0.301', '0.136', '0.106'], cum=0.543\n",
      "    âœ“ head_9: shape=torch.Size([51200, 24]), var=['0.236', '0.166', '0.115'], cum=0.517\n",
      "    âœ“ head_10: shape=torch.Size([51200, 24]), var=['0.332', '0.187', '0.103'], cum=0.622\n",
      "    âœ“ head_11: shape=torch.Size([51200, 24]), var=['0.294', '0.206', '0.138'], cum=0.637\n",
      "    âœ“ head_12: shape=torch.Size([51200, 24]), var=['0.360', '0.135', '0.125'], cum=0.619\n",
      "    âœ“ head_13: shape=torch.Size([51200, 24]), var=['0.333', '0.125', '0.112'], cum=0.570\n",
      "    âœ“ head_14: shape=torch.Size([51200, 24]), var=['0.320', '0.155', '0.115'], cum=0.590\n",
      "    âœ“ head_15: shape=torch.Size([51200, 24]), var=['0.317', '0.140', '0.095'], cum=0.552\n",
      "\n",
      "  Block 3:\n",
      "    âœ“ head_0: shape=torch.Size([51200, 24]), var=['0.360', '0.228', '0.104'], cum=0.692\n",
      "    âœ“ head_1: shape=torch.Size([51200, 24]), var=['0.253', '0.149', '0.111'], cum=0.512\n",
      "    âœ“ head_2: shape=torch.Size([51200, 24]), var=['0.271', '0.174', '0.129'], cum=0.573\n",
      "    âœ“ head_3: shape=torch.Size([51200, 24]), var=['0.258', '0.153', '0.115'], cum=0.527\n",
      "    âœ“ head_4: shape=torch.Size([51200, 24]), var=['0.212', '0.122', '0.101'], cum=0.436\n",
      "    âœ“ head_5: shape=torch.Size([51200, 24]), var=['0.289', '0.110', '0.101'], cum=0.501\n",
      "    âœ“ head_6: shape=torch.Size([51200, 24]), var=['0.225', '0.191', '0.152'], cum=0.568\n",
      "    âœ“ head_7: shape=torch.Size([51200, 24]), var=['0.204', '0.183', '0.158'], cum=0.544\n",
      "    âœ“ head_8: shape=torch.Size([51200, 24]), var=['0.241', '0.144', '0.122'], cum=0.508\n",
      "    âœ“ head_9: shape=torch.Size([51200, 24]), var=['0.332', '0.111', '0.096'], cum=0.539\n",
      "    âœ“ head_10: shape=torch.Size([51200, 24]), var=['0.197', '0.151', '0.114'], cum=0.462\n",
      "    âœ“ head_11: shape=torch.Size([51200, 24]), var=['0.224', '0.181', '0.118'], cum=0.523\n",
      "    âœ“ head_12: shape=torch.Size([51200, 24]), var=['0.350', '0.118', '0.083'], cum=0.552\n",
      "    âœ“ head_13: shape=torch.Size([51200, 24]), var=['0.190', '0.136', '0.112'], cum=0.437\n",
      "    âœ“ head_14: shape=torch.Size([51200, 24]), var=['0.278', '0.153', '0.103'], cum=0.534\n",
      "    âœ“ head_15: shape=torch.Size([51200, 24]), var=['0.171', '0.123', '0.102'], cum=0.395\n",
      "\n",
      "  Block 4:\n",
      "    âœ“ head_0: shape=torch.Size([51200, 24]), var=['0.286', '0.169', '0.120'], cum=0.575\n",
      "    âœ“ head_1: shape=torch.Size([51200, 24]), var=['0.214', '0.164', '0.119'], cum=0.497\n",
      "    âœ“ head_2: shape=torch.Size([51200, 24]), var=['0.339', '0.140', '0.088'], cum=0.568\n",
      "    âœ“ head_3: shape=torch.Size([51200, 24]), var=['0.249', '0.110', '0.104'], cum=0.463\n",
      "    âœ“ head_4: shape=torch.Size([51200, 24]), var=['0.198', '0.144', '0.097'], cum=0.438\n",
      "    âœ“ head_5: shape=torch.Size([51200, 24]), var=['0.210', '0.124', '0.098'], cum=0.432\n",
      "    âœ“ head_6: shape=torch.Size([51200, 24]), var=['0.245', '0.176', '0.112'], cum=0.534\n",
      "    âœ“ head_7: shape=torch.Size([51200, 24]), var=['0.209', '0.181', '0.077'], cum=0.468\n",
      "    âœ“ head_8: shape=torch.Size([51200, 24]), var=['0.234', '0.149', '0.125'], cum=0.508\n",
      "    âœ“ head_9: shape=torch.Size([51200, 24]), var=['0.229', '0.152', '0.112'], cum=0.493\n",
      "    âœ“ head_10: shape=torch.Size([51200, 24]), var=['0.368', '0.114', '0.094'], cum=0.575\n",
      "    âœ“ head_11: shape=torch.Size([51200, 24]), var=['0.305', '0.161', '0.100'], cum=0.566\n",
      "    âœ“ head_12: shape=torch.Size([51200, 24]), var=['0.274', '0.114', '0.105'], cum=0.493\n",
      "    âœ“ head_13: shape=torch.Size([51200, 24]), var=['0.330', '0.148', '0.086'], cum=0.564\n",
      "    âœ“ head_14: shape=torch.Size([51200, 24]), var=['0.210', '0.152', '0.119'], cum=0.481\n",
      "    âœ“ head_15: shape=torch.Size([51200, 24]), var=['0.233', '0.162', '0.092'], cum=0.487\n",
      "\n",
      "  Block 5:\n",
      "    âœ“ head_0: shape=torch.Size([51200, 24]), var=['0.175', '0.136', '0.095'], cum=0.406\n",
      "    âœ“ head_1: shape=torch.Size([51200, 24]), var=['0.199', '0.137', '0.111'], cum=0.447\n",
      "    âœ“ head_2: shape=torch.Size([51200, 24]), var=['0.158', '0.145', '0.112'], cum=0.414\n",
      "    âœ“ head_3: shape=torch.Size([51200, 24]), var=['0.274', '0.161', '0.136'], cum=0.571\n",
      "    âœ“ head_4: shape=torch.Size([51200, 24]), var=['0.265', '0.157', '0.112'], cum=0.535\n",
      "    âœ“ head_5: shape=torch.Size([51200, 24]), var=['0.261', '0.137', '0.103'], cum=0.502\n",
      "    âœ“ head_6: shape=torch.Size([51200, 24]), var=['0.296', '0.139', '0.097'], cum=0.532\n",
      "    âœ“ head_7: shape=torch.Size([51200, 24]), var=['0.193', '0.136', '0.114'], cum=0.442\n",
      "    âœ“ head_8: shape=torch.Size([51200, 24]), var=['0.336', '0.094', '0.091'], cum=0.522\n",
      "    âœ“ head_9: shape=torch.Size([51200, 24]), var=['0.227', '0.192', '0.105'], cum=0.524\n",
      "    âœ“ head_10: shape=torch.Size([51200, 24]), var=['0.306', '0.138', '0.105'], cum=0.549\n",
      "    âœ“ head_11: shape=torch.Size([51200, 24]), var=['0.235', '0.125', '0.099'], cum=0.459\n",
      "    âœ“ head_12: shape=torch.Size([51200, 24]), var=['0.224', '0.152', '0.106'], cum=0.483\n",
      "    âœ“ head_13: shape=torch.Size([51200, 24]), var=['0.330', '0.132', '0.097'], cum=0.558\n",
      "    âœ“ head_14: shape=torch.Size([51200, 24]), var=['0.196', '0.114', '0.097'], cum=0.407\n",
      "    âœ“ head_15: shape=torch.Size([51200, 24]), var=['0.188', '0.135', '0.099'], cum=0.423\n",
      "\n",
      "layer_3:\n",
      "\n",
      "  Block 0:\n",
      "    âœ“ head_0: shape=torch.Size([12800, 24]), var=['0.319', '0.178', '0.108'], cum=0.606\n",
      "    âœ“ head_1: shape=torch.Size([12800, 24]), var=['0.304', '0.187', '0.091'], cum=0.583\n",
      "    âœ“ head_2: shape=torch.Size([12800, 24]), var=['0.364', '0.127', '0.125'], cum=0.616\n",
      "    âœ“ head_3: shape=torch.Size([12800, 24]), var=['0.248', '0.188', '0.114'], cum=0.550\n",
      "    âœ“ head_4: shape=torch.Size([12800, 24]), var=['0.331', '0.231', '0.107'], cum=0.669\n",
      "    âœ“ head_5: shape=torch.Size([12800, 24]), var=['0.613', '0.134', '0.066'], cum=0.812\n",
      "    âœ“ head_6: shape=torch.Size([12800, 24]), var=['0.207', '0.157', '0.136'], cum=0.499\n",
      "    âœ“ head_7: shape=torch.Size([12800, 24]), var=['0.381', '0.146', '0.079'], cum=0.606\n",
      "    âœ“ head_8: shape=torch.Size([12800, 24]), var=['0.320', '0.180', '0.109'], cum=0.610\n",
      "    âœ“ head_9: shape=torch.Size([12800, 24]), var=['0.518', '0.107', '0.076'], cum=0.701\n",
      "    âœ“ head_10: shape=torch.Size([12800, 24]), var=['0.424', '0.132', '0.119'], cum=0.675\n",
      "    âœ“ head_11: shape=torch.Size([12800, 24]), var=['0.284', '0.152', '0.133'], cum=0.568\n",
      "    âœ“ head_12: shape=torch.Size([12800, 24]), var=['0.469', '0.129', '0.080'], cum=0.679\n",
      "    âœ“ head_13: shape=torch.Size([12800, 24]), var=['0.264', '0.217', '0.098'], cum=0.578\n",
      "    âœ“ head_14: shape=torch.Size([12800, 24]), var=['0.317', '0.255', '0.100'], cum=0.672\n",
      "    âœ“ head_15: shape=torch.Size([12800, 24]), var=['0.252', '0.161', '0.102'], cum=0.515\n",
      "    âœ“ head_16: shape=torch.Size([12800, 24]), var=['0.485', '0.157', '0.104'], cum=0.745\n",
      "    âœ“ head_17: shape=torch.Size([12800, 24]), var=['0.232', '0.154', '0.114'], cum=0.500\n",
      "    âœ“ head_18: shape=torch.Size([12800, 24]), var=['0.341', '0.193', '0.128'], cum=0.662\n",
      "    âœ“ head_19: shape=torch.Size([12800, 24]), var=['0.333', '0.135', '0.118'], cum=0.586\n",
      "    âœ“ head_20: shape=torch.Size([12800, 24]), var=['0.323', '0.166', '0.161'], cum=0.650\n",
      "    âœ“ head_21: shape=torch.Size([12800, 24]), var=['0.253', '0.153', '0.119'], cum=0.526\n",
      "    âœ“ head_22: shape=torch.Size([12800, 24]), var=['0.373', '0.165', '0.086'], cum=0.624\n",
      "    âœ“ head_23: shape=torch.Size([12800, 24]), var=['0.274', '0.158', '0.128'], cum=0.560\n",
      "    âœ“ head_24: shape=torch.Size([12800, 24]), var=['0.318', '0.186', '0.074'], cum=0.578\n",
      "    âœ“ head_25: shape=torch.Size([12800, 24]), var=['0.306', '0.186', '0.109'], cum=0.600\n",
      "    âœ“ head_26: shape=torch.Size([12800, 24]), var=['0.265', '0.165', '0.131'], cum=0.562\n",
      "    âœ“ head_27: shape=torch.Size([12800, 24]), var=['0.277', '0.164', '0.092'], cum=0.533\n",
      "    âœ“ head_28: shape=torch.Size([12800, 24]), var=['0.366', '0.175', '0.123'], cum=0.664\n",
      "    âœ“ head_29: shape=torch.Size([12800, 24]), var=['0.268', '0.148', '0.136'], cum=0.552\n",
      "    âœ“ head_30: shape=torch.Size([12800, 24]), var=['0.373', '0.204', '0.148'], cum=0.726\n",
      "    âœ“ head_31: shape=torch.Size([12800, 24]), var=['0.300', '0.248', '0.088'], cum=0.636\n",
      "\n",
      "  Block 1:\n",
      "    âœ“ head_0: shape=torch.Size([12800, 24]), var=['0.310', '0.190', '0.113'], cum=0.613\n",
      "    âœ“ head_1: shape=torch.Size([12800, 24]), var=['0.287', '0.169', '0.121'], cum=0.577\n",
      "    âœ“ head_2: shape=torch.Size([12800, 24]), var=['0.258', '0.181', '0.125'], cum=0.564\n",
      "    âœ“ head_3: shape=torch.Size([12800, 24]), var=['0.447', '0.152', '0.120'], cum=0.719\n",
      "    âœ“ head_4: shape=torch.Size([12800, 24]), var=['0.372', '0.144', '0.122'], cum=0.639\n",
      "    âœ“ head_5: shape=torch.Size([12800, 24]), var=['0.241', '0.205', '0.116'], cum=0.562\n",
      "    âœ“ head_6: shape=torch.Size([12800, 24]), var=['0.303', '0.185', '0.100'], cum=0.589\n",
      "    âœ“ head_7: shape=torch.Size([12800, 24]), var=['0.468', '0.155', '0.081'], cum=0.704\n",
      "    âœ“ head_8: shape=torch.Size([12800, 24]), var=['0.479', '0.166', '0.099'], cum=0.744\n",
      "    âœ“ head_9: shape=torch.Size([12800, 24]), var=['0.307', '0.171', '0.106'], cum=0.584\n",
      "    âœ“ head_10: shape=torch.Size([12800, 24]), var=['0.419', '0.157', '0.106'], cum=0.681\n",
      "    âœ“ head_11: shape=torch.Size([12800, 24]), var=['0.257', '0.194', '0.152'], cum=0.603\n",
      "    âœ“ head_12: shape=torch.Size([12800, 24]), var=['0.364', '0.156', '0.107'], cum=0.627\n",
      "    âœ“ head_13: shape=torch.Size([12800, 24]), var=['0.265', '0.192', '0.178'], cum=0.635\n",
      "    âœ“ head_14: shape=torch.Size([12800, 24]), var=['0.304', '0.219', '0.131'], cum=0.654\n",
      "    âœ“ head_15: shape=torch.Size([12800, 24]), var=['0.258', '0.221', '0.121'], cum=0.601\n",
      "    âœ“ head_16: shape=torch.Size([12800, 24]), var=['0.300', '0.137', '0.114'], cum=0.550\n",
      "    âœ“ head_17: shape=torch.Size([12800, 24]), var=['0.294', '0.228', '0.131'], cum=0.653\n",
      "    âœ“ head_18: shape=torch.Size([12800, 24]), var=['0.261', '0.234', '0.141'], cum=0.636\n",
      "    âœ“ head_19: shape=torch.Size([12800, 24]), var=['0.301', '0.226', '0.129'], cum=0.656\n",
      "    âœ“ head_20: shape=torch.Size([12800, 24]), var=['0.267', '0.161', '0.138'], cum=0.566\n",
      "    âœ“ head_21: shape=torch.Size([12800, 24]), var=['0.303', '0.151', '0.091'], cum=0.546\n",
      "    âœ“ head_22: shape=torch.Size([12800, 24]), var=['0.269', '0.183', '0.128'], cum=0.580\n",
      "    âœ“ head_23: shape=torch.Size([12800, 24]), var=['0.354', '0.203', '0.079'], cum=0.636\n",
      "    âœ“ head_24: shape=torch.Size([12800, 24]), var=['0.306', '0.173', '0.093'], cum=0.571\n",
      "    âœ“ head_25: shape=torch.Size([12800, 24]), var=['0.284', '0.178', '0.125'], cum=0.586\n",
      "    âœ“ head_26: shape=torch.Size([12800, 24]), var=['0.348', '0.189', '0.093'], cum=0.630\n",
      "    âœ“ head_27: shape=torch.Size([12800, 24]), var=['0.521', '0.120', '0.094'], cum=0.735\n",
      "    âœ“ head_28: shape=torch.Size([12800, 24]), var=['0.263', '0.189', '0.134'], cum=0.586\n",
      "    âœ“ head_29: shape=torch.Size([12800, 24]), var=['0.820', '0.056', '0.032'], cum=0.908\n",
      "    âœ“ head_30: shape=torch.Size([12800, 24]), var=['0.336', '0.211', '0.106'], cum=0.653\n",
      "    âœ“ head_31: shape=torch.Size([12800, 24]), var=['0.337', '0.195', '0.103'], cum=0.635\n",
      "\n",
      "================================================================================\n",
      "âœ… Fitting completato!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "ðŸ“Š PCA Variance Ratios:\n",
      "================================================================================\n",
      "\n",
      "layer_0:\n",
      "  block_0:\n",
      "    head_0: top3=['0.879', '0.061', '0.051'], cum=0.991\n",
      "    head_1: top3=['0.782', '0.090', '0.076'], cum=0.948\n",
      "    head_2: top3=['0.883', '0.072', '0.035'], cum=0.989\n",
      "    head_3: top3=['0.836', '0.070', '0.037'], cum=0.943\n",
      "  block_1:\n",
      "    head_0: top3=['0.345', '0.237', '0.149'], cum=0.730\n",
      "    head_1: top3=['0.750', '0.115', '0.042'], cum=0.907\n",
      "    head_2: top3=['0.620', '0.139', '0.116'], cum=0.875\n",
      "    head_3: top3=['0.538', '0.225', '0.105'], cum=0.868\n",
      "\n",
      "layer_1:\n",
      "  block_0:\n",
      "    head_0: top3=['0.434', '0.158', '0.113'], cum=0.706\n",
      "    head_1: top3=['0.241', '0.158', '0.117'], cum=0.515\n",
      "    head_2: top3=['0.243', '0.146', '0.121'], cum=0.511\n",
      "    head_3: top3=['0.620', '0.144', '0.089'], cum=0.854\n",
      "    head_4: top3=['0.576', '0.113', '0.062'], cum=0.751\n",
      "    head_5: top3=['0.254', '0.140', '0.112'], cum=0.506\n",
      "    head_6: top3=['0.495', '0.246', '0.119'], cum=0.860\n",
      "    head_7: top3=['0.279', '0.186', '0.091'], cum=0.557\n",
      "  block_1:\n",
      "    head_0: top3=['0.374', '0.278', '0.082'], cum=0.735\n",
      "    head_1: top3=['0.275', '0.225', '0.108'], cum=0.608\n",
      "    head_2: top3=['0.268', '0.154', '0.141'], cum=0.564\n",
      "    head_3: top3=['0.333', '0.198', '0.131'], cum=0.662\n",
      "    head_4: top3=['0.461', '0.152', '0.076'], cum=0.690\n",
      "    head_5: top3=['0.251', '0.165', '0.128'], cum=0.544\n",
      "    head_6: top3=['0.391', '0.188', '0.121'], cum=0.700\n",
      "    head_7: top3=['0.275', '0.250', '0.141'], cum=0.666\n",
      "\n",
      "layer_2:\n",
      "  block_0:\n",
      "    head_0: top3=['0.338', '0.208', '0.110'], cum=0.655\n",
      "    head_1: top3=['0.368', '0.145', '0.124'], cum=0.637\n",
      "    head_2: top3=['0.383', '0.156', '0.071'], cum=0.611\n",
      "    head_3: top3=['0.401', '0.160', '0.143'], cum=0.704\n",
      "    head_4: top3=['0.292', '0.230', '0.170'], cum=0.692\n",
      "    head_5: top3=['0.391', '0.229', '0.156'], cum=0.775\n",
      "    head_6: top3=['0.379', '0.243', '0.128'], cum=0.750\n",
      "    head_7: top3=['0.444', '0.267', '0.078'], cum=0.790\n",
      "    head_8: top3=['0.295', '0.151', '0.130'], cum=0.576\n",
      "    head_9: top3=['0.306', '0.147', '0.118'], cum=0.570\n",
      "    head_10: top3=['0.338', '0.190', '0.152'], cum=0.680\n",
      "    head_11: top3=['0.406', '0.178', '0.137'], cum=0.720\n",
      "    head_12: top3=['0.374', '0.178', '0.143'], cum=0.695\n",
      "    head_13: top3=['0.282', '0.238', '0.130'], cum=0.650\n",
      "    head_14: top3=['0.358', '0.142', '0.091'], cum=0.591\n",
      "    head_15: top3=['0.429', '0.140', '0.097'], cum=0.666\n",
      "  block_1:\n",
      "    head_0: top3=['0.327', '0.193', '0.116'], cum=0.636\n",
      "    head_1: top3=['0.328', '0.208', '0.083'], cum=0.619\n",
      "    head_2: top3=['0.178', '0.150', '0.106'], cum=0.434\n",
      "    head_3: top3=['0.258', '0.175', '0.133'], cum=0.566\n",
      "    head_4: top3=['0.357', '0.163', '0.123'], cum=0.643\n",
      "    head_5: top3=['0.316', '0.156', '0.141'], cum=0.613\n",
      "    head_6: top3=['0.269', '0.220', '0.111'], cum=0.599\n",
      "    head_7: top3=['0.372', '0.143', '0.085'], cum=0.600\n",
      "    head_8: top3=['0.240', '0.131', '0.100'], cum=0.470\n",
      "    head_9: top3=['0.462', '0.147', '0.090'], cum=0.699\n",
      "    head_10: top3=['0.278', '0.154', '0.123'], cum=0.555\n",
      "    head_11: top3=['0.258', '0.206', '0.148'], cum=0.612\n",
      "    head_12: top3=['0.275', '0.214', '0.143'], cum=0.631\n",
      "    head_13: top3=['0.322', '0.191', '0.101'], cum=0.614\n",
      "    head_14: top3=['0.286', '0.185', '0.141'], cum=0.612\n",
      "    head_15: top3=['0.270', '0.255', '0.154'], cum=0.679\n",
      "  block_2:\n",
      "    head_0: top3=['0.298', '0.207', '0.121'], cum=0.626\n",
      "    head_1: top3=['0.297', '0.182', '0.110'], cum=0.589\n",
      "    head_2: top3=['0.326', '0.111', '0.096'], cum=0.533\n",
      "    head_3: top3=['0.334', '0.175', '0.089'], cum=0.599\n",
      "    head_4: top3=['0.247', '0.133', '0.105'], cum=0.485\n",
      "    head_5: top3=['0.307', '0.242', '0.117'], cum=0.666\n",
      "    head_6: top3=['0.338', '0.307', '0.087'], cum=0.732\n",
      "    head_7: top3=['0.324', '0.126', '0.120'], cum=0.570\n",
      "    head_8: top3=['0.301', '0.136', '0.106'], cum=0.543\n",
      "    head_9: top3=['0.236', '0.166', '0.115'], cum=0.517\n",
      "    head_10: top3=['0.332', '0.187', '0.103'], cum=0.622\n",
      "    head_11: top3=['0.294', '0.206', '0.138'], cum=0.637\n",
      "    head_12: top3=['0.360', '0.135', '0.125'], cum=0.619\n",
      "    head_13: top3=['0.333', '0.125', '0.112'], cum=0.570\n",
      "    head_14: top3=['0.320', '0.155', '0.115'], cum=0.590\n",
      "    head_15: top3=['0.317', '0.140', '0.095'], cum=0.552\n",
      "  block_3:\n",
      "    head_0: top3=['0.360', '0.228', '0.104'], cum=0.692\n",
      "    head_1: top3=['0.253', '0.149', '0.111'], cum=0.512\n",
      "    head_2: top3=['0.271', '0.174', '0.129'], cum=0.573\n",
      "    head_3: top3=['0.258', '0.153', '0.115'], cum=0.527\n",
      "    head_4: top3=['0.212', '0.122', '0.101'], cum=0.436\n",
      "    head_5: top3=['0.289', '0.110', '0.101'], cum=0.501\n",
      "    head_6: top3=['0.225', '0.191', '0.152'], cum=0.568\n",
      "    head_7: top3=['0.204', '0.183', '0.158'], cum=0.544\n",
      "    head_8: top3=['0.241', '0.144', '0.122'], cum=0.508\n",
      "    head_9: top3=['0.332', '0.111', '0.096'], cum=0.539\n",
      "    head_10: top3=['0.197', '0.151', '0.114'], cum=0.462\n",
      "    head_11: top3=['0.224', '0.181', '0.118'], cum=0.523\n",
      "    head_12: top3=['0.350', '0.118', '0.083'], cum=0.552\n",
      "    head_13: top3=['0.190', '0.136', '0.112'], cum=0.437\n",
      "    head_14: top3=['0.278', '0.153', '0.103'], cum=0.534\n",
      "    head_15: top3=['0.171', '0.123', '0.102'], cum=0.395\n",
      "  block_4:\n",
      "    head_0: top3=['0.286', '0.169', '0.120'], cum=0.575\n",
      "    head_1: top3=['0.214', '0.164', '0.119'], cum=0.497\n",
      "    head_2: top3=['0.339', '0.140', '0.088'], cum=0.568\n",
      "    head_3: top3=['0.249', '0.110', '0.104'], cum=0.463\n",
      "    head_4: top3=['0.198', '0.144', '0.097'], cum=0.438\n",
      "    head_5: top3=['0.210', '0.124', '0.098'], cum=0.432\n",
      "    head_6: top3=['0.245', '0.176', '0.112'], cum=0.534\n",
      "    head_7: top3=['0.209', '0.181', '0.077'], cum=0.468\n",
      "    head_8: top3=['0.234', '0.149', '0.125'], cum=0.508\n",
      "    head_9: top3=['0.229', '0.152', '0.112'], cum=0.493\n",
      "    head_10: top3=['0.368', '0.114', '0.094'], cum=0.575\n",
      "    head_11: top3=['0.305', '0.161', '0.100'], cum=0.566\n",
      "    head_12: top3=['0.274', '0.114', '0.105'], cum=0.493\n",
      "    head_13: top3=['0.330', '0.148', '0.086'], cum=0.564\n",
      "    head_14: top3=['0.210', '0.152', '0.119'], cum=0.481\n",
      "    head_15: top3=['0.233', '0.162', '0.092'], cum=0.487\n",
      "  block_5:\n",
      "    head_0: top3=['0.175', '0.136', '0.095'], cum=0.406\n",
      "    head_1: top3=['0.199', '0.137', '0.111'], cum=0.447\n",
      "    head_2: top3=['0.158', '0.145', '0.112'], cum=0.414\n",
      "    head_3: top3=['0.274', '0.161', '0.136'], cum=0.571\n",
      "    head_4: top3=['0.265', '0.157', '0.112'], cum=0.535\n",
      "    head_5: top3=['0.261', '0.137', '0.103'], cum=0.502\n",
      "    head_6: top3=['0.296', '0.139', '0.097'], cum=0.532\n",
      "    head_7: top3=['0.193', '0.136', '0.114'], cum=0.442\n",
      "    head_8: top3=['0.336', '0.094', '0.091'], cum=0.522\n",
      "    head_9: top3=['0.227', '0.192', '0.105'], cum=0.524\n",
      "    head_10: top3=['0.306', '0.138', '0.105'], cum=0.549\n",
      "    head_11: top3=['0.235', '0.125', '0.099'], cum=0.459\n",
      "    head_12: top3=['0.224', '0.152', '0.106'], cum=0.483\n",
      "    head_13: top3=['0.330', '0.132', '0.097'], cum=0.558\n",
      "    head_14: top3=['0.196', '0.114', '0.097'], cum=0.407\n",
      "    head_15: top3=['0.188', '0.135', '0.099'], cum=0.423\n",
      "\n",
      "layer_3:\n",
      "  block_0:\n",
      "    head_0: top3=['0.319', '0.178', '0.108'], cum=0.606\n",
      "    head_1: top3=['0.304', '0.187', '0.091'], cum=0.583\n",
      "    head_2: top3=['0.364', '0.127', '0.125'], cum=0.616\n",
      "    head_3: top3=['0.248', '0.188', '0.114'], cum=0.550\n",
      "    head_4: top3=['0.331', '0.231', '0.107'], cum=0.669\n",
      "    head_5: top3=['0.613', '0.134', '0.066'], cum=0.812\n",
      "    head_6: top3=['0.207', '0.157', '0.136'], cum=0.499\n",
      "    head_7: top3=['0.381', '0.146', '0.079'], cum=0.606\n",
      "    head_8: top3=['0.320', '0.180', '0.109'], cum=0.610\n",
      "    head_9: top3=['0.518', '0.107', '0.076'], cum=0.701\n",
      "    head_10: top3=['0.424', '0.132', '0.119'], cum=0.675\n",
      "    head_11: top3=['0.284', '0.152', '0.133'], cum=0.568\n",
      "    head_12: top3=['0.469', '0.129', '0.080'], cum=0.679\n",
      "    head_13: top3=['0.264', '0.217', '0.098'], cum=0.578\n",
      "    head_14: top3=['0.317', '0.255', '0.100'], cum=0.672\n",
      "    head_15: top3=['0.252', '0.161', '0.102'], cum=0.515\n",
      "    head_16: top3=['0.485', '0.157', '0.104'], cum=0.745\n",
      "    head_17: top3=['0.232', '0.154', '0.114'], cum=0.500\n",
      "    head_18: top3=['0.341', '0.193', '0.128'], cum=0.662\n",
      "    head_19: top3=['0.333', '0.135', '0.118'], cum=0.586\n",
      "    head_20: top3=['0.323', '0.166', '0.161'], cum=0.650\n",
      "    head_21: top3=['0.253', '0.153', '0.119'], cum=0.526\n",
      "    head_22: top3=['0.373', '0.165', '0.086'], cum=0.624\n",
      "    head_23: top3=['0.274', '0.158', '0.128'], cum=0.560\n",
      "    head_24: top3=['0.318', '0.186', '0.074'], cum=0.578\n",
      "    head_25: top3=['0.306', '0.186', '0.109'], cum=0.600\n",
      "    head_26: top3=['0.265', '0.165', '0.131'], cum=0.562\n",
      "    head_27: top3=['0.277', '0.164', '0.092'], cum=0.533\n",
      "    head_28: top3=['0.366', '0.175', '0.123'], cum=0.664\n",
      "    head_29: top3=['0.268', '0.148', '0.136'], cum=0.552\n",
      "    head_30: top3=['0.373', '0.204', '0.148'], cum=0.726\n",
      "    head_31: top3=['0.300', '0.248', '0.088'], cum=0.636\n",
      "  block_1:\n",
      "    head_0: top3=['0.310', '0.190', '0.113'], cum=0.613\n",
      "    head_1: top3=['0.287', '0.169', '0.121'], cum=0.577\n",
      "    head_2: top3=['0.258', '0.181', '0.125'], cum=0.564\n",
      "    head_3: top3=['0.447', '0.152', '0.120'], cum=0.719\n",
      "    head_4: top3=['0.372', '0.144', '0.122'], cum=0.639\n",
      "    head_5: top3=['0.241', '0.205', '0.116'], cum=0.562\n",
      "    head_6: top3=['0.303', '0.185', '0.100'], cum=0.589\n",
      "    head_7: top3=['0.468', '0.155', '0.081'], cum=0.704\n",
      "    head_8: top3=['0.479', '0.166', '0.099'], cum=0.744\n",
      "    head_9: top3=['0.307', '0.171', '0.106'], cum=0.584\n",
      "    head_10: top3=['0.419', '0.157', '0.106'], cum=0.681\n",
      "    head_11: top3=['0.257', '0.194', '0.152'], cum=0.603\n",
      "    head_12: top3=['0.364', '0.156', '0.107'], cum=0.627\n",
      "    head_13: top3=['0.265', '0.192', '0.178'], cum=0.635\n",
      "    head_14: top3=['0.304', '0.219', '0.131'], cum=0.654\n",
      "    head_15: top3=['0.258', '0.221', '0.121'], cum=0.601\n",
      "    head_16: top3=['0.300', '0.137', '0.114'], cum=0.550\n",
      "    head_17: top3=['0.294', '0.228', '0.131'], cum=0.653\n",
      "    head_18: top3=['0.261', '0.234', '0.141'], cum=0.636\n",
      "    head_19: top3=['0.301', '0.226', '0.129'], cum=0.656\n",
      "    head_20: top3=['0.267', '0.161', '0.138'], cum=0.566\n",
      "    head_21: top3=['0.303', '0.151', '0.091'], cum=0.546\n",
      "    head_22: top3=['0.269', '0.183', '0.128'], cum=0.580\n",
      "    head_23: top3=['0.354', '0.203', '0.079'], cum=0.636\n",
      "    head_24: top3=['0.306', '0.173', '0.093'], cum=0.571\n",
      "    head_25: top3=['0.284', '0.178', '0.125'], cum=0.586\n",
      "    head_26: top3=['0.348', '0.189', '0.093'], cum=0.630\n",
      "    head_27: top3=['0.521', '0.120', '0.094'], cum=0.735\n",
      "    head_28: top3=['0.263', '0.189', '0.134'], cum=0.586\n",
      "    head_29: top3=['0.820', '0.056', '0.032'], cum=0.908\n",
      "    head_30: top3=['0.336', '0.211', '0.106'], cum=0.653\n",
      "    head_31: top3=['0.337', '0.195', '0.103'], cum=0.635\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Fitting PCA Components\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Fit PCA\n",
    "print(f\"Fitting PCA on {len(pca_dataset)} samples...\")\n",
    "variance_ratios = clap_residual.clap.audio_encoder.base.htsat.fit_spectral_layers(\n",
    "    pca_loader,\n",
    "    max_samples=PCA_SAMPLES\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š PCA Variance Ratios:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Gestione corretta della struttura gerarchica\n",
    "for layer_name, layer_data in variance_ratios.items():\n",
    "    print(f\"\\n{layer_name}:\")\n",
    "    \n",
    "    # Controlla se Ã¨ mode 'attention' (struttura annidata) o 'layer' (array diretto)\n",
    "    if isinstance(layer_data, dict):\n",
    "        # Mode ATTENTION: layer_data contiene blocchi\n",
    "        for block_name, block_data in layer_data.items():\n",
    "            print(f\"  {block_name}:\")\n",
    "            \n",
    "            # block_data contiene le teste\n",
    "            for head_name, head_variance in block_data.items():\n",
    "                # head_variance Ã¨ l'array numpy con le variance ratios\n",
    "                top3 = head_variance[:3]\n",
    "                cumulative = head_variance[:3].sum()\n",
    "                print(f\"    {head_name}: top3={[f'{v:.3f}' for v in top3]}, cum={cumulative:.3f}\")\n",
    "    else:\n",
    "        # Mode LAYER: layer_data Ã¨ direttamente l'array di variance ratios\n",
    "        top5 = layer_data[:5]\n",
    "        cumulative = layer_data[:5].sum()\n",
    "        print(f\"  Top 5 components: {[f'{v:.4f}' for v in top5]}\")\n",
    "        print(f\"  Cumulative variance (top 5): {cumulative:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7e54bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Text embeddings shape: torch.Size([14, 1024])\n",
      "\n",
      "ðŸ“Š Testing on 300 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a14923a49d26499e9ccaa0513dd46ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Baseline:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Baseline Accuracy: 0.477 (47.7%)\n"
     ]
    }
   ],
   "source": [
    "# Get text embeddings ONCE for all classes\n",
    "text_embeddings = clap_standard.get_text_embeddings(text_labels)\n",
    "print(f\"   Text embeddings shape: {text_embeddings.shape}\")\n",
    "\n",
    "# Test on subset first (use full dataset later)\n",
    "test_size = 300  # Start with 200 samples for speed\n",
    "print(f\"\\nðŸ“Š Testing on {test_size} samples...\")\n",
    "\n",
    "y_preds_baseline, y_labels = [], []\n",
    "\n",
    "for i in tqdm(range(test_size), desc=\"Baseline\"):\n",
    "    # Get audio file path and label\n",
    "    audio_path, target, one_hot_target = dataset[-(i+1)]\n",
    "    \n",
    "    # Get audio embedding\n",
    "    audio_embedding = clap_standard.get_audio_embeddings([audio_path], resample=True)\n",
    "    \n",
    "    # Compute similarity\n",
    "    similarity = clap_standard.compute_similarity(audio_embedding, text_embeddings)\n",
    "    \n",
    "    # Get prediction\n",
    "    y_pred = F.softmax(similarity.detach().cpu(), dim=1).numpy()\n",
    "    y_preds_baseline.append(y_pred)\n",
    "    y_labels.append(one_hot_target.detach().cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "y_labels_array = np.concatenate(y_labels, axis=0)\n",
    "y_preds_baseline_array = np.concatenate(y_preds_baseline, axis=0)\n",
    "\n",
    "baseline_acc = accuracy_score(\n",
    "    np.argmax(y_labels_array, axis=1), \n",
    "    np.argmax(y_preds_baseline_array, axis=1)\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Baseline Accuracy: {baseline_acc:.3f} ({baseline_acc*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1acd5e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4766666666666667"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e60c8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Text embeddings shape: torch.Size([14, 1024])\n",
      "\n",
      "ðŸ“Š Testing on 300 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a1e98446604b4bb64c90e3d7124737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "residual:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Residual Accuracy: 0.477 (47.7%)\n"
     ]
    }
   ],
   "source": [
    "# Get text embeddings ONCE for all classes\n",
    "text_embeddings = clap_residual.get_text_embeddings(text_labels)\n",
    "print(f\"   Text embeddings shape: {text_embeddings.shape}\")\n",
    "\n",
    "# Test on subset first (use full dataset later)\n",
    "test_size = 300  # Start with 200 samples for speed\n",
    "print(f\"\\nðŸ“Š Testing on {test_size} samples...\")\n",
    "\n",
    "y_preds_residual, y_labels = [], []\n",
    "\n",
    "for i in tqdm(range(test_size), desc=\"residual\"):\n",
    "    # Get audio file path and label\n",
    "    audio_path, target, one_hot_target = dataset[-(i+1)]\n",
    "    \n",
    "    # Get audio embedding\n",
    "    audio_embedding = clap_residual.get_audio_embeddings([audio_path], resample=True)\n",
    "    \n",
    "    # Compute similarity\n",
    "    similarity = clap_residual.compute_similarity(audio_embedding, text_embeddings)\n",
    "    \n",
    "    # Get prediction\n",
    "    y_pred = F.softmax(similarity.detach().cpu(), dim=1).numpy()\n",
    "    y_preds_residual.append(y_pred)\n",
    "    y_labels.append(one_hot_target.detach().cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "y_labels_array = np.concatenate(y_labels, axis=0)\n",
    "y_preds_residual_array = np.concatenate(y_preds_residual, axis=0)\n",
    "\n",
    "residual_acc = accuracy_score(\n",
    "    np.argmax(y_labels_array, axis=1), \n",
    "    np.argmax(y_preds_residual_array, axis=1)\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Residual Accuracy: {residual_acc:.3f} ({residual_acc*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4928d699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4766666666666667"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residual_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8e65ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.9266509e-03, 1.7361898e-06, 8.6167414e-04, ..., 1.2689318e-02,\n",
       "        1.4975495e-03, 1.2030244e-03],\n",
       "       [5.8291259e-04, 2.4542878e-08, 1.0958888e-03, ..., 6.5481677e-03,\n",
       "        5.7724334e-04, 4.0299790e-05],\n",
       "       [3.2923691e-04, 1.4916168e-07, 2.2600570e-03, ..., 2.1775109e-03,\n",
       "        1.9798054e-04, 2.3463765e-05],\n",
       "       ...,\n",
       "       [4.1160925e-05, 5.4688858e-06, 2.3399117e-04, ..., 4.6371697e-03,\n",
       "        5.7084661e-04, 5.0900719e-04],\n",
       "       [8.4686953e-06, 2.0872583e-06, 8.4836545e-05, ..., 1.3730243e-04,\n",
       "        2.6141252e-05, 4.6615824e-05],\n",
       "       [2.4429901e-06, 1.2070299e-06, 1.6206970e-05, ..., 2.3242106e-05,\n",
       "        5.3767344e-06, 7.7631994e-06]], shape=(300, 14), dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds_residual_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "732a402f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.1678806e-03, 1.2864175e-06, 3.1639051e-04, ..., 1.0069638e-02,\n",
       "        4.3978886e-04, 4.1518782e-04],\n",
       "       [2.4781207e-02, 4.1674798e-07, 8.6041988e-04, ..., 1.3217582e-01,\n",
       "        2.1362571e-02, 1.8601153e-03],\n",
       "       [3.2923743e-04, 1.4916178e-07, 2.2600584e-03, ..., 2.1775123e-03,\n",
       "        1.9798067e-04, 2.3463779e-05],\n",
       "       ...,\n",
       "       [4.1160984e-05, 5.4688940e-06, 2.3399139e-04, ..., 4.6371715e-03,\n",
       "        5.7084742e-04, 5.0900766e-04],\n",
       "       [8.4687190e-06, 2.0872642e-06, 8.4836785e-05, ..., 1.3730268e-04,\n",
       "        2.6141302e-05, 4.6615911e-05],\n",
       "       [2.4429924e-06, 1.2070299e-06, 1.6206954e-05, ..., 2.3242106e-05,\n",
       "        5.3767399e-06, 7.7632076e-06]], shape=(300, 14), dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds_baseline_array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9fdc7130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ðŸ” Verifica Stato Fitting di TUTTI gli Spectral Layers\n",
      "================================================================================\n",
      "\n",
      "âœ… TUTTI i layer sono fittati correttamente con pesi = 1.0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VERIFICA: Tutti gli Spectral Layers sono stati fittati?\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ” Verifica Stato Fitting di TUTTI gli Spectral Layers\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "htsat = clap_residual.clap.audio_encoder.base.htsat\n",
    "all_fitted = True\n",
    "\n",
    "for layer_idx in residual_config['target_layers']:\n",
    "    layer_name = f'layer_{layer_idx}'\n",
    "    \n",
    "    if layer_name in htsat.spectral_layers:\n",
    "        block_spectral_layers = htsat.spectral_layers[layer_name]\n",
    "        \n",
    "        for block_idx, head_spectral_layers in enumerate(block_spectral_layers):\n",
    "            for head_idx, spectral_layer in enumerate(head_spectral_layers):\n",
    "                is_fitted = spectral_layer.is_fitted.item()\n",
    "                \n",
    "                if not is_fitted:\n",
    "                    print(f\"âŒ {layer_name}, Block {block_idx}, Head {head_idx}: NOT FITTED!\")\n",
    "                    all_fitted = False\n",
    "                else:\n",
    "                    # Verifica che i pesi siano effettivamente 1.0\n",
    "                    weights = spectral_layer.pc_weights.data\n",
    "                    all_ones = torch.allclose(weights, torch.ones_like(weights), atol=1e-6)\n",
    "                    \n",
    "                    if not all_ones:\n",
    "                        print(f\"âš ï¸  {layer_name}, Block {block_idx}, Head {head_idx}: \"\n",
    "                              f\"Fitted ma pesi NON sono 1.0!\")\n",
    "                        print(f\"    Primi 5 pesi: {weights[:5].tolist()}\")\n",
    "                        all_fitted = False\n",
    "\n",
    "if all_fitted:\n",
    "    print(\"\\nâœ… TUTTI i layer sono fittati correttamente con pesi = 1.0\")\n",
    "else:\n",
    "    print(\"\\nâŒ PROBLEMA: Alcuni layer non sono fittati correttamente!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "726a89f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path, target, one_hot_target = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d03d337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6079,  0.6576, -0.5101,  ...,  1.8574,  0.9557,  0.2268]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clap_residual.get_audio_embeddings([audio_path], resample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2aa61e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7623,  0.2343, -0.5101,  ...,  1.8940,  0.9414, -0.0119]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clap_standard.get_audio_embeddings([audio_path], resample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c17b4e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m torch.allclose(\u001b[43mx\u001b[49m, y, atol=\u001b[32m1e-5\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "torch.allclose(x, y, atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac8fac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5497e-06)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(x - y).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9dda6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0003)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(x - y).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899bf7d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2468c504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ResiDual-CLAP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
