{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "189dca85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import from your codebase\n",
    "from CLAPWrapper import CLAPWrapper\n",
    "from datasets.esc50 import ESC50\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e48fa266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Loading ESC50 Dataset\n",
      "================================================================================\n",
      "Loading audio files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 17482.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset loaded: 2000 samples\n",
      "   Classes: 50 categories\n",
      "   Sample classes: ['airplane', 'breathing', 'brushing teeth', 'can opening', 'car horn']\n",
      "\n",
      "üìù Text prompts: 50 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Loading ESC50 Dataset\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "root_path = \"./data\"\n",
    "dataset = ESC50(root=root_path, download=True)\n",
    "print(f\"‚úÖ Dataset loaded: {len(dataset)} samples\")\n",
    "print(f\"   Classes: {len(dataset.classes)} categories\")\n",
    "print(f\"   Sample classes: {dataset.classes[:5]}\")\n",
    "\n",
    "# Prepare text prompts\n",
    "prompt = 'this is the sound of '\n",
    "text_labels = [prompt + x for x in dataset.classes]\n",
    "print(f\"\\nüìù Text prompts: {len(text_labels)} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebc7cf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Initializing Models\n",
      "================================================================================\n",
      "\n",
      "üîß Loading CLAP Standard...\n",
      "\n",
      "üîß Loading ResiDualCLAP...\n",
      "[2, 2, 6, 2]\n",
      "üîç Detecting layer dimensions...\n",
      "  ‚úì 0: torch.Size([1, 1024, 192])\n",
      "  ‚úì 1: torch.Size([1, 256, 384])\n",
      "  ‚úì 2: torch.Size([1, 64, 768])\n",
      "  ‚úì 3: torch.Size([1, 64, 768])\n",
      "  ‚úì layer_3: 768D ‚Üí 7 PCs\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: Initialize Models\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Initializing Models\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Residual config con pc_weights = 1.0 (identit√†)\n",
    "residual_config = {\n",
    "    'n_components_ratio': .01,\n",
    "    'reweight_factor': 3.0,\n",
    "    'target_layers': [3],  # Layers dove applicare reweighting\n",
    "    'analysis_mode': True\n",
    "}\n",
    "\n",
    "print(\"\\nüîß Loading CLAP Standard...\")\n",
    "clap_standard = CLAPWrapper(\n",
    "    version='2023',  # or '2022'\n",
    "    use_cuda=torch.cuda.is_available(),\n",
    "    type='classic'\n",
    ")\n",
    "\n",
    "print(\"\\nüîß Loading ResiDualCLAP...\")\n",
    "clap_residual = CLAPWrapper(\n",
    "    version='2023',\n",
    "    use_cuda=torch.cuda.is_available(),\n",
    "    type='residual',\n",
    "    residual_config=residual_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e07ed051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Fitting PCA Components\n",
      "================================================================================\n",
      "Collecting samples for PCA fitting (max 200 samples)...\n",
      "Fitting PCA on 200 samples...\n",
      "\n",
      "================================================================================\n",
      "üîç PHASE 1: Collecting Hidden States from HTSAT Layers\n",
      "================================================================================\n",
      "Target layers: ['layer_3']\n",
      "Max samples to collect: 100\n",
      "Batches in dataloader: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting samples: 100%|‚ñà‚ñà‚ñà| 7/7 [00:21<00:00,  3.10s/batch, samples=112/100, layers=1/1, failed=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Collection completed in 21.70s\n",
      "  ‚Ä¢ Total samples processed: 112\n",
      "  ‚Ä¢ Successful batches: 7\n",
      "  ‚Ä¢ Failed batches: 0\n",
      "  ‚Ä¢ Samples per second: 5.2\n",
      "\n",
      "üì¶ Collected data per layer:\n",
      "  ‚Ä¢ layer_3: 7 batches, 7,168 tokens\n",
      "\n",
      "================================================================================\n",
      "üìä PHASE 2: Fitting PCA Components\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting PCA:   0%|          | 0/1 [00:00<?, ?layer/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Processing layer_3:\n",
      "  ‚Ä¢ Concatenating 7 batches...\n",
      "  ‚Ä¢ Combined shape: torch.Size([7168, 768]) (7,168 samples)\n",
      "  ‚Ä¢ Memory usage: 21.0 MB\n",
      "  ‚Ä¢ Running PCA decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting PCA: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.92layer/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚Ä¢ PCA completed in 0.51s\n",
      "\n",
      "  üìà Variance Analysis:\n",
      "     ‚Ä¢ Total components: 7\n",
      "     ‚Ä¢ Top 5 variances: ['0.0721', '0.0603', '0.0539', '0.0486', '0.0451']\n",
      "     ‚Ä¢ Cumulative variance:\n",
      "        - 50% variance: 8/7 components (114.3%)\n",
      "        - 70% variance: 8/7 components (114.3%)\n",
      "        - 80% variance: 8/7 components (114.3%)\n",
      "        - 90% variance: 8/7 components (114.3%)\n",
      "        - 95% variance: 8/7 components (114.3%)\n",
      "\n",
      "     Top 10 components bar:\n",
      "        PC 1: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 0.0721\n",
      "        PC 2: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 0.0603\n",
      "        PC 3: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 0.0539\n",
      "        PC 4: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 0.0486\n",
      "        PC 5: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 0.0451\n",
      "        PC 6: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 0.0399\n",
      "        PC 7: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 0.0344\n",
      "\n",
      "================================================================================\n",
      "‚úì PCA Fitting Completed Successfully\n",
      "================================================================================\n",
      "Total time: 22.23s\n",
      "\n",
      "Spectral layers ready for reweighting:\n",
      "  ‚Ä¢ layer_3: ‚úì FITTED\n",
      "\n",
      "üìä PCA Variance Ratios:\n",
      "   layer_3: Top 5 components = [0.07210616 0.06026053 0.0539495  0.04858526 0.04506697]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Fitting PCA Components\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare audio samples for PCA fitting\n",
    "print(\"Collecting samples for PCA fitting (max 200 samples)...\")\n",
    "\n",
    "# Create a simple dataloader wrapper per PCA fitting\n",
    "class SimpleAudioDataset:\n",
    "    def __init__(self, wrapper, esc50_dataset, max_samples=1000):\n",
    "        self.wrapper = wrapper\n",
    "        self.audio_paths = []\n",
    "        for i in range(min(max_samples, len(esc50_dataset))):\n",
    "            audio_path, _, _ = esc50_dataset[i]\n",
    "            self.audio_paths.append(audio_path)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_tensor = self.wrapper.load_audio_into_tensor(\n",
    "            self.audio_paths[idx],\n",
    "            self.wrapper.args.duration,\n",
    "            resample=True\n",
    "        )\n",
    "        # ‚úÖ Assicurati sia 1D\n",
    "        if audio_tensor.dim() > 1:\n",
    "            audio_tensor = audio_tensor.squeeze()\n",
    "        \n",
    "        return audio_tensor\n",
    "\n",
    "# Create dataset and loader\n",
    "pca_dataset = SimpleAudioDataset(clap_residual, dataset, max_samples=200)\n",
    "pca_loader = DataLoader(\n",
    "    pca_dataset, \n",
    "    batch_size=16, \n",
    "    shuffle=False,\n",
    "    num_workers=0,  # Start with 0 for debugging\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "# Fit PCA\n",
    "print(f\"Fitting PCA on {len(pca_dataset)} samples...\")\n",
    "variance_ratios = clap_residual.clap.audio_encoder.base.htsat.fit_spectral_layers(\n",
    "    pca_loader,\n",
    "    max_samples=100\n",
    ")\n",
    "\n",
    "print(\"\\nüìä PCA Variance Ratios:\")\n",
    "for layer_name, ratios in variance_ratios.items():\n",
    "    print(f\"   {layer_name}: Top 5 components = {ratios[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7e54bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Text embeddings shape: torch.Size([50, 1024])\n",
      "\n",
      "üìä Testing on 100 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c9587d5d70541f2b968b02fdfefcfc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Baseline:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Baseline Accuracy: 0.920 (92.0%)\n"
     ]
    }
   ],
   "source": [
    "# Get text embeddings ONCE for all classes\n",
    "text_embeddings = clap_standard.get_text_embeddings(text_labels)\n",
    "print(f\"   Text embeddings shape: {text_embeddings.shape}\")\n",
    "\n",
    "# Test on subset first (use full dataset later)\n",
    "test_size = 100  # Start with 200 samples for speed\n",
    "print(f\"\\nüìä Testing on {test_size} samples...\")\n",
    "\n",
    "y_preds_baseline, y_labels = [], []\n",
    "\n",
    "for i in tqdm(range(test_size), desc=\"Baseline\"):\n",
    "    # Get audio file path and label\n",
    "    audio_path, target, one_hot_target = dataset[-(i+1)]\n",
    "    \n",
    "    # Get audio embedding\n",
    "    audio_embedding = clap_standard.get_audio_embeddings([audio_path], resample=True)\n",
    "    \n",
    "    # Compute similarity\n",
    "    similarity = clap_standard.compute_similarity(audio_embedding, text_embeddings)\n",
    "    \n",
    "    # Get prediction\n",
    "    y_pred = F.softmax(similarity.detach().cpu(), dim=1).numpy()\n",
    "    y_preds_baseline.append(y_pred)\n",
    "    y_labels.append(one_hot_target.detach().cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "y_labels_array = np.concatenate(y_labels, axis=0)\n",
    "y_preds_baseline_array = np.concatenate(y_preds_baseline, axis=0)\n",
    "\n",
    "baseline_acc = accuracy_score(\n",
    "    np.argmax(y_labels_array, axis=1), \n",
    "    np.argmax(y_preds_baseline_array, axis=1)\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Baseline Accuracy: {baseline_acc:.3f} ({baseline_acc*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e60c8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Text embeddings shape: torch.Size([50, 1024])\n",
      "\n",
      "üìä Testing on 100 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5386a3275b1848ec8b8acd3146ee6f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Baseline:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Baseline Accuracy: 0.910 (91.0%)\n"
     ]
    }
   ],
   "source": [
    "# Get text embeddings ONCE for all classes\n",
    "text_embeddings = clap_residual.get_text_embeddings(text_labels)\n",
    "print(f\"   Text embeddings shape: {text_embeddings.shape}\")\n",
    "\n",
    "# Test on subset first (use full dataset later)\n",
    "test_size = 100  # Start with 200 samples for speed\n",
    "print(f\"\\nüìä Testing on {test_size} samples...\")\n",
    "\n",
    "y_preds_baseline, y_labels = [], []\n",
    "\n",
    "for i in tqdm(range(test_size), desc=\"Baseline\"):\n",
    "    # Get audio file path and label\n",
    "    audio_path, target, one_hot_target = dataset[-(i+1)]\n",
    "    \n",
    "    # Get audio embedding\n",
    "    audio_embedding = clap_residual.get_audio_embeddings([audio_path], resample=True)\n",
    "    \n",
    "    # Compute similarity\n",
    "    similarity = clap_residual.compute_similarity(audio_embedding, text_embeddings)\n",
    "    \n",
    "    # Get prediction\n",
    "    y_pred = F.softmax(similarity.detach().cpu(), dim=1).numpy()\n",
    "    y_preds_baseline.append(y_pred)\n",
    "    y_labels.append(one_hot_target.detach().cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "y_labels_array = np.concatenate(y_labels, axis=0)\n",
    "y_preds_baseline_array = np.concatenate(y_preds_baseline, axis=0)\n",
    "\n",
    "baseline_acc = accuracy_score(\n",
    "    np.argmax(y_labels_array, axis=1), \n",
    "    np.argmax(y_preds_baseline_array, axis=1)\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Baseline Accuracy: {baseline_acc:.3f} ({baseline_acc*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "726a89f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path, target, one_hot_target = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d03d337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAIII, reweiting layer 0\n",
      "VAIII, reweiting layer 1\n",
      "VAIII, reweiting layer 2\n",
      "VAIII, reweiting layer 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8734,  0.3351, -0.7393,  ...,  2.1447,  1.0662, -0.1054]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clap_residual.get_audio_embeddings([audio_path], resample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2aa61e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7623,  0.2343, -0.5101,  ...,  1.8940,  0.9414, -0.0119]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clap_standard.get_audio_embeddings([audio_path], resample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ee6592",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ResiDual-CLAP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
