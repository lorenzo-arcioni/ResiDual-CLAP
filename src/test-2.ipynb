{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "189dca85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import from your codebase\n",
    "from CLAPWrapper import CLAPWrapper\n",
    "from datasets.esc50 import ESC50\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e48fa266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Loading ESC50 Dataset\n",
      "================================================================================\n",
      "Loading audio files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 14151.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset loaded: 2000 samples\n",
      "   Classes: 50 categories\n",
      "   Sample classes: ['airplane', 'breathing', 'brushing teeth', 'can opening', 'car horn']\n",
      "\n",
      "ðŸ“ Text prompts: 50 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Loading ESC50 Dataset\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "root_path = \"../data\"\n",
    "dataset = ESC50(root=root_path, download=False)\n",
    "print(f\"âœ… Dataset loaded: {len(dataset)} samples\")\n",
    "print(f\"   Classes: {len(dataset.classes)} categories\")\n",
    "print(f\"   Sample classes: {dataset.classes[:5]}\")\n",
    "\n",
    "# Prepare text prompts\n",
    "prompt = 'this is the sound of '\n",
    "text_labels = [prompt + x for x in dataset.classes]\n",
    "print(f\"\\nðŸ“ Text prompts: {len(text_labels)} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebc7cf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Initializing Models\n",
      "================================================================================\n",
      "\n",
      "ðŸ”§ Loading CLAP Standard...\n",
      "\n",
      "ðŸ”§ Loading ResiDualCLAP...\n",
      "\n",
      "================================================================================\n",
      "ðŸ”§ Setup ResiDual HTSAT\n",
      "================================================================================\n",
      "ModalitÃ : LAYER\n",
      "Target layers: [0, 1, 2, 3]\n",
      "PCA components ratio: 1.0\n",
      "Reweight factor: 0.0\n",
      "\n",
      "âœ“ layer_0:\n",
      "  ModalitÃ : LAYER reweighting\n",
      "  Layer dim: 96D â†’ 96 PCs\n",
      "\n",
      "âœ“ layer_1:\n",
      "  ModalitÃ : LAYER reweighting\n",
      "  Layer dim: 192D â†’ 192 PCs\n",
      "\n",
      "âœ“ layer_2:\n",
      "  ModalitÃ : LAYER reweighting\n",
      "  Layer dim: 384D â†’ 384 PCs\n",
      "\n",
      "âœ“ layer_3:\n",
      "  ModalitÃ : LAYER reweighting\n",
      "  Layer dim: 768D â†’ 768 PCs\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: Initialize Models\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Initializing Models\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Residual config con pc_weights = 1.0 (identitÃ )\n",
    "residual_config = {\n",
    "    'mode': 'layer',\n",
    "    'n_components_ratio': 1.0,\n",
    "    'reweight_factor': 0.0,\n",
    "    'target_layers': [0, 1, 2, 3],  # Layers dove applicare reweighting\n",
    "}\n",
    "\n",
    "print(\"\\nðŸ”§ Loading CLAP Standard...\")\n",
    "clap_standard = CLAPWrapper(\n",
    "    version='2023',  # or '2022'\n",
    "    use_cuda=torch.cuda.is_available(),\n",
    "    type='classic'\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ”§ Loading ResiDualCLAP...\")\n",
    "clap_residual = CLAPWrapper(\n",
    "    version='2023',\n",
    "    use_cuda=torch.cuda.is_available(),\n",
    "    type='residual',\n",
    "    residual_config=residual_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e07ed051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting samples for PCA fitting (max 200 samples)...OK\n"
     ]
    }
   ],
   "source": [
    "# Prepare audio samples for PCA fitting\n",
    "print(\"Collecting samples for PCA fitting (max 200 samples)...\", end='')\n",
    "\n",
    "# Create a simple dataloader wrapper per PCA fitting\n",
    "class SimpleAudioDataset:\n",
    "    def __init__(self, wrapper, esc50_dataset, max_samples=1000):\n",
    "        self.wrapper = wrapper\n",
    "        self.audio_paths = []\n",
    "        for i in range(min(max_samples, len(esc50_dataset))):\n",
    "            audio_path, _, _ = esc50_dataset[i]\n",
    "            self.audio_paths.append(audio_path)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_tensor = self.wrapper.load_audio_into_tensor(\n",
    "            self.audio_paths[idx],\n",
    "            self.wrapper.args.duration,\n",
    "            resample=True\n",
    "        )\n",
    "        # âœ… Assicurati sia 1D\n",
    "        if audio_tensor.dim() > 1:\n",
    "            audio_tensor = audio_tensor.squeeze()\n",
    "        \n",
    "        return audio_tensor\n",
    "\n",
    "# Create dataset and loader\n",
    "pca_dataset = SimpleAudioDataset(clap_residual, dataset, max_samples=50)\n",
    "pca_loader = DataLoader(\n",
    "    pca_dataset, \n",
    "    batch_size=16, \n",
    "    shuffle=False,\n",
    "    num_workers=0,  # Start with 0 for debugging\n",
    "    pin_memory=False\n",
    ")\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4e2ba8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([308700])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea6b11aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 308700])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pca_loader)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09ef5dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f5a8ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Fitting PCA Components\n",
      "================================================================================\n",
      "Fitting PCA on 50 samples...\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š Fitting Spectral Layers\n",
      "================================================================================\n",
      "ModalitÃ : LAYER\n",
      "Target layers: [0, 1, 2, 3]\n",
      "Max samples: 50\n",
      "================================================================================\n",
      "\n",
      "ðŸ“¦ Fase 1: Raccolta hidden states...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Raccolta: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:10<00:00,  2.50s/it, samples=50]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Raccolti 50 samples totali\n",
      "\n",
      "================================================================================\n",
      "ðŸ”¬ Fase 2: Calcolo PCA e inizializzazione pesi...\n",
      "================================================================================\n",
      "\n",
      "\n",
      "âœ“ layer_0:\n",
      "  Shape: torch.Size([204800, 96])\n",
      "  Top-5 variance: ['0.2955', '0.0760', '0.0705', '0.0538', '0.0452']\n",
      "  Cumulative top-5: 0.5412\n",
      "  Total explained: 1.0000\n",
      "\n",
      "âœ“ layer_1:\n",
      "  Shape: torch.Size([51200, 192])\n",
      "  Top-5 variance: ['0.1277', '0.0751', '0.0544', '0.0481', '0.0349']\n",
      "  Cumulative top-5: 0.3403\n",
      "  Total explained: 1.0000\n",
      "\n",
      "âœ“ layer_2:\n",
      "  Shape: torch.Size([12800, 384])\n",
      "  Top-5 variance: ['0.0702', '0.0592', '0.0449', '0.0311', '0.0308']\n",
      "  Cumulative top-5: 0.2362\n",
      "  Total explained: 1.0000\n",
      "\n",
      "âœ“ layer_3:\n",
      "  Shape: torch.Size([3200, 768])\n",
      "  Top-5 variance: ['0.1211', '0.0795', '0.0751', '0.0657', '0.0530']\n",
      "  Cumulative top-5: 0.3944\n",
      "  Total explained: 1.0000\n",
      "\n",
      "================================================================================\n",
      "âœ… Fitting completato!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "ðŸ“Š PCA Variance Ratios:\n",
      "   layer_0: Top 5 components = [0.2955418  0.07602653 0.07051117 0.05384455 0.04524239]\n",
      "   layer_1: Top 5 components = [0.12771684 0.07512135 0.05444162 0.0481239  0.034897  ]\n",
      "   layer_2: Top 5 components = [0.07020482 0.05919423 0.04493992 0.03109107 0.03078486]\n",
      "   layer_3: Top 5 components = [0.12111126 0.07946752 0.07508165 0.06570911 0.05304396]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Fitting PCA Components\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Fit PCA\n",
    "print(f\"Fitting PCA on {len(pca_dataset)} samples...\")\n",
    "variance_ratios = clap_residual.clap.audio_encoder.base.htsat.fit_spectral_layers(\n",
    "    pca_loader,\n",
    "    max_samples=50\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š PCA Variance Ratios:\")\n",
    "for layer_name, ratios in variance_ratios.items():\n",
    "    print(f\"   {layer_name}: Top 5 components = {ratios[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7e54bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Text embeddings shape: torch.Size([50, 1024])\n",
      "\n",
      "ðŸ“Š Testing on 100 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c748b9b9e8440e8e2dfceea3e23479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Baseline:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Baseline Accuracy: 0.920 (92.0%)\n"
     ]
    }
   ],
   "source": [
    "# Get text embeddings ONCE for all classes\n",
    "text_embeddings = clap_standard.get_text_embeddings(text_labels)\n",
    "print(f\"   Text embeddings shape: {text_embeddings.shape}\")\n",
    "\n",
    "# Test on subset first (use full dataset later)\n",
    "test_size = 100  # Start with 200 samples for speed\n",
    "print(f\"\\nðŸ“Š Testing on {test_size} samples...\")\n",
    "\n",
    "y_preds_baseline, y_labels = [], []\n",
    "\n",
    "for i in tqdm(range(test_size), desc=\"Baseline\"):\n",
    "    # Get audio file path and label\n",
    "    audio_path, target, one_hot_target = dataset[-(i+1)]\n",
    "    \n",
    "    # Get audio embedding\n",
    "    audio_embedding = clap_standard.get_audio_embeddings([audio_path], resample=True)\n",
    "    \n",
    "    # Compute similarity\n",
    "    similarity = clap_standard.compute_similarity(audio_embedding, text_embeddings)\n",
    "    \n",
    "    # Get prediction\n",
    "    y_pred = F.softmax(similarity.detach().cpu(), dim=1).numpy()\n",
    "    y_preds_baseline.append(y_pred)\n",
    "    y_labels.append(one_hot_target.detach().cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "y_labels_array = np.concatenate(y_labels, axis=0)\n",
    "y_preds_baseline_array = np.concatenate(y_preds_baseline, axis=0)\n",
    "\n",
    "baseline_acc = accuracy_score(\n",
    "    np.argmax(y_labels_array, axis=1), \n",
    "    np.argmax(y_preds_baseline_array, axis=1)\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Baseline Accuracy: {baseline_acc:.3f} ({baseline_acc*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e60c8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Text embeddings shape: torch.Size([50, 1024])\n",
      "\n",
      "ðŸ“Š Testing on 100 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd8a55997934586bb4af78e8c402f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "residual:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Residual Accuracy: 0.920 (92.0%)\n"
     ]
    }
   ],
   "source": [
    "# Get text embeddings ONCE for all classes\n",
    "text_embeddings = clap_residual.get_text_embeddings(text_labels)\n",
    "print(f\"   Text embeddings shape: {text_embeddings.shape}\")\n",
    "\n",
    "# Test on subset first (use full dataset later)\n",
    "test_size = 100  # Start with 200 samples for speed\n",
    "print(f\"\\nðŸ“Š Testing on {test_size} samples...\")\n",
    "\n",
    "y_preds_residual, y_labels = [], []\n",
    "\n",
    "for i in tqdm(range(test_size), desc=\"residual\"):\n",
    "    # Get audio file path and label\n",
    "    audio_path, target, one_hot_target = dataset[-(i+1)]\n",
    "    \n",
    "    # Get audio embedding\n",
    "    audio_embedding = clap_residual.get_audio_embeddings([audio_path], resample=True)\n",
    "    \n",
    "    # Compute similarity\n",
    "    similarity = clap_residual.compute_similarity(audio_embedding, text_embeddings)\n",
    "    \n",
    "    # Get prediction\n",
    "    y_pred = F.softmax(similarity.detach().cpu(), dim=1).numpy()\n",
    "    y_preds_residual.append(y_pred)\n",
    "    y_labels.append(one_hot_target.detach().cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "y_labels_array = np.concatenate(y_labels, axis=0)\n",
    "y_preds_residual_array = np.concatenate(y_preds_residual, axis=0)\n",
    "\n",
    "baseline_acc = accuracy_score(\n",
    "    np.argmax(y_labels_array, axis=1), \n",
    "    np.argmax(y_preds_residual_array, axis=1)\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Residual Accuracy: {baseline_acc:.3f} ({baseline_acc*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "726a89f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path, target, one_hot_target = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d03d337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7623,  0.2343, -0.5101,  ...,  1.8940,  0.9414, -0.0119]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clap_residual.get_audio_embeddings([audio_path], resample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2aa61e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7623,  0.2343, -0.5101,  ...,  1.8940,  0.9414, -0.0119]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clap_standard.get_audio_embeddings([audio_path], resample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c17b4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ResiDual-CLAP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
