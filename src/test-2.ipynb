{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "189dca85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import from your codebase\n",
    "from CLAPWrapper import CLAPWrapper\n",
    "from datasets.esc50 import ESC50\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e48fa266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Loading ESC50 Dataset\n",
      "================================================================================\n",
      "Loading audio files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 18674.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset loaded: 2000 samples\n",
      "   Classes: 50 categories\n",
      "   Sample classes: ['airplane', 'breathing', 'brushing teeth', 'can opening', 'car horn']\n",
      "\n",
      "üìù Text prompts: 50 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Loading ESC50 Dataset\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "root_path = \"./data\"\n",
    "dataset = ESC50(root=root_path, download=True)\n",
    "print(f\"‚úÖ Dataset loaded: {len(dataset)} samples\")\n",
    "print(f\"   Classes: {len(dataset.classes)} categories\")\n",
    "print(f\"   Sample classes: {dataset.classes[:5]}\")\n",
    "\n",
    "# Prepare text prompts\n",
    "prompt = 'this is the sound of '\n",
    "text_labels = [prompt + x for x in dataset.classes]\n",
    "print(f\"\\nüìù Text prompts: {len(text_labels)} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebc7cf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Initializing Models\n",
      "================================================================================\n",
      "\n",
      "üîß Loading CLAP Standard...\n",
      "\n",
      "üîß Loading ResiDualCLAP...\n",
      "[2, 2, 6, 2]\n",
      "üîç Detecting layer dimensions...\n",
      "torch.Size([1, 1, 256, 256]) 1.5)\n",
      "torch.Size([1, 4096, 96]) 2)\n",
      "torch.Size([1, 4096, 96]) 3)\n",
      "torch.Size([1, 1024, 192]) 4)\n",
      "torch.Size([1, 256, 384]) 5)\n",
      "torch.Size([1, 64, 768]) 6)\n",
      "torch.Size([1, 64, 768]) 7)\n",
      "  ‚úì 0: torch.Size([1, 1024, 192])\n",
      "  ‚úì 1: torch.Size([1, 256, 384])\n",
      "  ‚úì 2: torch.Size([1, 64, 768])\n",
      "  ‚úì 3: torch.Size([1, 64, 768])\n",
      "  ‚úì layer_0: 192D ‚Üí 19 PCs\n",
      "  ‚úì layer_1: 384D ‚Üí 38 PCs\n",
      "  ‚úì layer_2: 768D ‚Üí 76 PCs\n",
      "  ‚úì layer_3: 768D ‚Üí 76 PCs\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: Initialize Models\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Initializing Models\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Residual config con pc_weights = 1.0 (identit√†)\n",
    "residual_config = {\n",
    "    'n_components_ratio': .1,\n",
    "    'reweight_factor': 2.0,\n",
    "    'target_layers': [0, 1, 2, 3],  # Layers dove applicare reweighting\n",
    "    'analysis_mode': True\n",
    "}\n",
    "\n",
    "print(\"\\nüîß Loading CLAP Standard...\")\n",
    "clap_standard = CLAPWrapper(\n",
    "    version='2023',  # or '2022'\n",
    "    use_cuda=torch.cuda.is_available(),\n",
    "    type='classic'\n",
    ")\n",
    "\n",
    "print(\"\\nüîß Loading ResiDualCLAP...\")\n",
    "clap_residual = CLAPWrapper(\n",
    "    version='2023',\n",
    "    use_cuda=torch.cuda.is_available(),\n",
    "    type='residual',\n",
    "    residual_config=residual_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e07ed051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting samples for PCA fitting (max 200 samples)...OK\n"
     ]
    }
   ],
   "source": [
    "# Prepare audio samples for PCA fitting\n",
    "print(\"Collecting samples for PCA fitting (max 200 samples)...\", end='')\n",
    "\n",
    "# Create a simple dataloader wrapper per PCA fitting\n",
    "class SimpleAudioDataset:\n",
    "    def __init__(self, wrapper, esc50_dataset, max_samples=1000):\n",
    "        self.wrapper = wrapper\n",
    "        self.audio_paths = []\n",
    "        for i in range(min(max_samples, len(esc50_dataset))):\n",
    "            audio_path, _, _ = esc50_dataset[i]\n",
    "            self.audio_paths.append(audio_path)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_tensor = self.wrapper.load_audio_into_tensor(\n",
    "            self.audio_paths[idx],\n",
    "            self.wrapper.args.duration,\n",
    "            resample=True\n",
    "        )\n",
    "        # ‚úÖ Assicurati sia 1D\n",
    "        if audio_tensor.dim() > 1:\n",
    "            audio_tensor = audio_tensor.squeeze()\n",
    "        \n",
    "        return audio_tensor\n",
    "\n",
    "# Create dataset and loader\n",
    "pca_dataset = SimpleAudioDataset(clap_residual, dataset, max_samples=50)\n",
    "pca_loader = DataLoader(\n",
    "    pca_dataset, \n",
    "    batch_size=16, \n",
    "    shuffle=False,\n",
    "    num_workers=0,  # Start with 0 for debugging\n",
    "    pin_memory=False\n",
    ")\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4e2ba8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([308700])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea6b11aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 308700])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pca_loader)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09ef5dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f5a8ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Fitting PCA Components\n",
      "================================================================================\n",
      "Fitting PCA on 50 samples...\n",
      "\n",
      "================================================================================\n",
      "üîç PHASE 1: Collecting Hidden States from HTSAT Layers\n",
      "================================================================================\n",
      "Target layers: ['layer_0', 'layer_1', 'layer_2', 'layer_3']\n",
      "Max samples to collect: 50\n",
      "Batches in dataloader: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting samples:   0%|                                                  | 0/4 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 256, 256]) 1)\n",
      "torch.Size([16, 1, 256, 256]) 1.5)\n",
      "torch.Size([16, 4096, 96]) 2)\n",
      "torch.Size([16, 4096, 96]) 3)\n",
      "torch.Size([16, 1024, 192]) 4)\n",
      "torch.Size([16, 256, 384]) 5)\n",
      "torch.Size([16, 64, 768]) 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting samples:   0%|                                                  | 0/4 [00:03<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 64, 768]) 7)\n",
      "torch.Size([16, 1024, 192])\n",
      "\n",
      "üìä PCA Variance Ratios:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      7\u001b[39m variance_ratios = clap_residual.clap.audio_encoder.base.htsat.fit_spectral_layers(\n\u001b[32m      8\u001b[39m     pca_loader,\n\u001b[32m      9\u001b[39m     max_samples=\u001b[32m50\u001b[39m\n\u001b[32m     10\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìä PCA Variance Ratios:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer_name, ratios \u001b[38;5;129;01min\u001b[39;00m \u001b[43mvariance_ratios\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m():\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Top 5 components = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mratios[:\u001b[32m5\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'int' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Fitting PCA Components\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Fit PCA\n",
    "print(f\"Fitting PCA on {len(pca_dataset)} samples...\")\n",
    "variance_ratios = clap_residual.clap.audio_encoder.base.htsat.fit_spectral_layers(\n",
    "    pca_loader,\n",
    "    max_samples=50\n",
    ")\n",
    "\n",
    "print(\"\\nüìä PCA Variance Ratios:\")\n",
    "for layer_name, ratios in variance_ratios.items():\n",
    "    print(f\"   {layer_name}: Top 5 components = {ratios[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbbe5fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'collected_outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Dopo aver fatto la collection\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer_name, outputs \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcollected_outputs\u001b[49m.items():\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m outputs:\n\u001b[32m      4\u001b[39m         X = torch.cat(outputs, dim=\u001b[32m0\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'collected_outputs' is not defined"
     ]
    }
   ],
   "source": [
    "# Dopo aver fatto la collection\n",
    "for layer_name, outputs in collected_outputs.items():\n",
    "    if outputs:\n",
    "        X = torch.cat(outputs, dim=0)\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Checking {layer_name}\")\n",
    "        quick_rank_check(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e54bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Text embeddings shape: torch.Size([50, 1024])\n",
      "\n",
      "üìä Testing on 100 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89dcba842e54e83be19d1a5af20655f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Baseline:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Baseline Accuracy: 0.920 (92.0%)\n"
     ]
    }
   ],
   "source": [
    "# Get text embeddings ONCE for all classes\n",
    "text_embeddings = clap_standard.get_text_embeddings(text_labels)\n",
    "print(f\"   Text embeddings shape: {text_embeddings.shape}\")\n",
    "\n",
    "# Test on subset first (use full dataset later)\n",
    "test_size = 100  # Start with 200 samples for speed\n",
    "print(f\"\\nüìä Testing on {test_size} samples...\")\n",
    "\n",
    "y_preds_baseline, y_labels = [], []\n",
    "\n",
    "for i in tqdm(range(test_size), desc=\"Baseline\"):\n",
    "    # Get audio file path and label\n",
    "    audio_path, target, one_hot_target = dataset[-(i+1)]\n",
    "    \n",
    "    # Get audio embedding\n",
    "    audio_embedding = clap_standard.get_audio_embeddings([audio_path], resample=True)\n",
    "    \n",
    "    # Compute similarity\n",
    "    similarity = clap_standard.compute_similarity(audio_embedding, text_embeddings)\n",
    "    \n",
    "    # Get prediction\n",
    "    y_pred = F.softmax(similarity.detach().cpu(), dim=1).numpy()\n",
    "    y_preds_baseline.append(y_pred)\n",
    "    y_labels.append(one_hot_target.detach().cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "y_labels_array = np.concatenate(y_labels, axis=0)\n",
    "y_preds_baseline_array = np.concatenate(y_preds_baseline, axis=0)\n",
    "\n",
    "baseline_acc = accuracy_score(\n",
    "    np.argmax(y_labels_array, axis=1), \n",
    "    np.argmax(y_preds_baseline_array, axis=1)\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Baseline Accuracy: {baseline_acc:.3f} ({baseline_acc*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e60c8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Text embeddings shape: torch.Size([50, 1024])\n",
      "\n",
      "üìä Testing on 100 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "677ec485bce84a958ae0f585bf7381d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "residual:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Residual Accuracy: 0.920 (92.0%)\n"
     ]
    }
   ],
   "source": [
    "# Get text embeddings ONCE for all classes\n",
    "text_embeddings = clap_residual.get_text_embeddings(text_labels)\n",
    "print(f\"   Text embeddings shape: {text_embeddings.shape}\")\n",
    "\n",
    "# Test on subset first (use full dataset later)\n",
    "test_size = 100  # Start with 200 samples for speed\n",
    "print(f\"\\nüìä Testing on {test_size} samples...\")\n",
    "\n",
    "y_preds_residual, y_labels = [], []\n",
    "\n",
    "for i in tqdm(range(test_size), desc=\"residual\"):\n",
    "    # Get audio file path and label\n",
    "    audio_path, target, one_hot_target = dataset[-(i+1)]\n",
    "    \n",
    "    # Get audio embedding\n",
    "    audio_embedding = clap_residual.get_audio_embeddings([audio_path], resample=True)\n",
    "    \n",
    "    # Compute similarity\n",
    "    similarity = clap_residual.compute_similarity(audio_embedding, text_embeddings)\n",
    "    \n",
    "    # Get prediction\n",
    "    y_pred = F.softmax(similarity.detach().cpu(), dim=1).numpy()\n",
    "    y_preds_residual.append(y_pred)\n",
    "    y_labels.append(one_hot_target.detach().cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "y_labels_array = np.concatenate(y_labels, axis=0)\n",
    "y_preds_residual_array = np.concatenate(y_preds_residual, axis=0)\n",
    "\n",
    "baseline_acc = accuracy_score(\n",
    "    np.argmax(y_labels_array, axis=1), \n",
    "    np.argmax(y_preds_baseline_array, axis=1)\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Residual Accuracy: {baseline_acc:.3f} ({baseline_acc*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726a89f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path, target, one_hot_target = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d03d337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8840,  0.3185, -0.6708,  ...,  2.1432,  1.0298, -0.1217]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clap_residual.get_audio_embeddings([audio_path], resample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aa61e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7623,  0.2343, -0.5101,  ...,  1.8940,  0.9414, -0.0119]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clap_standard.get_audio_embeddings([audio_path], resample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ee6592",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ResiDual-CLAP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
