{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189dca85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import from your codebase\n",
    "from CLAPWrapper import CLAPWrapper\n",
    "from datasets.esc50 import ESC50\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48fa266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Loading ESC50 Dataset\n",
      "================================================================================\n",
      "Loading audio files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 15630.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset loaded: 2000 samples\n",
      "   Classes: 50 categories\n",
      "   Sample classes: ['airplane', 'breathing', 'brushing teeth', 'can opening', 'car horn']\n",
      "\n",
      "ðŸ“ Text prompts: 50 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Loading ESC50 Dataset\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "root_path = \"../data\"\n",
    "dataset = ESC50(root=root_path, download=False)\n",
    "print(f\"âœ… Dataset loaded: {len(dataset)} samples\")\n",
    "print(f\"   Classes: {len(dataset.classes)} categories\")\n",
    "print(f\"   Sample classes: {dataset.classes[:5]}\")\n",
    "\n",
    "# Prepare text prompts\n",
    "prompt = 'this is the sound of '\n",
    "text_labels = [prompt + x for x in dataset.classes]\n",
    "print(f\"\\nðŸ“ Text prompts: {len(text_labels)} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc7cf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Initializing Models\n",
      "================================================================================\n",
      "\n",
      "ðŸ”§ Loading CLAP Standard...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”§ Loading ResiDualCLAP...\n",
      "\n",
      "================================================================================\n",
      "ðŸ”§ Setup ResiDual HTSAT\n",
      "================================================================================\n",
      "ModalitÃ : ATTENTION\n",
      "Target layers: [0, 1, 2, 3]\n",
      "PCA components ratio: 1.0\n",
      "Reweight factor: 0.0\n",
      "\n",
      "âœ“ layer_0:\n",
      "  ModalitÃ : PER-HEAD reweighting\n",
      "  Num blocks: 2\n",
      "  Heads per block: 4\n",
      "  Total heads: 8\n",
      "  Head dim: 24D â†’ 24 PCs\n",
      "\n",
      "âœ“ layer_1:\n",
      "  ModalitÃ : PER-HEAD reweighting\n",
      "  Num blocks: 2\n",
      "  Heads per block: 8\n",
      "  Total heads: 16\n",
      "  Head dim: 24D â†’ 24 PCs\n",
      "\n",
      "âœ“ layer_2:\n",
      "  ModalitÃ : PER-HEAD reweighting\n",
      "  Num blocks: 6\n",
      "  Heads per block: 16\n",
      "  Total heads: 96\n",
      "  Head dim: 24D â†’ 24 PCs\n",
      "\n",
      "âœ“ layer_3:\n",
      "  ModalitÃ : PER-HEAD reweighting\n",
      "  Num blocks: 2\n",
      "  Heads per block: 32\n",
      "  Total heads: 64\n",
      "  Head dim: 24D â†’ 24 PCs\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: Initialize Models\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Initializing Models\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Residual config con pc_weights = 1.0 (identitÃ )\n",
    "residual_config = {\n",
    "    'mode': 'attention',\n",
    "    'n_components_ratio': 1.0,\n",
    "    'reweight_factor': 0.0,\n",
    "    'target_layers': [0, 1, 2, 3],  # Layers dove applicare reweighting\n",
    "}\n",
    "\n",
    "print(\"\\nðŸ”§ Loading CLAP Standard...\")\n",
    "clap_standard = CLAPWrapper(\n",
    "    version='2023',  # or '2022'\n",
    "    use_cuda=torch.cuda.is_available(),\n",
    "    type='classic'\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ”§ Loading ResiDualCLAP...\")\n",
    "clap_residual = CLAPWrapper(\n",
    "    version='2023',\n",
    "    use_cuda=torch.cuda.is_available(),\n",
    "    type='residual',\n",
    "    residual_config=residual_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07ed051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting samples for PCA fitting (max 200 samples)...OK\n"
     ]
    }
   ],
   "source": [
    "# Prepare audio samples for PCA fitting\n",
    "print(\"Collecting samples for PCA fitting (max 200 samples)...\", end='')\n",
    "\n",
    "# Create a simple dataloader wrapper per PCA fitting\n",
    "class SimpleAudioDataset:\n",
    "    def __init__(self, wrapper, esc50_dataset, max_samples=1000):\n",
    "        self.wrapper = wrapper\n",
    "        self.audio_paths = []\n",
    "        for i in range(min(max_samples, len(esc50_dataset))):\n",
    "            audio_path, _, _ = esc50_dataset[i]\n",
    "            self.audio_paths.append(audio_path)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_tensor = self.wrapper.load_audio_into_tensor(\n",
    "            self.audio_paths[idx],\n",
    "            self.wrapper.args.duration,\n",
    "            resample=True\n",
    "        )\n",
    "        # âœ… Assicurati sia 1D\n",
    "        if audio_tensor.dim() > 1:\n",
    "            audio_tensor = audio_tensor.squeeze()\n",
    "        \n",
    "        return audio_tensor\n",
    "\n",
    "# Create dataset and loader\n",
    "pca_dataset = SimpleAudioDataset(clap_residual, dataset, max_samples=50)\n",
    "pca_loader = DataLoader(\n",
    "    pca_dataset, \n",
    "    batch_size=16, \n",
    "    shuffle=False,\n",
    "    num_workers=0,  # Start with 0 for debugging\n",
    "    pin_memory=False\n",
    ")\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e2ba8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([308700])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_dataset[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6b11aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 308700])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pca_loader)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ef5dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5a8ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Fitting PCA Components\n",
      "================================================================================\n",
      "Fitting PCA on 50 samples...\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š Fitting Spectral Layers\n",
      "================================================================================\n",
      "ModalitÃ : ATTENTION\n",
      "Target layers: [0, 1, 2, 3]\n",
      "Max samples: 50\n",
      "================================================================================\n",
      "\n",
      "ðŸ“¦ Fase 1: Raccolta hidden states...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Raccolta:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” DEBUG: Processing layer_0 in ATTENTION mode with fitting\n",
      "  Block 0: input shape = torch.Size([16, 4096, 96])\n",
      "    After norm1: torch.Size([16, 4096, 96]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 64, 64, 96])\n",
      "    After shift: torch.Size([16, 64, 64, 96])\n",
      "    After window_partition: torch.Size([1024, 8, 8, 96]), is_contiguous=True\n",
      "    After view for attention: torch.Size([1024, 64, 96])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1024, 64, 96]), collect=True\n",
      "      After QKV: q=torch.Size([1024, 4, 64, 24]), k=torch.Size([1024, 4, 64, 24]), v=torch.Size([1024, 4, 64, 24])\n",
      "      After attn scores: torch.Size([1024, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1024, 4, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([1024, 4, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([1024, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([65536, 24])\n",
      "        Head 1: shape=torch.Size([1024, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([65536, 24])\n",
      "        Head 2: shape=torch.Size([1024, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([65536, 24])\n",
      "        Head 3: shape=torch.Size([1024, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([65536, 24])\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([1024, 64, 96])\n",
      "    After attention: torch.Size([1024, 64, 96])\n",
      "    After view for reverse: torch.Size([1024, 8, 8, 96])\n",
      "    After window_reverse: torch.Size([16, 64, 64, 96])\n",
      "    After view back to 3D: torch.Size([16, 4096, 96])\n",
      "    After FFN: torch.Size([16, 4096, 96])\n",
      "  Block 1: input shape = torch.Size([16, 4096, 96])\n",
      "    After norm1: torch.Size([16, 4096, 96]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 64, 64, 96])\n",
      "    After shift: torch.Size([16, 64, 64, 96])\n",
      "    After window_partition: torch.Size([1024, 8, 8, 96]), is_contiguous=True\n",
      "    After view for attention: torch.Size([1024, 64, 96])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1024, 64, 96]), collect=True\n",
      "      After QKV: q=torch.Size([1024, 4, 64, 24]), k=torch.Size([1024, 4, 64, 24]), v=torch.Size([1024, 4, 64, 24])\n",
      "      After attn scores: torch.Size([1024, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1024, 4, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([1024, 4, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([1024, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([65536, 24])\n",
      "        Head 1: shape=torch.Size([1024, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([65536, 24])\n",
      "        Head 2: shape=torch.Size([1024, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([65536, 24])\n",
      "        Head 3: shape=torch.Size([1024, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([65536, 24])\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([1024, 64, 96])\n",
      "    After attention: torch.Size([1024, 64, 96])\n",
      "    After view for reverse: torch.Size([1024, 8, 8, 96])\n",
      "    After window_reverse: torch.Size([16, 64, 64, 96])\n",
      "    After view back to 3D: torch.Size([16, 4096, 96])\n",
      "    After FFN: torch.Size([16, 4096, 96])\n",
      "  After downsampling: torch.Size([16, 1024, 192])\n",
      "\n",
      "ðŸ” DEBUG: Processing layer_1 in ATTENTION mode with fitting\n",
      "  Block 0: input shape = torch.Size([16, 1024, 192])\n",
      "    After norm1: torch.Size([16, 1024, 192]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 32, 32, 192])\n",
      "    After shift: torch.Size([16, 32, 32, 192])\n",
      "    After window_partition: torch.Size([256, 8, 8, 192]), is_contiguous=True\n",
      "    After view for attention: torch.Size([256, 64, 192])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([256, 64, 192]), collect=True\n",
      "      After QKV: q=torch.Size([256, 8, 64, 24]), k=torch.Size([256, 8, 64, 24]), v=torch.Size([256, 8, 64, 24])\n",
      "      After attn scores: torch.Size([256, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([256, 8, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([256, 8, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 1: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 2: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 3: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 4: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 5: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 6: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 7: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([256, 64, 192])\n",
      "    After attention: torch.Size([256, 64, 192])\n",
      "    After view for reverse: torch.Size([256, 8, 8, 192])\n",
      "    After window_reverse: torch.Size([16, 32, 32, 192])\n",
      "    After view back to 3D: torch.Size([16, 1024, 192])\n",
      "    After FFN: torch.Size([16, 1024, 192])\n",
      "  Block 1: input shape = torch.Size([16, 1024, 192])\n",
      "    After norm1: torch.Size([16, 1024, 192]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 32, 32, 192])\n",
      "    After shift: torch.Size([16, 32, 32, 192])\n",
      "    After window_partition: torch.Size([256, 8, 8, 192]), is_contiguous=True\n",
      "    After view for attention: torch.Size([256, 64, 192])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([256, 64, 192]), collect=True\n",
      "      After QKV: q=torch.Size([256, 8, 64, 24]), k=torch.Size([256, 8, 64, 24]), v=torch.Size([256, 8, 64, 24])\n",
      "      After attn scores: torch.Size([256, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([256, 8, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([256, 8, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 1: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 2: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 3: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 4: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 5: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 6: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 7: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([256, 64, 192])\n",
      "    After attention: torch.Size([256, 64, 192])\n",
      "    After view for reverse: torch.Size([256, 8, 8, 192])\n",
      "    After window_reverse: torch.Size([16, 32, 32, 192])\n",
      "    After view back to 3D: torch.Size([16, 1024, 192])\n",
      "    After FFN: torch.Size([16, 1024, 192])\n",
      "  After downsampling: torch.Size([16, 256, 384])\n",
      "\n",
      "ðŸ” DEBUG: Processing layer_2 in ATTENTION mode with fitting\n",
      "  Block 0: input shape = torch.Size([16, 256, 384])\n",
      "    After norm1: torch.Size([16, 256, 384]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 16, 16, 384])\n",
      "    After shift: torch.Size([16, 16, 16, 384])\n",
      "    After window_partition: torch.Size([64, 8, 8, 384]), is_contiguous=True\n",
      "    After view for attention: torch.Size([64, 64, 384])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 384]), collect=True\n",
      "      After QKV: q=torch.Size([64, 16, 64, 24]), k=torch.Size([64, 16, 64, 24]), v=torch.Size([64, 16, 64, 24])\n",
      "      After attn scores: torch.Size([64, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 1: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 2: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 3: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 4: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 5: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 6: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 7: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 8: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 9: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 10: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 11: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 12: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 13: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 14: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 15: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([64, 64, 384])\n",
      "    After attention: torch.Size([64, 64, 384])\n",
      "    After view for reverse: torch.Size([64, 8, 8, 384])\n",
      "    After window_reverse: torch.Size([16, 16, 16, 384])\n",
      "    After view back to 3D: torch.Size([16, 256, 384])\n",
      "    After FFN: torch.Size([16, 256, 384])\n",
      "  Block 1: input shape = torch.Size([16, 256, 384])\n",
      "    After norm1: torch.Size([16, 256, 384]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 16, 16, 384])\n",
      "    After shift: torch.Size([16, 16, 16, 384])\n",
      "    After window_partition: torch.Size([64, 8, 8, 384]), is_contiguous=True\n",
      "    After view for attention: torch.Size([64, 64, 384])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 384]), collect=True\n",
      "      After QKV: q=torch.Size([64, 16, 64, 24]), k=torch.Size([64, 16, 64, 24]), v=torch.Size([64, 16, 64, 24])\n",
      "      After attn scores: torch.Size([64, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 1: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 2: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 3: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 4: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 5: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 6: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 7: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 8: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 9: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 10: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 11: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 12: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 13: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 14: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 15: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([64, 64, 384])\n",
      "    After attention: torch.Size([64, 64, 384])\n",
      "    After view for reverse: torch.Size([64, 8, 8, 384])\n",
      "    After window_reverse: torch.Size([16, 16, 16, 384])\n",
      "    After view back to 3D: torch.Size([16, 256, 384])\n",
      "    After FFN: torch.Size([16, 256, 384])\n",
      "  Block 2: input shape = torch.Size([16, 256, 384])\n",
      "    After norm1: torch.Size([16, 256, 384]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 16, 16, 384])\n",
      "    After shift: torch.Size([16, 16, 16, 384])\n",
      "    After window_partition: torch.Size([64, 8, 8, 384]), is_contiguous=True\n",
      "    After view for attention: torch.Size([64, 64, 384])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 384]), collect=True\n",
      "      After QKV: q=torch.Size([64, 16, 64, 24]), k=torch.Size([64, 16, 64, 24]), v=torch.Size([64, 16, 64, 24])\n",
      "      After attn scores: torch.Size([64, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 1: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 2: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 3: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 4: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 5: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 6: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 7: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 8: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 9: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 10: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 11: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 12: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 13: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 14: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 15: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([64, 64, 384])\n",
      "    After attention: torch.Size([64, 64, 384])\n",
      "    After view for reverse: torch.Size([64, 8, 8, 384])\n",
      "    After window_reverse: torch.Size([16, 16, 16, 384])\n",
      "    After view back to 3D: torch.Size([16, 256, 384])\n",
      "    After FFN: torch.Size([16, 256, 384])\n",
      "  Block 3: input shape = torch.Size([16, 256, 384])\n",
      "    After norm1: torch.Size([16, 256, 384]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 16, 16, 384])\n",
      "    After shift: torch.Size([16, 16, 16, 384])\n",
      "    After window_partition: torch.Size([64, 8, 8, 384]), is_contiguous=True\n",
      "    After view for attention: torch.Size([64, 64, 384])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 384]), collect=True\n",
      "      After QKV: q=torch.Size([64, 16, 64, 24]), k=torch.Size([64, 16, 64, 24]), v=torch.Size([64, 16, 64, 24])\n",
      "      After attn scores: torch.Size([64, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 1: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 2: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 3: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 4: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 5: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 6: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 7: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 8: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 9: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 10: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 11: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 12: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 13: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 14: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 15: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([64, 64, 384])\n",
      "    After attention: torch.Size([64, 64, 384])\n",
      "    After view for reverse: torch.Size([64, 8, 8, 384])\n",
      "    After window_reverse: torch.Size([16, 16, 16, 384])\n",
      "    After view back to 3D: torch.Size([16, 256, 384])\n",
      "    After FFN: torch.Size([16, 256, 384])\n",
      "  Block 4: input shape = torch.Size([16, 256, 384])\n",
      "    After norm1: torch.Size([16, 256, 384]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 16, 16, 384])\n",
      "    After shift: torch.Size([16, 16, 16, 384])\n",
      "    After window_partition: torch.Size([64, 8, 8, 384]), is_contiguous=True\n",
      "    After view for attention: torch.Size([64, 64, 384])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 384]), collect=True\n",
      "      After QKV: q=torch.Size([64, 16, 64, 24]), k=torch.Size([64, 16, 64, 24]), v=torch.Size([64, 16, 64, 24])\n",
      "      After attn scores: torch.Size([64, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 1: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 2: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 3: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 4: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 5: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 6: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 7: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 8: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 9: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 10: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 11: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 12: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 13: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 14: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 15: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([64, 64, 384])\n",
      "    After attention: torch.Size([64, 64, 384])\n",
      "    After view for reverse: torch.Size([64, 8, 8, 384])\n",
      "    After window_reverse: torch.Size([16, 16, 16, 384])\n",
      "    After view back to 3D: torch.Size([16, 256, 384])\n",
      "    After FFN: torch.Size([16, 256, 384])\n",
      "  Block 5: input shape = torch.Size([16, 256, 384])\n",
      "    After norm1: torch.Size([16, 256, 384]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 16, 16, 384])\n",
      "    After shift: torch.Size([16, 16, 16, 384])\n",
      "    After window_partition: torch.Size([64, 8, 8, 384]), is_contiguous=True\n",
      "    After view for attention: torch.Size([64, 64, 384])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 384]), collect=True\n",
      "      After QKV: q=torch.Size([64, 16, 64, 24]), k=torch.Size([64, 16, 64, 24]), v=torch.Size([64, 16, 64, 24])\n",
      "      After attn scores: torch.Size([64, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 1: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 2: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 3: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 4: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 5: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 6: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 7: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 8: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 9: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 10: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 11: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 12: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 13: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 14: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 15: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([64, 64, 384])\n",
      "    After attention: torch.Size([64, 64, 384])\n",
      "    After view for reverse: torch.Size([64, 8, 8, 384])\n",
      "    After window_reverse: torch.Size([16, 16, 16, 384])\n",
      "    After view back to 3D: torch.Size([16, 256, 384])\n",
      "    After FFN: torch.Size([16, 256, 384])\n",
      "  After downsampling: torch.Size([16, 64, 768])\n",
      "\n",
      "ðŸ” DEBUG: Processing layer_3 in ATTENTION mode with fitting\n",
      "  Block 0: input shape = torch.Size([16, 64, 768])\n",
      "    After norm1: torch.Size([16, 64, 768]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 8, 8, 768])\n",
      "    After shift: torch.Size([16, 8, 8, 768])\n",
      "    After window_partition: torch.Size([16, 8, 8, 768]), is_contiguous=True\n",
      "    After view for attention: torch.Size([16, 64, 768])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 768]), collect=True\n",
      "      After QKV: q=torch.Size([16, 32, 64, 24]), k=torch.Size([16, 32, 64, 24]), v=torch.Size([16, 32, 64, 24])\n",
      "      After attn scores: torch.Size([16, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 32, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([16, 32, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 1: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 2: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 3: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 4: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 5: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 6: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 7: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 8: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 9: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 10: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 11: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 12: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 13: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 14: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 15: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 16: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 17: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 18: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 19: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 20: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 21: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 22: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 23: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 24: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 25: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 26: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 27: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 28: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 29: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 30: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 31: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([16, 64, 768])\n",
      "    After attention: torch.Size([16, 64, 768])\n",
      "    After view for reverse: torch.Size([16, 8, 8, 768])\n",
      "    After window_reverse: torch.Size([16, 8, 8, 768])\n",
      "    After view back to 3D: torch.Size([16, 64, 768])\n",
      "    After FFN: torch.Size([16, 64, 768])\n",
      "  Block 1: input shape = torch.Size([16, 64, 768])\n",
      "    After norm1: torch.Size([16, 64, 768]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 8, 8, 768])\n",
      "    After shift: torch.Size([16, 8, 8, 768])\n",
      "    After window_partition: torch.Size([16, 8, 8, 768]), is_contiguous=True\n",
      "    After view for attention: torch.Size([16, 64, 768])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 768]), collect=True\n",
      "      After QKV: q=torch.Size([16, 32, 64, 24]), k=torch.Size([16, 32, 64, 24]), v=torch.Size([16, 32, 64, 24])\n",
      "      After attn scores: torch.Size([16, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 32, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([16, 32, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 1: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 2: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 3: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 4: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 5: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 6: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 7: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 8: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 9: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 10: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 11: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 12: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 13: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 14: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 15: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 16: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 17: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 18: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 19: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 20: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 21: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 22: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 23: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 24: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 25: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 26: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 27: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 28: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 29: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 30: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 31: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([16, 64, 768])\n",
      "    After attention: torch.Size([16, 64, 768])\n",
      "    After view for reverse: torch.Size([16, 8, 8, 768])\n",
      "    After window_reverse: torch.Size([16, 8, 8, 768])\n",
      "    After view back to 3D: torch.Size([16, 64, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Raccolta:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:03<00:10,  3.42s/it, samples=16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    After FFN: torch.Size([16, 64, 768])\n",
      "\n",
      "ðŸ” DEBUG: Processing layer_0 in ATTENTION mode with fitting\n",
      "  Block 0: input shape = torch.Size([16, 4096, 96])\n",
      "    After norm1: torch.Size([16, 4096, 96]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 64, 64, 96])\n",
      "    After shift: torch.Size([16, 64, 64, 96])\n",
      "    After window_partition: torch.Size([1024, 8, 8, 96]), is_contiguous=True\n",
      "    After view for attention: torch.Size([1024, 64, 96])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1024, 64, 96]), collect=True\n",
      "      After QKV: q=torch.Size([1024, 4, 64, 24]), k=torch.Size([1024, 4, 64, 24]), v=torch.Size([1024, 4, 64, 24])\n",
      "      After attn scores: torch.Size([1024, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1024, 4, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([1024, 4, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([1024, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([65536, 24])\n",
      "        Head 1: shape=torch.Size([1024, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([65536, 24])\n",
      "        Head 2: shape=torch.Size([1024, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([65536, 24])\n",
      "        Head 3: shape=torch.Size([1024, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([65536, 24])\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([1024, 64, 96])\n",
      "    After attention: torch.Size([1024, 64, 96])\n",
      "    After view for reverse: torch.Size([1024, 8, 8, 96])\n",
      "    After window_reverse: torch.Size([16, 64, 64, 96])\n",
      "    After view back to 3D: torch.Size([16, 4096, 96])\n",
      "    After FFN: torch.Size([16, 4096, 96])\n",
      "  Block 1: input shape = torch.Size([16, 4096, 96])\n",
      "    After norm1: torch.Size([16, 4096, 96]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 64, 64, 96])\n",
      "    After shift: torch.Size([16, 64, 64, 96])\n",
      "    After window_partition: torch.Size([1024, 8, 8, 96]), is_contiguous=True\n",
      "    After view for attention: torch.Size([1024, 64, 96])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1024, 64, 96]), collect=True\n",
      "      After QKV: q=torch.Size([1024, 4, 64, 24]), k=torch.Size([1024, 4, 64, 24]), v=torch.Size([1024, 4, 64, 24])\n",
      "      After attn scores: torch.Size([1024, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1024, 4, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([1024, 4, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([1024, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([65536, 24])\n",
      "        Head 1: shape=torch.Size([1024, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([65536, 24])\n",
      "        Head 2: shape=torch.Size([1024, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([65536, 24])\n",
      "        Head 3: shape=torch.Size([1024, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([65536, 24])\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([1024, 64, 96])\n",
      "    After attention: torch.Size([1024, 64, 96])\n",
      "    After view for reverse: torch.Size([1024, 8, 8, 96])\n",
      "    After window_reverse: torch.Size([16, 64, 64, 96])\n",
      "    After view back to 3D: torch.Size([16, 4096, 96])\n",
      "    After FFN: torch.Size([16, 4096, 96])\n",
      "  After downsampling: torch.Size([16, 1024, 192])\n",
      "\n",
      "ðŸ” DEBUG: Processing layer_1 in ATTENTION mode with fitting\n",
      "  Block 0: input shape = torch.Size([16, 1024, 192])\n",
      "    After norm1: torch.Size([16, 1024, 192]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 32, 32, 192])\n",
      "    After shift: torch.Size([16, 32, 32, 192])\n",
      "    After window_partition: torch.Size([256, 8, 8, 192]), is_contiguous=True\n",
      "    After view for attention: torch.Size([256, 64, 192])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([256, 64, 192]), collect=True\n",
      "      After QKV: q=torch.Size([256, 8, 64, 24]), k=torch.Size([256, 8, 64, 24]), v=torch.Size([256, 8, 64, 24])\n",
      "      After attn scores: torch.Size([256, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([256, 8, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([256, 8, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 1: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 2: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 3: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 4: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 5: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 6: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 7: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([256, 64, 192])\n",
      "    After attention: torch.Size([256, 64, 192])\n",
      "    After view for reverse: torch.Size([256, 8, 8, 192])\n",
      "    After window_reverse: torch.Size([16, 32, 32, 192])\n",
      "    After view back to 3D: torch.Size([16, 1024, 192])\n",
      "    After FFN: torch.Size([16, 1024, 192])\n",
      "  Block 1: input shape = torch.Size([16, 1024, 192])\n",
      "    After norm1: torch.Size([16, 1024, 192]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 32, 32, 192])\n",
      "    After shift: torch.Size([16, 32, 32, 192])\n",
      "    After window_partition: torch.Size([256, 8, 8, 192]), is_contiguous=True\n",
      "    After view for attention: torch.Size([256, 64, 192])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([256, 64, 192]), collect=True\n",
      "      After QKV: q=torch.Size([256, 8, 64, 24]), k=torch.Size([256, 8, 64, 24]), v=torch.Size([256, 8, 64, 24])\n",
      "      After attn scores: torch.Size([256, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([256, 8, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([256, 8, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 1: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 2: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 3: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 4: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 5: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 6: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 7: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([256, 64, 192])\n",
      "    After attention: torch.Size([256, 64, 192])\n",
      "    After view for reverse: torch.Size([256, 8, 8, 192])\n",
      "    After window_reverse: torch.Size([16, 32, 32, 192])\n",
      "    After view back to 3D: torch.Size([16, 1024, 192])\n",
      "    After FFN: torch.Size([16, 1024, 192])\n",
      "  After downsampling: torch.Size([16, 256, 384])\n",
      "\n",
      "ðŸ” DEBUG: Processing layer_2 in ATTENTION mode with fitting\n",
      "  Block 0: input shape = torch.Size([16, 256, 384])\n",
      "    After norm1: torch.Size([16, 256, 384]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 16, 16, 384])\n",
      "    After shift: torch.Size([16, 16, 16, 384])\n",
      "    After window_partition: torch.Size([64, 8, 8, 384]), is_contiguous=True\n",
      "    After view for attention: torch.Size([64, 64, 384])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 384]), collect=True\n",
      "      After QKV: q=torch.Size([64, 16, 64, 24]), k=torch.Size([64, 16, 64, 24]), v=torch.Size([64, 16, 64, 24])\n",
      "      After attn scores: torch.Size([64, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 1: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 2: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 3: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 4: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 5: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 6: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 7: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 8: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 9: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 10: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 11: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 12: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 13: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 14: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 15: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([64, 64, 384])\n",
      "    After attention: torch.Size([64, 64, 384])\n",
      "    After view for reverse: torch.Size([64, 8, 8, 384])\n",
      "    After window_reverse: torch.Size([16, 16, 16, 384])\n",
      "    After view back to 3D: torch.Size([16, 256, 384])\n",
      "    After FFN: torch.Size([16, 256, 384])\n",
      "  Block 1: input shape = torch.Size([16, 256, 384])\n",
      "    After norm1: torch.Size([16, 256, 384]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 16, 16, 384])\n",
      "    After shift: torch.Size([16, 16, 16, 384])\n",
      "    After window_partition: torch.Size([64, 8, 8, 384]), is_contiguous=True\n",
      "    After view for attention: torch.Size([64, 64, 384])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 384]), collect=True\n",
      "      After QKV: q=torch.Size([64, 16, 64, 24]), k=torch.Size([64, 16, 64, 24]), v=torch.Size([64, 16, 64, 24])\n",
      "      After attn scores: torch.Size([64, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 1: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 2: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 3: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 4: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 5: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 6: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 7: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 8: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 9: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 10: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 11: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 12: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 13: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 14: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 15: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([64, 64, 384])\n",
      "    After attention: torch.Size([64, 64, 384])\n",
      "    After view for reverse: torch.Size([64, 8, 8, 384])\n",
      "    After window_reverse: torch.Size([16, 16, 16, 384])\n",
      "    After view back to 3D: torch.Size([16, 256, 384])\n",
      "    After FFN: torch.Size([16, 256, 384])\n",
      "  Block 2: input shape = torch.Size([16, 256, 384])\n",
      "    After norm1: torch.Size([16, 256, 384]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 16, 16, 384])\n",
      "    After shift: torch.Size([16, 16, 16, 384])\n",
      "    After window_partition: torch.Size([64, 8, 8, 384]), is_contiguous=True\n",
      "    After view for attention: torch.Size([64, 64, 384])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 384]), collect=True\n",
      "      After QKV: q=torch.Size([64, 16, 64, 24]), k=torch.Size([64, 16, 64, 24]), v=torch.Size([64, 16, 64, 24])\n",
      "      After attn scores: torch.Size([64, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 1: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 2: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 3: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 4: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 5: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 6: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 7: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 8: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 9: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 10: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 11: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 12: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 13: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 14: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 15: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([64, 64, 384])\n",
      "    After attention: torch.Size([64, 64, 384])\n",
      "    After view for reverse: torch.Size([64, 8, 8, 384])\n",
      "    After window_reverse: torch.Size([16, 16, 16, 384])\n",
      "    After view back to 3D: torch.Size([16, 256, 384])\n",
      "    After FFN: torch.Size([16, 256, 384])\n",
      "  Block 3: input shape = torch.Size([16, 256, 384])\n",
      "    After norm1: torch.Size([16, 256, 384]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 16, 16, 384])\n",
      "    After shift: torch.Size([16, 16, 16, 384])\n",
      "    After window_partition: torch.Size([64, 8, 8, 384]), is_contiguous=True\n",
      "    After view for attention: torch.Size([64, 64, 384])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 384]), collect=True\n",
      "      After QKV: q=torch.Size([64, 16, 64, 24]), k=torch.Size([64, 16, 64, 24]), v=torch.Size([64, 16, 64, 24])\n",
      "      After attn scores: torch.Size([64, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 1: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 2: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 3: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 4: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 5: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 6: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 7: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 8: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 9: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 10: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 11: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 12: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 13: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 14: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 15: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([64, 64, 384])\n",
      "    After attention: torch.Size([64, 64, 384])\n",
      "    After view for reverse: torch.Size([64, 8, 8, 384])\n",
      "    After window_reverse: torch.Size([16, 16, 16, 384])\n",
      "    After view back to 3D: torch.Size([16, 256, 384])\n",
      "    After FFN: torch.Size([16, 256, 384])\n",
      "  Block 4: input shape = torch.Size([16, 256, 384])\n",
      "    After norm1: torch.Size([16, 256, 384]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 16, 16, 384])\n",
      "    After shift: torch.Size([16, 16, 16, 384])\n",
      "    After window_partition: torch.Size([64, 8, 8, 384]), is_contiguous=True\n",
      "    After view for attention: torch.Size([64, 64, 384])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 384]), collect=True\n",
      "      After QKV: q=torch.Size([64, 16, 64, 24]), k=torch.Size([64, 16, 64, 24]), v=torch.Size([64, 16, 64, 24])\n",
      "      After attn scores: torch.Size([64, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 1: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 2: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 3: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 4: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 5: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 6: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 7: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 8: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 9: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 10: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 11: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 12: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 13: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 14: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 15: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([64, 64, 384])\n",
      "    After attention: torch.Size([64, 64, 384])\n",
      "    After view for reverse: torch.Size([64, 8, 8, 384])\n",
      "    After window_reverse: torch.Size([16, 16, 16, 384])\n",
      "    After view back to 3D: torch.Size([16, 256, 384])\n",
      "    After FFN: torch.Size([16, 256, 384])\n",
      "  Block 5: input shape = torch.Size([16, 256, 384])\n",
      "    After norm1: torch.Size([16, 256, 384]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 16, 16, 384])\n",
      "    After shift: torch.Size([16, 16, 16, 384])\n",
      "    After window_partition: torch.Size([64, 8, 8, 384]), is_contiguous=True\n",
      "    After view for attention: torch.Size([64, 64, 384])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 384]), collect=True\n",
      "      After QKV: q=torch.Size([64, 16, 64, 24]), k=torch.Size([64, 16, 64, 24]), v=torch.Size([64, 16, 64, 24])\n",
      "      After attn scores: torch.Size([64, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 1: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 2: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 3: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 4: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 5: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 6: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 7: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 8: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 9: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 10: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 11: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 12: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 13: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 14: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 15: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([64, 64, 384])\n",
      "    After attention: torch.Size([64, 64, 384])\n",
      "    After view for reverse: torch.Size([64, 8, 8, 384])\n",
      "    After window_reverse: torch.Size([16, 16, 16, 384])\n",
      "    After view back to 3D: torch.Size([16, 256, 384])\n",
      "    After FFN: torch.Size([16, 256, 384])\n",
      "  After downsampling: torch.Size([16, 64, 768])\n",
      "\n",
      "ðŸ” DEBUG: Processing layer_3 in ATTENTION mode with fitting\n",
      "  Block 0: input shape = torch.Size([16, 64, 768])\n",
      "    After norm1: torch.Size([16, 64, 768]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 8, 8, 768])\n",
      "    After shift: torch.Size([16, 8, 8, 768])\n",
      "    After window_partition: torch.Size([16, 8, 8, 768]), is_contiguous=True\n",
      "    After view for attention: torch.Size([16, 64, 768])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 768]), collect=True\n",
      "      After QKV: q=torch.Size([16, 32, 64, 24]), k=torch.Size([16, 32, 64, 24]), v=torch.Size([16, 32, 64, 24])\n",
      "      After attn scores: torch.Size([16, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 32, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([16, 32, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 1: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 2: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 3: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 4: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 5: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 6: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 7: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 8: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 9: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 10: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 11: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 12: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 13: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 14: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 15: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 16: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 17: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 18: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 19: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 20: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 21: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 22: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 23: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 24: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 25: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 26: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 27: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 28: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 29: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 30: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 31: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([16, 64, 768])\n",
      "    After attention: torch.Size([16, 64, 768])\n",
      "    After view for reverse: torch.Size([16, 8, 8, 768])\n",
      "    After window_reverse: torch.Size([16, 8, 8, 768])\n",
      "    After view back to 3D: torch.Size([16, 64, 768])\n",
      "    After FFN: torch.Size([16, 64, 768])\n",
      "  Block 1: input shape = torch.Size([16, 64, 768])\n",
      "    After norm1: torch.Size([16, 64, 768]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 8, 8, 768])\n",
      "    After shift: torch.Size([16, 8, 8, 768])\n",
      "    After window_partition: torch.Size([16, 8, 8, 768]), is_contiguous=True\n",
      "    After view for attention: torch.Size([16, 64, 768])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 768]), collect=True\n",
      "      After QKV: q=torch.Size([16, 32, 64, 24]), k=torch.Size([16, 32, 64, 24]), v=torch.Size([16, 32, 64, 24])\n",
      "      After attn scores: torch.Size([16, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 32, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([16, 32, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 1: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 2: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 3: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 4: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 5: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 6: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 7: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 8: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 9: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 10: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 11: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 12: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 13: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 14: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 15: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 16: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 17: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 18: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 19: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 20: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 21: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 22: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 23: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 24: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 25: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 26: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 27: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 28: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 29: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 30: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 31: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([16, 64, 768])\n",
      "    After attention: torch.Size([16, 64, 768])\n",
      "    After view for reverse: torch.Size([16, 8, 8, 768])\n",
      "    After window_reverse: torch.Size([16, 8, 8, 768])\n",
      "    After view back to 3D: torch.Size([16, 64, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Raccolta:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:06<00:06,  3.39s/it, samples=32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    After FFN: torch.Size([16, 64, 768])\n",
      "\n",
      "ðŸ” DEBUG: Processing layer_0 in ATTENTION mode with fitting\n",
      "  Block 0: input shape = torch.Size([16, 4096, 96])\n",
      "    After norm1: torch.Size([16, 4096, 96]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 64, 64, 96])\n",
      "    After shift: torch.Size([16, 64, 64, 96])\n",
      "    After window_partition: torch.Size([1024, 8, 8, 96]), is_contiguous=True\n",
      "    After view for attention: torch.Size([1024, 64, 96])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1024, 64, 96]), collect=True\n",
      "      After QKV: q=torch.Size([1024, 4, 64, 24]), k=torch.Size([1024, 4, 64, 24]), v=torch.Size([1024, 4, 64, 24])\n",
      "      After attn scores: torch.Size([1024, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1024, 4, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([1024, 4, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([1024, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([65536, 24])\n",
      "        Head 1: shape=torch.Size([1024, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([65536, 24])\n",
      "        Head 2: shape=torch.Size([1024, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([65536, 24])\n",
      "        Head 3: shape=torch.Size([1024, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([65536, 24])\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([1024, 64, 96])\n",
      "    After attention: torch.Size([1024, 64, 96])\n",
      "    After view for reverse: torch.Size([1024, 8, 8, 96])\n",
      "    After window_reverse: torch.Size([16, 64, 64, 96])\n",
      "    After view back to 3D: torch.Size([16, 4096, 96])\n",
      "    After FFN: torch.Size([16, 4096, 96])\n",
      "  Block 1: input shape = torch.Size([16, 4096, 96])\n",
      "    After norm1: torch.Size([16, 4096, 96]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 64, 64, 96])\n",
      "    After shift: torch.Size([16, 64, 64, 96])\n",
      "    After window_partition: torch.Size([1024, 8, 8, 96]), is_contiguous=True\n",
      "    After view for attention: torch.Size([1024, 64, 96])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1024, 64, 96]), collect=True\n",
      "      After QKV: q=torch.Size([1024, 4, 64, 24]), k=torch.Size([1024, 4, 64, 24]), v=torch.Size([1024, 4, 64, 24])\n",
      "      After attn scores: torch.Size([1024, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1024, 4, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([1024, 4, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([1024, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([65536, 24])\n",
      "        Head 1: shape=torch.Size([1024, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([65536, 24])\n",
      "        Head 2: shape=torch.Size([1024, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([65536, 24])\n",
      "        Head 3: shape=torch.Size([1024, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([65536, 24])\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([1024, 64, 96])\n",
      "    After attention: torch.Size([1024, 64, 96])\n",
      "    After view for reverse: torch.Size([1024, 8, 8, 96])\n",
      "    After window_reverse: torch.Size([16, 64, 64, 96])\n",
      "    After view back to 3D: torch.Size([16, 4096, 96])\n",
      "    After FFN: torch.Size([16, 4096, 96])\n",
      "  After downsampling: torch.Size([16, 1024, 192])\n",
      "\n",
      "ðŸ” DEBUG: Processing layer_1 in ATTENTION mode with fitting\n",
      "  Block 0: input shape = torch.Size([16, 1024, 192])\n",
      "    After norm1: torch.Size([16, 1024, 192]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 32, 32, 192])\n",
      "    After shift: torch.Size([16, 32, 32, 192])\n",
      "    After window_partition: torch.Size([256, 8, 8, 192]), is_contiguous=True\n",
      "    After view for attention: torch.Size([256, 64, 192])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([256, 64, 192]), collect=True\n",
      "      After QKV: q=torch.Size([256, 8, 64, 24]), k=torch.Size([256, 8, 64, 24]), v=torch.Size([256, 8, 64, 24])\n",
      "      After attn scores: torch.Size([256, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([256, 8, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([256, 8, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 1: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 2: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 3: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 4: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 5: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 6: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 7: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([256, 64, 192])\n",
      "    After attention: torch.Size([256, 64, 192])\n",
      "    After view for reverse: torch.Size([256, 8, 8, 192])\n",
      "    After window_reverse: torch.Size([16, 32, 32, 192])\n",
      "    After view back to 3D: torch.Size([16, 1024, 192])\n",
      "    After FFN: torch.Size([16, 1024, 192])\n",
      "  Block 1: input shape = torch.Size([16, 1024, 192])\n",
      "    After norm1: torch.Size([16, 1024, 192]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 32, 32, 192])\n",
      "    After shift: torch.Size([16, 32, 32, 192])\n",
      "    After window_partition: torch.Size([256, 8, 8, 192]), is_contiguous=True\n",
      "    After view for attention: torch.Size([256, 64, 192])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([256, 64, 192]), collect=True\n",
      "      After QKV: q=torch.Size([256, 8, 64, 24]), k=torch.Size([256, 8, 64, 24]), v=torch.Size([256, 8, 64, 24])\n",
      "      After attn scores: torch.Size([256, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([256, 8, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([256, 8, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 1: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 2: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 3: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 4: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 5: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 6: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "        Head 7: shape=torch.Size([256, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([16384, 24])\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([256, 64, 192])\n",
      "    After attention: torch.Size([256, 64, 192])\n",
      "    After view for reverse: torch.Size([256, 8, 8, 192])\n",
      "    After window_reverse: torch.Size([16, 32, 32, 192])\n",
      "    After view back to 3D: torch.Size([16, 1024, 192])\n",
      "    After FFN: torch.Size([16, 1024, 192])\n",
      "  After downsampling: torch.Size([16, 256, 384])\n",
      "\n",
      "ðŸ” DEBUG: Processing layer_2 in ATTENTION mode with fitting\n",
      "  Block 0: input shape = torch.Size([16, 256, 384])\n",
      "    After norm1: torch.Size([16, 256, 384]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 16, 16, 384])\n",
      "    After shift: torch.Size([16, 16, 16, 384])\n",
      "    After window_partition: torch.Size([64, 8, 8, 384]), is_contiguous=True\n",
      "    After view for attention: torch.Size([64, 64, 384])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 384]), collect=True\n",
      "      After QKV: q=torch.Size([64, 16, 64, 24]), k=torch.Size([64, 16, 64, 24]), v=torch.Size([64, 16, 64, 24])\n",
      "      After attn scores: torch.Size([64, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 1: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 2: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 3: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 4: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 5: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 6: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 7: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 8: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 9: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 10: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 11: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 12: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 13: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 14: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 15: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([64, 64, 384])\n",
      "    After attention: torch.Size([64, 64, 384])\n",
      "    After view for reverse: torch.Size([64, 8, 8, 384])\n",
      "    After window_reverse: torch.Size([16, 16, 16, 384])\n",
      "    After view back to 3D: torch.Size([16, 256, 384])\n",
      "    After FFN: torch.Size([16, 256, 384])\n",
      "  Block 1: input shape = torch.Size([16, 256, 384])\n",
      "    After norm1: torch.Size([16, 256, 384]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 16, 16, 384])\n",
      "    After shift: torch.Size([16, 16, 16, 384])\n",
      "    After window_partition: torch.Size([64, 8, 8, 384]), is_contiguous=True\n",
      "    After view for attention: torch.Size([64, 64, 384])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 384]), collect=True\n",
      "      After QKV: q=torch.Size([64, 16, 64, 24]), k=torch.Size([64, 16, 64, 24]), v=torch.Size([64, 16, 64, 24])\n",
      "      After attn scores: torch.Size([64, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 1: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 2: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 3: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 4: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 5: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 6: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 7: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 8: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 9: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 10: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 11: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 12: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 13: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 14: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 15: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([64, 64, 384])\n",
      "    After attention: torch.Size([64, 64, 384])\n",
      "    After view for reverse: torch.Size([64, 8, 8, 384])\n",
      "    After window_reverse: torch.Size([16, 16, 16, 384])\n",
      "    After view back to 3D: torch.Size([16, 256, 384])\n",
      "    After FFN: torch.Size([16, 256, 384])\n",
      "  Block 2: input shape = torch.Size([16, 256, 384])\n",
      "    After norm1: torch.Size([16, 256, 384]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 16, 16, 384])\n",
      "    After shift: torch.Size([16, 16, 16, 384])\n",
      "    After window_partition: torch.Size([64, 8, 8, 384]), is_contiguous=True\n",
      "    After view for attention: torch.Size([64, 64, 384])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 384]), collect=True\n",
      "      After QKV: q=torch.Size([64, 16, 64, 24]), k=torch.Size([64, 16, 64, 24]), v=torch.Size([64, 16, 64, 24])\n",
      "      After attn scores: torch.Size([64, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 1: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 2: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 3: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 4: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 5: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 6: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 7: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 8: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 9: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 10: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 11: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 12: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 13: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 14: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 15: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([64, 64, 384])\n",
      "    After attention: torch.Size([64, 64, 384])\n",
      "    After view for reverse: torch.Size([64, 8, 8, 384])\n",
      "    After window_reverse: torch.Size([16, 16, 16, 384])\n",
      "    After view back to 3D: torch.Size([16, 256, 384])\n",
      "    After FFN: torch.Size([16, 256, 384])\n",
      "  Block 3: input shape = torch.Size([16, 256, 384])\n",
      "    After norm1: torch.Size([16, 256, 384]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 16, 16, 384])\n",
      "    After shift: torch.Size([16, 16, 16, 384])\n",
      "    After window_partition: torch.Size([64, 8, 8, 384]), is_contiguous=True\n",
      "    After view for attention: torch.Size([64, 64, 384])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 384]), collect=True\n",
      "      After QKV: q=torch.Size([64, 16, 64, 24]), k=torch.Size([64, 16, 64, 24]), v=torch.Size([64, 16, 64, 24])\n",
      "      After attn scores: torch.Size([64, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 1: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 2: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 3: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 4: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 5: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 6: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 7: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 8: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 9: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 10: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 11: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 12: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 13: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 14: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 15: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([64, 64, 384])\n",
      "    After attention: torch.Size([64, 64, 384])\n",
      "    After view for reverse: torch.Size([64, 8, 8, 384])\n",
      "    After window_reverse: torch.Size([16, 16, 16, 384])\n",
      "    After view back to 3D: torch.Size([16, 256, 384])\n",
      "    After FFN: torch.Size([16, 256, 384])\n",
      "  Block 4: input shape = torch.Size([16, 256, 384])\n",
      "    After norm1: torch.Size([16, 256, 384]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 16, 16, 384])\n",
      "    After shift: torch.Size([16, 16, 16, 384])\n",
      "    After window_partition: torch.Size([64, 8, 8, 384]), is_contiguous=True\n",
      "    After view for attention: torch.Size([64, 64, 384])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 384]), collect=True\n",
      "      After QKV: q=torch.Size([64, 16, 64, 24]), k=torch.Size([64, 16, 64, 24]), v=torch.Size([64, 16, 64, 24])\n",
      "      After attn scores: torch.Size([64, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 1: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 2: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 3: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 4: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 5: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 6: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 7: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 8: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 9: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 10: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 11: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 12: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 13: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 14: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 15: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([64, 64, 384])\n",
      "    After attention: torch.Size([64, 64, 384])\n",
      "    After view for reverse: torch.Size([64, 8, 8, 384])\n",
      "    After window_reverse: torch.Size([16, 16, 16, 384])\n",
      "    After view back to 3D: torch.Size([16, 256, 384])\n",
      "    After FFN: torch.Size([16, 256, 384])\n",
      "  Block 5: input shape = torch.Size([16, 256, 384])\n",
      "    After norm1: torch.Size([16, 256, 384]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 16, 16, 384])\n",
      "    After shift: torch.Size([16, 16, 16, 384])\n",
      "    After window_partition: torch.Size([64, 8, 8, 384]), is_contiguous=True\n",
      "    After view for attention: torch.Size([64, 64, 384])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 384]), collect=True\n",
      "      After QKV: q=torch.Size([64, 16, 64, 24]), k=torch.Size([64, 16, 64, 24]), v=torch.Size([64, 16, 64, 24])\n",
      "      After attn scores: torch.Size([64, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([64, 16, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 1: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 2: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 3: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 4: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 5: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 6: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 7: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 8: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 9: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 10: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 11: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 12: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 13: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 14: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "        Head 15: shape=torch.Size([64, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([4096, 24])\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([64, 64, 384])\n",
      "    After attention: torch.Size([64, 64, 384])\n",
      "    After view for reverse: torch.Size([64, 8, 8, 384])\n",
      "    After window_reverse: torch.Size([16, 16, 16, 384])\n",
      "    After view back to 3D: torch.Size([16, 256, 384])\n",
      "    After FFN: torch.Size([16, 256, 384])\n",
      "  After downsampling: torch.Size([16, 64, 768])\n",
      "\n",
      "ðŸ” DEBUG: Processing layer_3 in ATTENTION mode with fitting\n",
      "  Block 0: input shape = torch.Size([16, 64, 768])\n",
      "    After norm1: torch.Size([16, 64, 768]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 8, 8, 768])\n",
      "    After shift: torch.Size([16, 8, 8, 768])\n",
      "    After window_partition: torch.Size([16, 8, 8, 768]), is_contiguous=True\n",
      "    After view for attention: torch.Size([16, 64, 768])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 768]), collect=True\n",
      "      After QKV: q=torch.Size([16, 32, 64, 24]), k=torch.Size([16, 32, 64, 24]), v=torch.Size([16, 32, 64, 24])\n",
      "      After attn scores: torch.Size([16, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 32, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([16, 32, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 1: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 2: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 3: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 4: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 5: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 6: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 7: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 8: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 9: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 10: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 11: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 12: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 13: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 14: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 15: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 16: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 17: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 18: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 19: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 20: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 21: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 22: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 23: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 24: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 25: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 26: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 27: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 28: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 29: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 30: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 31: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([16, 64, 768])\n",
      "    After attention: torch.Size([16, 64, 768])\n",
      "    After view for reverse: torch.Size([16, 8, 8, 768])\n",
      "    After window_reverse: torch.Size([16, 8, 8, 768])\n",
      "    After view back to 3D: torch.Size([16, 64, 768])\n",
      "    After FFN: torch.Size([16, 64, 768])\n",
      "  Block 1: input shape = torch.Size([16, 64, 768])\n",
      "    After norm1: torch.Size([16, 64, 768]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([16, 8, 8, 768])\n",
      "    After shift: torch.Size([16, 8, 8, 768])\n",
      "    After window_partition: torch.Size([16, 8, 8, 768]), is_contiguous=True\n",
      "    After view for attention: torch.Size([16, 64, 768])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 768]), collect=True\n",
      "      After QKV: q=torch.Size([16, 32, 64, 24]), k=torch.Size([16, 32, 64, 24]), v=torch.Size([16, 32, 64, 24])\n",
      "      After attn scores: torch.Size([16, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 32, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([16, 32, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 1: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 2: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 3: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 4: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 5: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 6: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 7: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 8: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 9: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 10: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 11: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 12: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 13: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 14: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 15: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 16: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 17: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 18: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 19: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 20: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 21: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 22: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 23: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 24: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 25: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 26: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 27: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 28: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 29: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 30: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "        Head 31: shape=torch.Size([16, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([1024, 24])\n",
      "      Applying reweighting to 32 heads...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Raccolta:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:10<00:03,  3.35s/it, samples=48]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Final output: torch.Size([16, 64, 768])\n",
      "    After attention: torch.Size([16, 64, 768])\n",
      "    After view for reverse: torch.Size([16, 8, 8, 768])\n",
      "    After window_reverse: torch.Size([16, 8, 8, 768])\n",
      "    After view back to 3D: torch.Size([16, 64, 768])\n",
      "    After FFN: torch.Size([16, 64, 768])\n",
      "\n",
      "ðŸ” DEBUG: Processing layer_0 in ATTENTION mode with fitting\n",
      "  Block 0: input shape = torch.Size([2, 4096, 96])\n",
      "    After norm1: torch.Size([2, 4096, 96]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([2, 64, 64, 96])\n",
      "    After shift: torch.Size([2, 64, 64, 96])\n",
      "    After window_partition: torch.Size([128, 8, 8, 96]), is_contiguous=True\n",
      "    After view for attention: torch.Size([128, 64, 96])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([128, 64, 96]), collect=True\n",
      "      After QKV: q=torch.Size([128, 4, 64, 24]), k=torch.Size([128, 4, 64, 24]), v=torch.Size([128, 4, 64, 24])\n",
      "      After attn scores: torch.Size([128, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([128, 4, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([128, 4, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([128, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([8192, 24])\n",
      "        Head 1: shape=torch.Size([128, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([8192, 24])\n",
      "        Head 2: shape=torch.Size([128, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([8192, 24])\n",
      "        Head 3: shape=torch.Size([128, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([8192, 24])\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([128, 64, 96])\n",
      "    After attention: torch.Size([128, 64, 96])\n",
      "    After view for reverse: torch.Size([128, 8, 8, 96])\n",
      "    After window_reverse: torch.Size([2, 64, 64, 96])\n",
      "    After view back to 3D: torch.Size([2, 4096, 96])\n",
      "    After FFN: torch.Size([2, 4096, 96])\n",
      "  Block 1: input shape = torch.Size([2, 4096, 96])\n",
      "    After norm1: torch.Size([2, 4096, 96]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([2, 64, 64, 96])\n",
      "    After shift: torch.Size([2, 64, 64, 96])\n",
      "    After window_partition: torch.Size([128, 8, 8, 96]), is_contiguous=True\n",
      "    After view for attention: torch.Size([128, 64, 96])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([128, 64, 96]), collect=True\n",
      "      After QKV: q=torch.Size([128, 4, 64, 24]), k=torch.Size([128, 4, 64, 24]), v=torch.Size([128, 4, 64, 24])\n",
      "      After attn scores: torch.Size([128, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([128, 4, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([128, 4, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([128, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([8192, 24])\n",
      "        Head 1: shape=torch.Size([128, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([8192, 24])\n",
      "        Head 2: shape=torch.Size([128, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([8192, 24])\n",
      "        Head 3: shape=torch.Size([128, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([8192, 24])\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([128, 64, 96])\n",
      "    After attention: torch.Size([128, 64, 96])\n",
      "    After view for reverse: torch.Size([128, 8, 8, 96])\n",
      "    After window_reverse: torch.Size([2, 64, 64, 96])\n",
      "    After view back to 3D: torch.Size([2, 4096, 96])\n",
      "    After FFN: torch.Size([2, 4096, 96])\n",
      "  After downsampling: torch.Size([2, 1024, 192])\n",
      "\n",
      "ðŸ” DEBUG: Processing layer_1 in ATTENTION mode with fitting\n",
      "  Block 0: input shape = torch.Size([2, 1024, 192])\n",
      "    After norm1: torch.Size([2, 1024, 192]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([2, 32, 32, 192])\n",
      "    After shift: torch.Size([2, 32, 32, 192])\n",
      "    After window_partition: torch.Size([32, 8, 8, 192]), is_contiguous=True\n",
      "    After view for attention: torch.Size([32, 64, 192])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([32, 64, 192]), collect=True\n",
      "      After QKV: q=torch.Size([32, 8, 64, 24]), k=torch.Size([32, 8, 64, 24]), v=torch.Size([32, 8, 64, 24])\n",
      "      After attn scores: torch.Size([32, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([32, 8, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([32, 8, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([32, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([2048, 24])\n",
      "        Head 1: shape=torch.Size([32, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([2048, 24])\n",
      "        Head 2: shape=torch.Size([32, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([2048, 24])\n",
      "        Head 3: shape=torch.Size([32, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([2048, 24])\n",
      "        Head 4: shape=torch.Size([32, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([2048, 24])\n",
      "        Head 5: shape=torch.Size([32, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([2048, 24])\n",
      "        Head 6: shape=torch.Size([32, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([2048, 24])\n",
      "        Head 7: shape=torch.Size([32, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([2048, 24])\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([32, 64, 192])\n",
      "    After attention: torch.Size([32, 64, 192])\n",
      "    After view for reverse: torch.Size([32, 8, 8, 192])\n",
      "    After window_reverse: torch.Size([2, 32, 32, 192])\n",
      "    After view back to 3D: torch.Size([2, 1024, 192])\n",
      "    After FFN: torch.Size([2, 1024, 192])\n",
      "  Block 1: input shape = torch.Size([2, 1024, 192])\n",
      "    After norm1: torch.Size([2, 1024, 192]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([2, 32, 32, 192])\n",
      "    After shift: torch.Size([2, 32, 32, 192])\n",
      "    After window_partition: torch.Size([32, 8, 8, 192]), is_contiguous=True\n",
      "    After view for attention: torch.Size([32, 64, 192])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([32, 64, 192]), collect=True\n",
      "      After QKV: q=torch.Size([32, 8, 64, 24]), k=torch.Size([32, 8, 64, 24]), v=torch.Size([32, 8, 64, 24])\n",
      "      After attn scores: torch.Size([32, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([32, 8, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([32, 8, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([32, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([2048, 24])\n",
      "        Head 1: shape=torch.Size([32, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([2048, 24])\n",
      "        Head 2: shape=torch.Size([32, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([2048, 24])\n",
      "        Head 3: shape=torch.Size([32, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([2048, 24])\n",
      "        Head 4: shape=torch.Size([32, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([2048, 24])\n",
      "        Head 5: shape=torch.Size([32, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([2048, 24])\n",
      "        Head 6: shape=torch.Size([32, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([2048, 24])\n",
      "        Head 7: shape=torch.Size([32, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([2048, 24])\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([32, 64, 192])\n",
      "    After attention: torch.Size([32, 64, 192])\n",
      "    After view for reverse: torch.Size([32, 8, 8, 192])\n",
      "    After window_reverse: torch.Size([2, 32, 32, 192])\n",
      "    After view back to 3D: torch.Size([2, 1024, 192])\n",
      "    After FFN: torch.Size([2, 1024, 192])\n",
      "  After downsampling: torch.Size([2, 256, 384])\n",
      "\n",
      "ðŸ” DEBUG: Processing layer_2 in ATTENTION mode with fitting\n",
      "  Block 0: input shape = torch.Size([2, 256, 384])\n",
      "    After norm1: torch.Size([2, 256, 384]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([2, 16, 16, 384])\n",
      "    After shift: torch.Size([2, 16, 16, 384])\n",
      "    After window_partition: torch.Size([8, 8, 8, 384]), is_contiguous=True\n",
      "    After view for attention: torch.Size([8, 64, 384])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([8, 64, 384]), collect=True\n",
      "      After QKV: q=torch.Size([8, 16, 64, 24]), k=torch.Size([8, 16, 64, 24]), v=torch.Size([8, 16, 64, 24])\n",
      "      After attn scores: torch.Size([8, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([8, 16, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([8, 16, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 1: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 2: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 3: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 4: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 5: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 6: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 7: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 8: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 9: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 10: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 11: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 12: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 13: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 14: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 15: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([8, 64, 384])\n",
      "    After attention: torch.Size([8, 64, 384])\n",
      "    After view for reverse: torch.Size([8, 8, 8, 384])\n",
      "    After window_reverse: torch.Size([2, 16, 16, 384])\n",
      "    After view back to 3D: torch.Size([2, 256, 384])\n",
      "    After FFN: torch.Size([2, 256, 384])\n",
      "  Block 1: input shape = torch.Size([2, 256, 384])\n",
      "    After norm1: torch.Size([2, 256, 384]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([2, 16, 16, 384])\n",
      "    After shift: torch.Size([2, 16, 16, 384])\n",
      "    After window_partition: torch.Size([8, 8, 8, 384]), is_contiguous=True\n",
      "    After view for attention: torch.Size([8, 64, 384])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([8, 64, 384]), collect=True\n",
      "      After QKV: q=torch.Size([8, 16, 64, 24]), k=torch.Size([8, 16, 64, 24]), v=torch.Size([8, 16, 64, 24])\n",
      "      After attn scores: torch.Size([8, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([8, 16, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([8, 16, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 1: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 2: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 3: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 4: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 5: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 6: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 7: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 8: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 9: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 10: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 11: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 12: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 13: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 14: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 15: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([8, 64, 384])\n",
      "    After attention: torch.Size([8, 64, 384])\n",
      "    After view for reverse: torch.Size([8, 8, 8, 384])\n",
      "    After window_reverse: torch.Size([2, 16, 16, 384])\n",
      "    After view back to 3D: torch.Size([2, 256, 384])\n",
      "    After FFN: torch.Size([2, 256, 384])\n",
      "  Block 2: input shape = torch.Size([2, 256, 384])\n",
      "    After norm1: torch.Size([2, 256, 384]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([2, 16, 16, 384])\n",
      "    After shift: torch.Size([2, 16, 16, 384])\n",
      "    After window_partition: torch.Size([8, 8, 8, 384]), is_contiguous=True\n",
      "    After view for attention: torch.Size([8, 64, 384])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([8, 64, 384]), collect=True\n",
      "      After QKV: q=torch.Size([8, 16, 64, 24]), k=torch.Size([8, 16, 64, 24]), v=torch.Size([8, 16, 64, 24])\n",
      "      After attn scores: torch.Size([8, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([8, 16, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([8, 16, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 1: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 2: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 3: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 4: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 5: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 6: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 7: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 8: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 9: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 10: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 11: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 12: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 13: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 14: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 15: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([8, 64, 384])\n",
      "    After attention: torch.Size([8, 64, 384])\n",
      "    After view for reverse: torch.Size([8, 8, 8, 384])\n",
      "    After window_reverse: torch.Size([2, 16, 16, 384])\n",
      "    After view back to 3D: torch.Size([2, 256, 384])\n",
      "    After FFN: torch.Size([2, 256, 384])\n",
      "  Block 3: input shape = torch.Size([2, 256, 384])\n",
      "    After norm1: torch.Size([2, 256, 384]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([2, 16, 16, 384])\n",
      "    After shift: torch.Size([2, 16, 16, 384])\n",
      "    After window_partition: torch.Size([8, 8, 8, 384]), is_contiguous=True\n",
      "    After view for attention: torch.Size([8, 64, 384])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([8, 64, 384]), collect=True\n",
      "      After QKV: q=torch.Size([8, 16, 64, 24]), k=torch.Size([8, 16, 64, 24]), v=torch.Size([8, 16, 64, 24])\n",
      "      After attn scores: torch.Size([8, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([8, 16, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([8, 16, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 1: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 2: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 3: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 4: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 5: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 6: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 7: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 8: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 9: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 10: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 11: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 12: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 13: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 14: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 15: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([8, 64, 384])\n",
      "    After attention: torch.Size([8, 64, 384])\n",
      "    After view for reverse: torch.Size([8, 8, 8, 384])\n",
      "    After window_reverse: torch.Size([2, 16, 16, 384])\n",
      "    After view back to 3D: torch.Size([2, 256, 384])\n",
      "    After FFN: torch.Size([2, 256, 384])\n",
      "  Block 4: input shape = torch.Size([2, 256, 384])\n",
      "    After norm1: torch.Size([2, 256, 384]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([2, 16, 16, 384])\n",
      "    After shift: torch.Size([2, 16, 16, 384])\n",
      "    After window_partition: torch.Size([8, 8, 8, 384]), is_contiguous=True\n",
      "    After view for attention: torch.Size([8, 64, 384])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([8, 64, 384]), collect=True\n",
      "      After QKV: q=torch.Size([8, 16, 64, 24]), k=torch.Size([8, 16, 64, 24]), v=torch.Size([8, 16, 64, 24])\n",
      "      After attn scores: torch.Size([8, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([8, 16, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([8, 16, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 1: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 2: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 3: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 4: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 5: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 6: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 7: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 8: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 9: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 10: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 11: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 12: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 13: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 14: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 15: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([8, 64, 384])\n",
      "    After attention: torch.Size([8, 64, 384])\n",
      "    After view for reverse: torch.Size([8, 8, 8, 384])\n",
      "    After window_reverse: torch.Size([2, 16, 16, 384])\n",
      "    After view back to 3D: torch.Size([2, 256, 384])\n",
      "    After FFN: torch.Size([2, 256, 384])\n",
      "  Block 5: input shape = torch.Size([2, 256, 384])\n",
      "    After norm1: torch.Size([2, 256, 384]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([2, 16, 16, 384])\n",
      "    After shift: torch.Size([2, 16, 16, 384])\n",
      "    After window_partition: torch.Size([8, 8, 8, 384]), is_contiguous=True\n",
      "    After view for attention: torch.Size([8, 64, 384])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([8, 64, 384]), collect=True\n",
      "      After QKV: q=torch.Size([8, 16, 64, 24]), k=torch.Size([8, 16, 64, 24]), v=torch.Size([8, 16, 64, 24])\n",
      "      After attn scores: torch.Size([8, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([8, 16, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([8, 16, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 1: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 2: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 3: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 4: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 5: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 6: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 7: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 8: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 9: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 10: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 11: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 12: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 13: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 14: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "        Head 15: shape=torch.Size([8, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([512, 24])\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([8, 64, 384])\n",
      "    After attention: torch.Size([8, 64, 384])\n",
      "    After view for reverse: torch.Size([8, 8, 8, 384])\n",
      "    After window_reverse: torch.Size([2, 16, 16, 384])\n",
      "    After view back to 3D: torch.Size([2, 256, 384])\n",
      "    After FFN: torch.Size([2, 256, 384])\n",
      "  After downsampling: torch.Size([2, 64, 768])\n",
      "\n",
      "ðŸ” DEBUG: Processing layer_3 in ATTENTION mode with fitting\n",
      "  Block 0: input shape = torch.Size([2, 64, 768])\n",
      "    After norm1: torch.Size([2, 64, 768]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([2, 8, 8, 768])\n",
      "    After shift: torch.Size([2, 8, 8, 768])\n",
      "    After window_partition: torch.Size([2, 8, 8, 768]), is_contiguous=True\n",
      "    After view for attention: torch.Size([2, 64, 768])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([2, 64, 768]), collect=True\n",
      "      After QKV: q=torch.Size([2, 32, 64, 24]), k=torch.Size([2, 32, 64, 24]), v=torch.Size([2, 32, 64, 24])\n",
      "      After attn scores: torch.Size([2, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([2, 32, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([2, 32, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 1: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 2: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 3: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 4: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 5: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 6: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 7: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 8: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 9: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 10: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 11: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 12: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 13: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 14: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 15: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 16: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 17: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 18: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 19: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 20: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 21: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 22: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 23: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 24: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 25: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 26: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 27: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 28: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 29: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 30: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 31: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([2, 64, 768])\n",
      "    After attention: torch.Size([2, 64, 768])\n",
      "    After view for reverse: torch.Size([2, 8, 8, 768])\n",
      "    After window_reverse: torch.Size([2, 8, 8, 768])\n",
      "    After view back to 3D: torch.Size([2, 64, 768])\n",
      "    After FFN: torch.Size([2, 64, 768])\n",
      "  Block 1: input shape = torch.Size([2, 64, 768])\n",
      "    After norm1: torch.Size([2, 64, 768]), is_contiguous=True\n",
      "    After view to 4D: torch.Size([2, 8, 8, 768])\n",
      "    After shift: torch.Size([2, 8, 8, 768])\n",
      "    After window_partition: torch.Size([2, 8, 8, 768]), is_contiguous=True\n",
      "    After view for attention: torch.Size([2, 64, 768])\n",
      "    Calling attention with collect_for_fitting=True\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([2, 64, 768]), collect=True\n",
      "      After QKV: q=torch.Size([2, 32, 64, 24]), k=torch.Size([2, 32, 64, 24]), v=torch.Size([2, 32, 64, 24])\n",
      "      After attn scores: torch.Size([2, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([2, 32, 64, 24]), is_contiguous=True\n",
      "      Calling _collect_heads...\n",
      "      ðŸ” _collect_heads: x_heads shape = torch.Size([2, 32, 64, 24]), is_contiguous=True\n",
      "        Head 0: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 1: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 2: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 3: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 4: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 5: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 6: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 7: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 8: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 9: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 10: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 11: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 12: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 13: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 14: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 15: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 16: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 17: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 18: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 19: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 20: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 21: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 22: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 23: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 24: shape=torch.Size([2, 64, 24]), is_contiguous=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Raccolta: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:10<00:00,  2.65s/it, samples=50]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 25: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 26: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 27: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 28: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 29: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 30: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "        Head 31: shape=torch.Size([2, 64, 24]), is_contiguous=False\n",
      "        After reshape: torch.Size([128, 24])\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([2, 64, 768])\n",
      "    After attention: torch.Size([2, 64, 768])\n",
      "    After view for reverse: torch.Size([2, 8, 8, 768])\n",
      "    After window_reverse: torch.Size([2, 8, 8, 768])\n",
      "    After view back to 3D: torch.Size([2, 64, 768])\n",
      "    After FFN: torch.Size([2, 64, 768])\n",
      "\n",
      "âœ“ Raccolti 50 samples totali\n",
      "\n",
      "ðŸ” DEBUG: Struttura collected_data:\n",
      "\n",
      "================================================================================\n",
      "ðŸ”¬ Fase 2: Calcolo PCA e inizializzazione pesi...\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "âœ… Fitting completato!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "ðŸ“Š PCA Variance Ratios:\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Fitting PCA Components\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Fit PCA\n",
    "print(f\"Fitting PCA on {len(pca_dataset)} samples...\")\n",
    "variance_ratios = clap_residual.clap.audio_encoder.base.htsat.fit_spectral_layers(\n",
    "    pca_loader,\n",
    "    max_samples=50\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š PCA Variance Ratios:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Gestione corretta della struttura gerarchica\n",
    "for layer_name, layer_data in variance_ratios.items():\n",
    "    print(f\"\\n{layer_name}:\")\n",
    "    \n",
    "    # Controlla se Ã¨ mode 'attention' (struttura annidata) o 'layer' (array diretto)\n",
    "    if isinstance(layer_data, dict):\n",
    "        # Mode ATTENTION: layer_data contiene blocchi\n",
    "        for block_name, block_data in layer_data.items():\n",
    "            print(f\"  {block_name}:\")\n",
    "            \n",
    "            # block_data contiene le teste\n",
    "            for head_name, head_variance in block_data.items():\n",
    "                # head_variance Ã¨ l'array numpy con le variance ratios\n",
    "                top3 = head_variance[:3]\n",
    "                cumulative = head_variance[:3].sum()\n",
    "                print(f\"    {head_name}: top3={[f'{v:.3f}' for v in top3]}, cum={cumulative:.3f}\")\n",
    "    else:\n",
    "        # Mode LAYER: layer_data Ã¨ direttamente l'array di variance ratios\n",
    "        top5 = layer_data[:5]\n",
    "        cumulative = layer_data[:5].sum()\n",
    "        print(f\"  Top 5 components: {[f'{v:.4f}' for v in top5]}\")\n",
    "        print(f\"  Cumulative variance (top 5): {cumulative:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e54bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Text embeddings shape: torch.Size([50, 1024])\n",
      "\n",
      "ðŸ“Š Testing on 100 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c748b9b9e8440e8e2dfceea3e23479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Baseline:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Baseline Accuracy: 0.920 (92.0%)\n"
     ]
    }
   ],
   "source": [
    "# Get text embeddings ONCE for all classes\n",
    "text_embeddings = clap_standard.get_text_embeddings(text_labels)\n",
    "print(f\"   Text embeddings shape: {text_embeddings.shape}\")\n",
    "\n",
    "# Test on subset first (use full dataset later)\n",
    "test_size = 100  # Start with 200 samples for speed\n",
    "print(f\"\\nðŸ“Š Testing on {test_size} samples...\")\n",
    "\n",
    "y_preds_baseline, y_labels = [], []\n",
    "\n",
    "for i in tqdm(range(test_size), desc=\"Baseline\"):\n",
    "    # Get audio file path and label\n",
    "    audio_path, target, one_hot_target = dataset[-(i+1)]\n",
    "    \n",
    "    # Get audio embedding\n",
    "    audio_embedding = clap_standard.get_audio_embeddings([audio_path], resample=True)\n",
    "    \n",
    "    # Compute similarity\n",
    "    similarity = clap_standard.compute_similarity(audio_embedding, text_embeddings)\n",
    "    \n",
    "    # Get prediction\n",
    "    y_pred = F.softmax(similarity.detach().cpu(), dim=1).numpy()\n",
    "    y_preds_baseline.append(y_pred)\n",
    "    y_labels.append(one_hot_target.detach().cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "y_labels_array = np.concatenate(y_labels, axis=0)\n",
    "y_preds_baseline_array = np.concatenate(y_preds_baseline, axis=0)\n",
    "\n",
    "baseline_acc = accuracy_score(\n",
    "    np.argmax(y_labels_array, axis=1), \n",
    "    np.argmax(y_preds_baseline_array, axis=1)\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Baseline Accuracy: {baseline_acc:.3f} ({baseline_acc*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e60c8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Text embeddings shape: torch.Size([50, 1024])\n",
      "\n",
      "ðŸ“Š Testing on 100 samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c92f8cc88ead4c0da1aeab6ea82d00fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "residual:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "\n",
      "âœ… Residual Accuracy: 0.920 (92.0%)\n"
     ]
    }
   ],
   "source": [
    "# Get text embeddings ONCE for all classes\n",
    "text_embeddings = clap_residual.get_text_embeddings(text_labels)\n",
    "print(f\"   Text embeddings shape: {text_embeddings.shape}\")\n",
    "\n",
    "# Test on subset first (use full dataset later)\n",
    "test_size = 100  # Start with 200 samples for speed\n",
    "print(f\"\\nðŸ“Š Testing on {test_size} samples...\")\n",
    "\n",
    "y_preds_residual, y_labels = [], []\n",
    "\n",
    "for i in tqdm(range(test_size), desc=\"residual\"):\n",
    "    # Get audio file path and label\n",
    "    audio_path, target, one_hot_target = dataset[-(i+1)]\n",
    "    \n",
    "    # Get audio embedding\n",
    "    audio_embedding = clap_residual.get_audio_embeddings([audio_path], resample=True)\n",
    "    \n",
    "    # Compute similarity\n",
    "    similarity = clap_residual.compute_similarity(audio_embedding, text_embeddings)\n",
    "    \n",
    "    # Get prediction\n",
    "    y_pred = F.softmax(similarity.detach().cpu(), dim=1).numpy()\n",
    "    y_preds_residual.append(y_pred)\n",
    "    y_labels.append(one_hot_target.detach().cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "y_labels_array = np.concatenate(y_labels, axis=0)\n",
    "y_preds_residual_array = np.concatenate(y_preds_residual, axis=0)\n",
    "\n",
    "baseline_acc = accuracy_score(\n",
    "    np.argmax(y_labels_array, axis=1), \n",
    "    np.argmax(y_preds_residual_array, axis=1)\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Residual Accuracy: {baseline_acc:.3f} ({baseline_acc*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726a89f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path, target, one_hot_target = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d03d337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([64, 64, 96]), collect=False\n",
      "      After QKV: q=torch.Size([64, 4, 64, 24]), k=torch.Size([64, 4, 64, 24]), v=torch.Size([64, 4, 64, 24])\n",
      "      After attn scores: torch.Size([64, 4, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([64, 4, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 4 heads...\n",
      "      Final output: torch.Size([64, 64, 96])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([16, 64, 192]), collect=False\n",
      "      After QKV: q=torch.Size([16, 8, 64, 24]), k=torch.Size([16, 8, 64, 24]), v=torch.Size([16, 8, 64, 24])\n",
      "      After attn scores: torch.Size([16, 8, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([16, 8, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 8 heads...\n",
      "      Final output: torch.Size([16, 64, 192])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([4, 64, 384]), collect=False\n",
      "      After QKV: q=torch.Size([4, 16, 64, 24]), k=torch.Size([4, 16, 64, 24]), v=torch.Size([4, 16, 64, 24])\n",
      "      After attn scores: torch.Size([4, 16, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([4, 16, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 16 heads...\n",
      "      Final output: torch.Size([4, 64, 384])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n",
      "      ðŸ” WindowAttentionReweighting.forward: x shape = torch.Size([1, 64, 768]), collect=False\n",
      "      After QKV: q=torch.Size([1, 32, 64, 24]), k=torch.Size([1, 32, 64, 24]), v=torch.Size([1, 32, 64, 24])\n",
      "      After attn scores: torch.Size([1, 32, 64, 64])\n",
      "      After attn @ v: x_heads shape = torch.Size([1, 32, 64, 24]), is_contiguous=True\n",
      "      Applying reweighting to 32 heads...\n",
      "      Final output: torch.Size([1, 64, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7623,  0.2343, -0.5101,  ...,  1.8940,  0.9414, -0.0119]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clap_residual.get_audio_embeddings([audio_path], resample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aa61e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7623,  0.2343, -0.5101,  ...,  1.8940,  0.9414, -0.0119]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clap_standard.get_audio_embeddings([audio_path], resample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c17b4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ResiDual-CLAP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
