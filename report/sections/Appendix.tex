\appendix

\section{Extended Dimensionality Analysis}
\label{app:extended_analysis}

This appendix provides comprehensive quantitative details and additional visualizations complementing the main results in Section~\ref{sec:dimensionality_results}.

\subsection{Detailed Block-Level Statistics}
\label{app:block_stats}

Table~\ref{tab:block_detailed} reports complete block-wise metrics across all 12 transformer blocks in HTS-AT, aggregating over the heads within each block as described in Section~\ref{sec:dimensionality_method}.

\paragraph{Relationship to Architecture.}
As illustrated in Figure~\ref{fig:htsat_architecture}, the spatial resolution decreases progressively through the network due to patch merging between stages. While this affects the number of tokens $N$ processed by each attention head, our analysis focuses on the intrinsic dimensionality of the \emph{head dimension} $d_h = 24$ after spatial aggregation (Eq.~\ref{eq:spatial_pooling}). Thus, the reported metrics characterize the semantic complexity of head representations independent of spatial resolution effects.

The hierarchical structure creates natural breakpoints for dimensionality analysis:
\begin{itemize}
    \item \textbf{Stage 1 (Blocks 0--1)}: High spatial resolution $(T/2P \times F/2P)$ but limited capacity ($D_0 = 96$). Early fusion of local spectral-temporal patterns.
    \item \textbf{Stage 2 (Blocks 2--3)}: First dimensionality jump coincides with 2× patch merging and head doubling. Transition from local to intermediate-scale features.
    \item \textbf{Stage 3 (Blocks 4--9)}: Deepest stage with 6 blocks enables iterative refinement at fixed spatial scale $(T/4P \times F/4P)$ and capacity ($D_2 = 384$). Gradual dimensionality growth reflects progressive feature abstraction.
    \item \textbf{Stage 4 (Blocks 10--11)}: Maximum capacity ($D_3 = 768$) without further spatial reduction. Minimal dimensionality increase suggests saturation.
\end{itemize}

\begin{table}[h]
\centering
\caption{Block-wise aggregated dimensionality metrics for TinySOL dataset. Blocks 0--1 (Stage 1), 2--3 (Stage 2), 4--9 (Stage 3), 10--11 (Stage 4). L = Linear ID ($d_{\text{PCA}_{99}}$), N = Nonlinear ID (TwoNN), L/N = Linear-nonlinear ratio, EVR1 = First PC variance explained.}
\label{tab:block_detailed}
\small
\begin{tabular}{ccccccc}
\toprule
\textbf{Block} & \textbf{Stage} & \textbf{Heads} & \textbf{L} & \textbf{N} & \textbf{L/N} & \textbf{EVR1} \\
\midrule
0 & 1 & 4 & 3.75 & 3.94 & 0.95 & 0.878 \\
1 & 1 & 4 & 8.00 & 4.94 & 1.62 & 0.575 \\
2 & 2 & 8 & 12.75 & 6.15 & 2.07 & 0.403 \\
3 & 2 & 8 & 17.50 & 6.75 & 2.59 & 0.460 \\
4 & 3 & 16 & 18.00 & 6.93 & 2.60 & 0.388 \\
5 & 3 & 16 & 20.13 & 7.08 & 2.84 & 0.279 \\
6 & 3 & 16 & 21.25 & 7.40 & 2.87 & 0.250 \\
7 & 3 & 16 & 21.38 & 7.42 & 2.88 & 0.243 \\
8 & 3 & 16 & 22.25 & 8.37 & 2.66 & 0.217 \\
9 & 3 & 16 & 21.88 & 8.11 & 2.70 & 0.241 \\
10 & 4 & 32 & 22.25 & 8.49 & 2.62 & 0.262 \\
11 & 4 & 32 & 21.59 & 8.00 & 2.70 & 0.272 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Interpretation.}
\begin{itemize}
    \item Block 0 operates near the linear regime (L/N $\approx$ 1), with almost 88\% variance in the first PC, indicating extremely constrained early processing.
    \item The largest single-block jump occurs at the Stage 1→2 transition (blocks 1→2: $\Delta$L = +4.75, +59\%), corresponding to doubling of attention heads (4→8) and hidden dimension (96→192).
    \item Stage 3 exhibits gradual linear ID growth (18.00 → 22.25 over 6 blocks) despite constant architecture, suggesting intra-stage feature refinement through depth.
    \item Stage 4 shows minimal progression (blocks 10→11: $\Delta$L = -0.66), consistent with representational saturation observed in the main text.
\end{itemize}

\subsection{Extended Cross-Dataset Analysis}
\label{app:cross_dataset_extended}

We replicate the full layer-wise analysis on ESC-50 and VocalSound to validate architectural generalizability. Tables~\ref{tab:esc50_layers} and~\ref{tab:vocalsound_layers} present complete statistics.

\begin{table}[h]
\centering
\caption{Layer-wise dimensionality metrics for ESC-50 dataset (50 environmental sound classes, 1000 stratified samples).}
\label{tab:esc50_layers}
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Layer} & \textbf{$d_{\text{PCA}_{99}}$} & \textbf{TwoNN} & \textbf{PR} & \textbf{EVR1} \\
\midrule
L0 & $5.2 \pm 2.1$ & $4.2 \pm 0.7$ & $1.8 \pm 0.8$ & $0.741 \pm 0.187$ \\
L1 & $14.3 \pm 2.8$ & $6.2 \pm 0.6$ & $3.9 \pm 1.3$ & $0.449 \pm 0.109$ \\
L2 & $20.1 \pm 2.0$ & $7.4 \pm 0.8$ & $7.5 \pm 1.8$ & $0.281 \pm 0.081$ \\
L3 & $20.3 \pm 1.2$ & $7.8 \pm 1.1$ & $7.4 \pm 1.9$ & $0.279 \pm 0.074$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Layer-wise dimensionality metrics for VocalSound dataset (6 vocal sound categories, 1000 stratified samples).}
\label{tab:vocalsound_layers}
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Layer} & \textbf{$d_{\text{PCA}_{99}}$} & \textbf{TwoNN} & \textbf{PR} & \textbf{EVR1} \\
\midrule
L0 & $6.1 \pm 2.3$ & $4.6 \pm 0.8$ & $2.1 \pm 1.0$ & $0.698 \pm 0.192$ \\
L1 & $16.2 \pm 2.4$ & $6.7 \pm 0.5$ & $4.5 \pm 1.5$ & $0.421 \pm 0.117$ \\
L2 & $21.9 \pm 1.9$ & $8.0 \pm 0.9$ & $8.2 \pm 2.0$ & $0.263 \pm 0.079$ \\
L3 & $22.7 \pm 1.1$ & $8.6 \pm 0.8$ & $8.3 \pm 1.6$ & $0.254 \pm 0.071$ \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Cross-Dataset Consistency Analysis.}
Despite differing semantic granularities (ESC-50: 50 classes, VocalSound: 6 classes, TinySOL: 14 classes), layer-wise trends remain remarkably stable:
\begin{itemize}
    \item \textbf{L0 Concentration}: All datasets exhibit EVR1 > 69\% in Stage 1, confirming universal early spectral concentration.
    \item \textbf{L1 Expansion}: The L0→L1 dimensionality jump is consistent (TinySOL: +9.2, ESC-50: +9.1, VocalSound: +10.1 for $d_{\text{PCA}_{99}}$), with coefficient of variation across datasets CV = 0.06.
    \item \textbf{L2-L3 Saturation}: All datasets show similar modest L2→L3 growth ($\Delta d < 2.0$), despite L3 having 2× the heads of L2, indicating architecture-driven capacity limits.
    \item \textbf{L/N Ratio Convergence}: By Stage 4, all datasets reach L/N $\approx$ 2.6--2.7, suggesting a universal nonlinear complexity regime independent of semantic domain.
\end{itemize}

\subsection{Statistical Validation}
\label{app:statistical_validation}

\subsubsection{ANOVA Results}

One-way ANOVA tests for layer differences on TinySOL dataset:

\begin{table}[h]
\centering
\caption{Statistical significance of layer effects on dimensionality metrics (TinySOL, $n=184$ heads). All tests use $\alpha = 0.05$.}
\label{tab:anova_results}
\small
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{F-statistic} & \textbf{p-value} & \textbf{Significance} \\
\midrule
$d_{\text{PCA}_{99}}$ & 262.64 & $< 0.001$ & *** \\
TwoNN & 58.93 & $< 0.001$ & *** \\
PR & 44.74 & $< 0.001$ & *** \\
EffRank & 76.03 & $< 0.001$ & *** \\
EVR(PC1) & 118.47 & $< 0.001$ & *** \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Post-Hoc Pairwise Comparisons}

Bonferroni-corrected pairwise t-tests for $d_{\text{PCA}_{99}}$ (6 comparisons, $\alpha_{\text{corrected}} = 0.0083$):

\begin{table}[h]
\centering
\caption{Pairwise layer comparisons for linear intrinsic dimensionality (TinySOL). All comparisons significant at corrected $\alpha = 0.0083$.}
\label{tab:posthoc_pairwise}
\small
\begin{tabular}{lccc}
\toprule
\textbf{Comparison} & \textbf{$\Delta d_{\text{PCA}_{99}}$} & \textbf{Cohen's $d$} & \textbf{p-value} \\
\midrule
L0 vs L1 & 9.22 & 3.86 & $< 0.001$ \\
L0 vs L2 & 14.93 & 7.12 & $< 0.001$ \\
L0 vs L3 & 16.04 & 8.94 & $< 0.001$ \\
L1 vs L2 & 5.71 & 2.34 & $< 0.001$ \\
L1 vs L3 & 6.82 & 3.19 & $< 0.001$ \\
L2 vs L3 & 1.11 & 0.78 & $< 0.001$ \\
\bottomrule
\end{tabular}
\end{table}

All effect sizes exceed Cohen's threshold for "large" effects ($d > 0.8$), with the L0 vs L3 comparison exhibiting extremely large effects ($d > 8$), confirming substantial representational differences across layers.

\subsection{Additional Visualizations}
\label{app:additional_viz}

\subsubsection{PC1 Dominance and Boxplots}

Figure~\ref{fig:additional_panels} presents complementary views of dimensionality structure.

\begin{figure*}[t]
    \centering
    \subfloat[PC1 Variance Dominance]{\includegraphics[width=0.48\textwidth]{panel_E_pc1_dominance.png}}
    \hfill
    \subfloat[Multi-Metric Boxplots]{\includegraphics[width=0.48\textwidth]{panel_F_boxplot.png}}
    
    \caption{Extended dimensionality analysis. (a) First principal component variance explained across all 184 heads. Horizontal line at 50\% marks equal-contribution threshold. Sharp decline from L0 (mean 73\%) to L3 (mean 27\%) quantifies transition from low-rank to distributed representations. (b) Boxplot comparison of four key metrics across layers, revealing consistent monotonic trends and increasing intra-layer variance in deeper stages (note wider boxes for L2-L3).}
    \label{fig:additional_panels}
\end{figure*}

\paragraph{Observations from Panel (a):}
\begin{itemize}
    \item Only 2 heads in L0 (2.1\%) fall below the 50\% EVR1 threshold, compared to 89\% of L3 heads, demonstrating near-universal early concentration.
    \item The EVR1 distribution shifts from unimodal (L0: concentrated near 0.7--0.8) to bimodal (L3: peaks at 0.2--0.3 and 0.35--0.4), suggesting emergence of head subpopulations with distinct specialization levels.
\end{itemize}

\paragraph{Observations from Panel (b):}
\begin{itemize}
    \item PR and EffRank exhibit parallel scaling, confirming their measurement of related spectral properties.
    \item TwoNN shows compressed scale relative to PCA99, visually emphasizing the linear-nonlinear gap discussed in the main text.
    \item Outliers (marked as individual points beyond whiskers) are rare in L0-L1 but frequent in L2-L3, consistent with increased head-level heterogeneity.
\end{itemize}

\subsection{Computational Details}
\label{app:computational_details}

All analyses were performed on an NVIDIA A100 GPU (40GB) using PyTorch 2.0.1 and Python 3.10. Key implementation details:

\begin{itemize}
    \item \textbf{Head Extraction}: Forward hooks registered via \texttt{torch.nn.Module.register\_forward\_hook}. Batch size 100 for extraction to balance memory and throughput.
    \item \textbf{PCA}: Computed via \texttt{sklearn.decomposition.PCA} with full SVD solver. Eigenvalue thresholding at machine epsilon ($\sim 10^{-7}$) to remove numerical noise.
    \item \textbf{TwoNN}: Implemented using \texttt{skdim.id.TwoNN} with default parameters (no $k$ selection required).
    \item \textbf{MLE}: \texttt{skdim.id.MLE} with $k=20$ neighbors, standard Euclidean metric.
    \item \textbf{Statistical Tests}: \texttt{scipy.stats} functions (\texttt{f\_oneway}, \texttt{ttest\_ind}, \texttt{spearmanr}) with standard settings.
\end{itemize}

Total extraction time: $\sim$45 minutes per dataset (1000 samples × 184 heads). Analysis pipeline code available at \url{https://github.com/[ANONYMOUS]/residual-audio}.

\subsection{Reproducibility Checklist}
\label{app:reproducibility}

To facilitate replication:
\begin{itemize}
    \item Random seeds: 42 (Python), 42 (NumPy), 42 (PyTorch)
    \item CLAP version: \texttt{laion/clap-htsat-unfused} checkpoint from HuggingFace
    \item Audio preprocessing: CLAP default (64-band mel, 10s duration, 48kHz resampling)
    \item Dataset versions: ESC-50 v2.0, TinySOL v3.0, VocalSound official release
    \item Stratified sampling: \texttt{sklearn.model\_selection.StratifiedShuffleSplit} with 1000 samples
\end{itemize}
