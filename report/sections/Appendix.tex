\appendix

% =============================================================
%  APPENDIX A — CLAP/HTS-AT ARCHITECTURE AND NOTATION
% =============================================================

\section{CLAP and HTS-AT: Architecture, Notation, and Residual Decomposition}
\label{app:architecture}

This appendix is a self-contained reference for the two components at the
core of our system. We begin by describing the CLAP dual-encoder model and
its configuration (\S\ref{app:clap_overview}). We then present the HTS-AT
audio backbone in detail, covering input preprocessing, hierarchical stage
structure, and the internal mechanics of each attention block
(\S\ref{app:htsat}). Finally, we derive the per-head residual decomposition
that underlies our analysis and define the aggregated representations used
throughout the paper (\S\ref{app:residual}). All notation is introduced
inline and collected in Table~\ref{tab:notation} for reference.

% ------------------------------------------------------------------
\subsection{CLAP Overview}
\label{app:clap_overview}
% ------------------------------------------------------------------

CLAP (Contrastive Language--Audio Pretraining)~\cite{elizalde2023clap} is a
dual-encoder model that aligns audio and text representations in a shared
embedding space via contrastive learning. The audio encoder is
HTS-AT~\cite{chen2022hts} and the text encoder is
GPT-2~\cite{radford2019gpt2} (embedding dimension 768), whose weights are
frozen throughout training. Both encoders are independently projected to a
joint space of dimension $d_{\mathrm{proj}} = 1024$ via dedicated linear
projection heads; similarity is measured by temperature-scaled cosine
similarity with InfoNCE loss at temperature $\tau = 0.003$.
Zero-shot audio classification is performed by comparing an audio embedding
against the embeddings of textual class prompts.

The full configuration used in this work is reported in
Table~\ref{tab:clap_config}.

\begin{table}[h]
\centering
\caption{CLAP configuration parameters.}
\label{tab:clap_config}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Text encoder                               & GPT-2 \\
Text encoder embedding dim                 & 768 \\
Audio encoder                              & HTS-AT \\
Audio encoder output dim ($D_3$)           & 768 \\
Joint projection dim ($d_{\mathrm{proj}}$) & 1024 \\
Contrastive temperature $\tau$             & 0.003 \\
Sampling rate                              & 44\,100\,Hz \\
Audio duration                             & 7\,s \\
Mel bands                                  & 64 \\
FFT window                                 & 1024 \\
Hop size                                   & 320 \\
$f_{\min}$ / $f_{\max}$                    & 50\,Hz / 8\,000\,Hz \\
\bottomrule
\end{tabular}
\end{table}

% ------------------------------------------------------------------
\subsection{HTS-AT Architecture}
\label{app:htsat}
% ------------------------------------------------------------------

HTS-AT~\cite{chen2022hts} is a Swin-Transformer variant adapted for audio
spectrograms. Its computation is organised into four hierarchical
\emph{stages}, each composed of several \emph{blocks}, as illustrated in
Figure~\ref{fig:htsat_architecture}. We describe the pipeline from raw audio
to the final embedding in order: input preprocessing, patch embedding, stage
structure, and the attention mechanism inside each block.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{images/htsat_architecture.png}
    \caption{HTS-AT hierarchical architecture. The model consists of four
        stages of increasing embedding dimension: Stage~0 (2 blocks,
        4 heads), Stage~1 (2 blocks, 8 heads), Stage~2 (6 blocks, 16 heads),
        and Stage~3 (2 blocks, 32 heads). PatchMerging is applied after
        Stages~0, 1, and~2, halving the spatial side length and doubling the
        feature dimension at each transition. Stage~3 has no PatchMerging.
        The input token sequence has spatial size $64\times64 = 4{,}096$
        tokens with embedding dimension $D_0 = 96$; after three PatchMerging
        operations the final spatial size is $8\times8 = 64$ tokens with
        $D_3 = 768$.}
    \label{fig:htsat_architecture}
\end{figure*}

\paragraph{Input preprocessing and patch embedding.}
Each audio clip is converted to a 64-band log-mel spectrogram
($f_{\min}=50$\,Hz, $f_{\max}=8{,}000$\,Hz, STFT window 1024, hop 320) and
normalised per mel-band. The resulting time--frequency matrix is rearranged
into a square image $\mathbf{x}_{\mathrm{img}} \in \mathbb{R}^{1\times256\times256}$
by folding the time axis into four contiguous segments of 256 frames stacked
vertically over the 64 mel-frequency bands ($4\times64=256$ rows, 256
columns). This image is then split into $4\times4$ non-overlapping patches
via a strided \texttt{Conv2d} layer (kernel $4\times4$, stride 4), followed
by \texttt{LayerNorm}, yielding the input token sequence
\begin{equation}
    \mathbf{Z}^{(0,0)} = \mathbf{X}_{\mathrm{in}}
    \in \mathbb{R}^{4096 \times 96},
\end{equation}
where $4096 = 64\times64$ tokens each have embedding dimension $D_0 = 96$.

\paragraph{Stage structure and PatchMerging.}
The four stages operate at progressively coarser spatial resolutions, as
summarised in Table~\ref{tab:architecture}. We index the residual stream as
$\mathbf{Z}^{(\ell,b)}$, where $\ell \in \{0,1,2,3\}$ is the stage index
and $b \in \{1,\ldots,B_\ell\}$ is the block index within that stage.

After Stages~0, 1, and~2, a \emph{PatchMerging} layer concatenates each
$2\times2$ neighbourhood of spatially adjacent tokens into a single vector of
dimension $4D_\ell$, then projects it to $2D_\ell$ via a bias-free linear
layer. This halves the spatial side length $S_\ell$ and doubles the
embedding dimension:
\begin{equation}
    \mathbb{R}^{S_\ell^2 \times D_\ell}
    \;\xrightarrow{\mathrm{PatchMerging}}\;
    \mathbb{R}^{(S_\ell/2)^2 \times 2D_\ell}.
\end{equation}
Stage~3 has no PatchMerging. The resulting spatial side lengths are
$S_\ell \in \{64, 32, 16, 8\}$ and the corresponding embedding dimensions
are $D_\ell \in \{96, 192, 384, 768\}$ (see Table~\ref{tab:architecture}).

\begin{table*}[h]
\centering
\setlength{\tabcolsep}{4pt}
\caption{HTS-AT stage-level architectural parameters. The per-head dimension
    $d_h = D_\ell / H_\ell = 24$ is constant across all stages. $S_\ell$
    is the spatial side length of the token grid at stage $\ell$;
    $N_w^\ell = S_\ell^2 / M$ is the number of attention windows, with
    $M = w^2 = 64$ tokens per window ($w=8$).}
\label{tab:architecture}
\begin{tabular}{lccccc}
\toprule
\textbf{Stage} $\ell$ & \textbf{Blocks} $B_\ell$ & \textbf{Heads} $H_\ell$ &
\textbf{Dim} $D_\ell$ & \textbf{Spatial} $S_\ell^2$ & \textbf{Windows} $N_w^\ell$ \\
\midrule
0 & 2 &  4 &  96 & $64\times64$ & 64 \\
1 & 2 &  8 & 192 & $32\times32$ & 16 \\
2 & 6 & 16 & 384 & $16\times16$ &  4 \\
3 & 2 & 32 & 768 & $8\times8$   &  1 \\
\midrule
\multicolumn{2}{l}{Total heads $H_{\mathrm{tot}}$}
    & \multicolumn{4}{l}{$2\!\cdot\!4 + 2\!\cdot\!8 + 6\!\cdot\!16 + 2\!\cdot\!32
      = 8+16+96+64 = 184$} \\
\bottomrule
\end{tabular}
\end{table*}

\paragraph{Block computation: W-MSA and MLP.}
Each block $b$ at stage $\ell$ applies two sub-layers in sequence, both
preceded by LayerNorm and connected via residual additions:
\begin{align}
    \mathbf{Z}^{(\ell,b)}
        &\leftarrow \mathbf{Z}^{(\ell,b-1)} +
          \mathrm{W\text{-}MSA}_{\ell,b}\!\left(
            \mathrm{LN}\!\left(\mathbf{Z}^{(\ell,b-1)}\right)
          \right), \label{eq:residual_attn} \\[4pt]
    \mathbf{Z}^{(\ell,b)}
        &\leftarrow \mathbf{Z}^{(\ell,b)} +
          \mathrm{MLP}_{\ell,b}\!\left(
            \mathrm{LN}\!\left(\mathbf{Z}^{(\ell,b)}\right)
          \right). \label{eq:residual_mlp}
\end{align}
Even-indexed blocks use standard Window Multi-head Self-Attention (W-MSA);
odd-indexed blocks use Shifted-Window MSA (SW-MSA), which shifts the
partition by $(w/2, w/2)$ tokens to enable cross-window interactions.
At Stage~3, where $S_3 = w = 8$ so the grid coincides with a single window,
the shift degenerates to zero and all blocks use W-MSA. The MLP sub-layer
is a two-layer feed-forward network with hidden dimension $4D_\ell$ and GELU
activation.

\paragraph{WindowAttention in detail.}
The W-MSA module at block $(\ell, b)$ first partitions the $S_\ell^2$ tokens
into $N_w^\ell = S_\ell^2 / M$ non-overlapping windows of $M = 64$ tokens
each, then applies multi-head attention independently within each window.
Queries, keys, and values are computed via a single fused projection
$W^{QKV}_{\ell,b} \in \mathbb{R}^{D_\ell \times 3D_\ell}$:
\begin{equation}
    \mathrm{LN}(\mathbf{Z}^{(\ell,b-1)})\;W^{QKV}_{\ell,b}
    \;\xrightarrow{\text{split}(D_\ell)}\;
    \mathbf{Q},\,\mathbf{K},\,\mathbf{V}
    \;\in\; \mathbb{R}^{N_w^\ell \times M \times D_\ell}.
\end{equation}
Each tensor is then reshaped to isolate the $H_\ell$ heads, yielding
per-head slices $\mathbf{Q}_h, \mathbf{K}_h, \mathbf{V}_h
\in \mathbb{R}^{N_w^\ell \times M \times d_h}$
with $d_h = D_\ell / H_\ell = 24$.

For each head $h \in \{1,\ldots,H_\ell\}$, attention is computed as:
\begin{equation}
    \mathbf{H}_{\ell,b,h}
    = \mathrm{Softmax}\!\left(
        \frac{\mathbf{Q}_h \mathbf{K}_h^\top}{\sqrt{d_h}}
        + \mathbf{B}_{\ell,b,h}
      \right)\mathbf{V}_h
    \;\in\; \mathbb{R}^{N_w^\ell \times M \times d_h},
    \label{eq:attn}
\end{equation}
where $\mathbf{B}_{\ell,b,h} \in \mathbb{R}^{M \times M}$ is the learned
relative position bias for head $h$ of block $(\ell,b)$. This bias encodes
the relative spatial offset between each pair of tokens \emph{within} a
window; it is shared across all $N_w^\ell$ windows (broadcast) and is
independent for each head. Concretely, it is read from a parameter table of
shape $((2w-1)^2, H_\ell) = (225, H_\ell)$ via a fixed index mapping, so
each head $h$ has its own column of learnable values.

All $H_\ell$ head outputs are then concatenated and passed through the
block-specific output projection $W^O_{\ell,b} \in \mathbb{R}^{D_\ell\times D_\ell}$
with bias $\mathbf{b}^O_{\ell,b} \in \mathbb{R}^{D_\ell}$, giving the
W-MSA output
\begin{equation}
    \mathbf{A}_{\ell,b}
    = \bigl[\mathbf{H}_{\ell,b,1} \|\cdots\| \mathbf{H}_{\ell,b,H_\ell}\bigr]
      \,W^O_{\ell,b} + \mathbf{b}^O_{\ell,b}
    \;\in\; \mathbb{R}^{N_w^\ell \cdot M \times D_\ell},
    \label{eq:wmsa_out}
\end{equation}
which is added to the residual stream in Eq.~\eqref{eq:residual_attn}.
For a complete step-by-step account of all tensor shapes across every stage,
see \url{https://github.com/lorenzo-arcioni/ResiDual-CLAP/blob/main/README.md}.

% ------------------------------------------------------------------
\subsection{Per-Head Residual Decomposition}
\label{app:residual}
% ------------------------------------------------------------------

The additive residual structure of Eq.~\eqref{eq:residual_attn} allows us
to decompose the W-MSA contribution $\mathbf{A}_{\ell,b}$ exactly into
independent per-head terms. Denoting by
$W^O_{\ell,b,h} \in \mathbb{R}^{d_h \times D_\ell}$ the row slice of
$W^O_{\ell,b}$ corresponding to head $h$ (rows $[(h-1)d_h,\; hd_h)$), we
distribute the output projection over heads:
\begin{align}
    \mathbf{A}_{\ell,b}
    &= \bigl[\mathbf{H}_{\ell,b,1} \|\cdots\| \mathbf{H}_{\ell,b,H_\ell}\bigr]
       W^O_{\ell,b} + \mathbf{b}^O_{\ell,b} \notag \\
    &= \sum_{h=1}^{H_\ell}
       \Bigl(\mathbf{H}_{\ell,b,h}\,W^O_{\ell,b,h}
       + \tfrac{\mathbf{b}^O_{\ell,b}}{H_\ell}\Bigr)
    \;=\; \sum_{h=1}^{H_\ell} \widehat{\mathbf{H}}_{\ell,b,h},
    \label{eq:head_decomp}
\end{align}
where we define the \emph{per-head projected contribution}
\begin{equation}
    \widehat{\mathbf{H}}_{\ell,b,h}
    \;=\; \mathbf{H}_{\ell,b,h}\,W^O_{\ell,b,h}
          + \frac{\mathbf{b}^O_{\ell,b}}{H_\ell}
    \;\in\; \mathbb{R}^{N_w^\ell \times M \times D_\ell}.
    \label{eq:hhat}
\end{equation}
This decomposition is exact and follows from the linearity of matrix
multiplication: because $W^O_{\ell,b}$ acts on the concatenation of all
head outputs, each head $h$ effectively multiplies only its own row slice
$W^O_{\ell,b,h}$, and the output bias can be distributed equally among
heads without any approximation.

The decomposition holds within a single stage, where $D_\ell$ is constant.
Across stage boundaries, PatchMerging changes both spatial resolution and
embedding dimension, breaking any global additive structure; cross-stage
comparisons must therefore account for this.

\begin{comment}
\paragraph{Spatial aggregation.}
For each audio sample, stage, block, and head, we define a spatially
aggregated scalar representation by averaging
$\widehat{\mathbf{H}}_{\ell,b,h}$ over all windows and token positions:
\begin{equation}
    \widehat{\mathbf{r}}_{\ell,b,h}
    = \frac{1}{N_w^\ell M}
      \sum_{i=1}^{N_w^\ell} \sum_{j=1}^{M}
      \widehat{\mathbf{H}}_{\ell,b,h}[i,j,:]
    \;\in\; \mathbb{R}^{D_\ell}.
    \label{eq:aggregation}
\end{equation}
For the pre-projection analysis (Appendix~\ref{app:preprojection}), the
analogous aggregation is applied to the raw head output
$\mathbf{H}_{\ell,b,h}$ before $W^O_{\ell,b}$:
\begin{equation}
    \mathbf{r}_{\ell,b,h}
    = \frac{1}{N_w^\ell M}
      \sum_{i=1}^{N_w^\ell} \sum_{j=1}^{M}
      \mathbf{H}_{\ell,b,h}[i,j,:]
    \;\in\; \mathbb{R}^{d_h}.
    \label{eq:spatial_pooling}
\end{equation}
Stacking these vectors across the $n$ audio samples in the dataset yields
the matrices $\widehat{\mathbf{R}}_{\ell,b,h} \in \mathbb{R}^{n \times D_\ell}$
and $\mathbf{R}_{\ell,b,h} \in \mathbb{R}^{n \times d_h}$, which are the
primary objects of our analysis. Because $D_\ell$ varies across stages, the
ambient dimension of $\widehat{\mathbf{R}}_{\ell,b,h}$ differs between
stages, and any cross-stage comparison of dimensionality metrics must account
for this varying ceiling.
\end{comment}

\begin{table*}[h]
\centering
\caption{Summary of notation used throughout the paper.}
\label{tab:notation}
\begin{tabularx}{\textwidth}{llX}
\toprule
\textbf{Symbol} & \textbf{Definition} & \textbf{Values / Notes} \\
\midrule
$\ell \in \{0,1,2,3\}$
    & Stage index & \\
$b \in \{1,\ldots,B_\ell\}$
    & Block index within stage $\ell$
    & $B_\ell \in \{2,2,6,2\}$ \\
$h \in \{1,\ldots,H_\ell\}$
    & Attention head index within block $(\ell,b)$
    & $H_\ell = 4 \cdot 2^\ell \in \{4,8,16,32\}$ \\
$S_\ell$
    & Spatial side length of the token grid at stage $\ell$
    & $S_\ell \in \{64, 32, 16, 8\}$ \\
$w = 8$
    & Attention window side length & \\
$M = w^2 = 64$
    & Tokens per attention window & \\
$N_w^\ell = S_\ell^2 / M$
    & Number of attention windows at stage $\ell$
    & $N_w^\ell \in \{64, 16, 4, 1\}$ \\
$d_h = 24$
    & Per-head feature dimension (constant across stages) & $d_h = D_\ell / H_\ell$ \\
$D_\ell = H_\ell \cdot d_h$
    & Total embedding dimension at stage $\ell$
    & $D_\ell \in \{96, 192, 384, 768\}$ \\
$d_{\mathrm{proj}} = 1024$
    & CLAP joint embedding dimension & \\
$W^{QKV}_{\ell,b} \in \mathbb{R}^{D_\ell \times 3D_\ell}$
    & Fused QKV projection at block $(\ell,b)$ & \\
$W^O_{\ell,b} \in \mathbb{R}^{D_\ell \times D_\ell}$
    & Output projection at block $(\ell,b)$ & \\
$W^O_{\ell,b,h} \in \mathbb{R}^{d_h \times D_\ell}$
    & Row slice of $W^O_{\ell,b}$ for head $h$
    & rows $[(h-1)d_h,\; hd_h)$ \\
$\mathbf{b}^O_{\ell,b} \in \mathbb{R}^{D_\ell}$
    & Output projection bias at block $(\ell,b)$ & \\
$\mathbf{B}_{\ell,b,h} \in \mathbb{R}^{M \times M}$
    & Relative position bias for head $h$ at block $(\ell,b)$ & \\
$\mathbf{H}_{\ell,b,h} \in \mathbb{R}^{N_w^\ell \times M \times d_h}$
    & Raw head output at block $(\ell,b)$, head $h$ (pre-$W^O_{\ell,b}$) & \\
$\widehat{\mathbf{H}}_{\ell,b,h} \in \mathbb{R}^{N_w^\ell \times M \times D_\ell}$
    & Per-head projected contribution (post-$W^O_{\ell,b}$); see Eq.~\eqref{eq:hhat} & \\
$\mathbf{r}_{\ell,b,h} \in \mathbb{R}^{d_h}$
    & Spatially aggregated raw head output; see Eq.~\eqref{eq:spatial_pooling} & \\
$\widehat{\mathbf{r}}_{\ell,b,h} \in \mathbb{R}^{D_\ell}$
    & Spatially aggregated projected head output; see Eq.~\eqref{eq:aggregation} & \\
$\mathbf{R}_{\ell,b,h} \in \mathbb{R}^{n \times d_h}$
    & Dataset matrix of $\mathbf{r}_{\ell,b,h}$ across $n$ samples & \\
$\widehat{\mathbf{R}}_{\ell,b,h} \in \mathbb{R}^{n \times D_\ell}$
    & Dataset matrix of $\widehat{\mathbf{r}}_{\ell,b,h}$ across $n$ samples & \\
$P: \mathbb{R}^{768} \to \mathbb{R}^{1024}$
    & CLAP audio projection head (two-layer MLP with residual) & \\
$n$
    & Number of audio samples in the dataset & \\
\bottomrule
\end{tabularx}
\end{table*}

% =============================================================
%  APPENDIX B — PRE-PROJECTION HEAD ANALYSIS
% =============================================================

\section{Pre-Projection Head Analysis}
\label{app:preprojection}

The main analysis operates on \emph{projected} head contributions
$\widehat{\mathbf{H}}_{\ell,b,h}$, which reflect each head's influence on the residual stream.
Here we describe a complementary analysis of \emph{raw pre-projection} outputs
$\mathbf{H}_{\ell,b,h}$, which characterise the intrinsic computational geometry of each head
independently of $W^O$.

\subsection{Motivation and Methodological Differences}

The raw head output $\mathbf{H}_{\ell,b,h} \in \mathbb{R}^{N_w \times M \times d_h}$ is produced
by the attention mechanism---specifically the weighted sum of value vectors---before any mixing
across heads. It lives in a constant $d_h = 24$-dimensional native space, offering two analytical
advantages:

\begin{enumerate}
    \item \textbf{Cross-stage comparability without projection.} $d_h = 24$ is identical for all
    184 heads regardless of stage, so dimensionality metrics are directly comparable without
    mapping to an external space. This isolates head-internal geometry from the CLAP projection.

    \item \textbf{Absence of $W^O$ distortion.} $W^O$ is a linear map that mixes head
    contributions and can rotate or rescale the geometry. The pre-projection space reflects what
    the attention mechanism \emph{computes}; the post-projection space reflects what it
    \emph{communicates} to the residual stream.
\end{enumerate}

\begin{table}[h]
\centering
\caption{Comparison of the two analysis pipelines.}
\label{tab:pipeline_comparison}
\begin{tabular}{lll}
\toprule
 & \textbf{Main (post-$W^O$)} & \textbf{Appendix (pre-$W^O$)} \\
\midrule
Representation & $\widehat{\mathbf{H}}_{\ell,b,h}$ & $\mathbf{H}_{\ell,b,h}$ \\
Ambient dim & $D_\ell \in \{96,192,384,768\}$ & $d_h = 24$ (constant) \\
Analysis space & $\mathbb{R}^{1024}$ (after $P$) & $\mathbb{R}^{24}$ (native) \\
Hook location & Before \texttt{.reshape}, then $\times W^O_h$ & Before \texttt{.reshape} only \\
$W^O$ applied & Yes (via $W^O_h$) & No \\
$P$ applied & Yes & No \\
Captures & Contribution to residual stream & Internal attention computation \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Extraction Procedure}

The same forward hooks are reused. Instead of applying $W^O_h$, we directly aggregate
$\mathbf{H}_{\ell,b,h}$ in the native $d_h$-dimensional space:
\begin{equation}
    \tilde{\mathbf{r}}_{\ell,b,h}
    = \frac{1}{N_w M}\sum_{i=1}^{N_w}\sum_{j=1}^{M}
      \mathbf{H}_{\ell,b,h}[i,j,:]
    \in \mathbb{R}^{24}.
    \label{eq:app_aggregation}
\end{equation}
For each head we collect $\widetilde{\mathbf{R}}_{\ell,b,h} \in \mathbb{R}^{n \times 24}$ and
apply the same estimators of Section~\ref{sec:dimensionality_method}, with 24 as the maximum
possible PCA dimension. No $W^O$ or CLAP projection is applied.

\subsection{Dimensionality Estimators in the Pre-Projection Space}

All estimators from Section~\ref{sec:dimensionality_method} apply with
$\widetilde{\mathbf{R}}_{\ell,b,h}$ replacing $\mathbf{R}_{\ell,b,h}$ and $d_h = 24$ as the
ambient dimension ceiling. Key consequences:

\paragraph{PCA.} The covariance $\widetilde{\mathbf{C}} \in \mathbb{R}^{24 \times 24}$ has at
most 24 non-zero eigenvalues. $d_{\mathrm{PCA}}(\alpha) \leq 24$ for all stages, making the
metric a direct measure of what fraction of the 24-dimensional native capacity each head exploits.

\paragraph{L/N Ratio.} With a fixed ambient dimension of 24, the ratio
$d_{\mathrm{PCA}_{99}} / d_{\mathrm{TwoNN}}$ isolates genuine manifold curvature from any ambient
dimension effect, providing a cleaner nonlinearity diagnostic than in the post-projection case.

\subsection{Interpretation and Relationship to Main Results}

A head with low intrinsic dimensionality in the pre-projection space computes a low-rank attention
pattern: the weighted combinations of value vectors collapse onto a small subspace of
$\mathbb{R}^{24}$. Comparing pre- and post-projection results reveals the role of $W^O$: if
dimensionality increases substantially after projection, $W^O$ expands the representational
geometry of that head in the residual stream; if it decreases, $W^O$ compresses or mixes it.

Concretely, pre-projection analysis is best suited to studying \emph{individual head
specialisation} in isolation; post-projection analysis---the perspective adopted in the main
body---is best suited to studying \emph{how heads collectively shape the residual stream} and,
ultimately, the CLAP embedding used for zero-shot classification.

% =============================================================
%  APPENDIX C — Extended Dimensionality Analysis
% =============================================================

\section{Extended Dimensionality Analysis}
\label{app:extended_analysis}

This appendix provides comprehensive quantitative details and additional visualizations complementing the main results in Section~\ref{sec:dimensionality_results}.

\subsection{Detailed Block-Level Statistics}
\label{app:block_stats}

Table~\ref{tab:block_detailed} reports complete block-wise metrics across all 12 transformer blocks in HTS-AT, aggregating over the heads within each block as described in Section~\ref{sec:dimensionality_method}.

\paragraph{Relationship to Architecture.}
As illustrated in Figure~\ref{fig:htsat_architecture}, the spatial resolution decreases progressively through the network due to patch merging between stages. While this affects the number of tokens $N$ processed by each attention head, our analysis focuses on the intrinsic dimensionality of the \emph{head dimension} $d_h = 24$ after spatial aggregation (Eq.~\ref{eq:spatial_pooling}). Thus, the reported metrics characterize the semantic complexity of head representations independent of spatial resolution effects.

The hierarchical structure creates natural breakpoints for dimensionality analysis:
\begin{itemize}
    \item \textbf{Stage 1 (Blocks 0--1)}: High spatial resolution $(T/2P \times F/2P)$ but limited capacity ($D_0 = 96$). Early fusion of local spectral-temporal patterns.
    \item \textbf{Stage 2 (Blocks 2--3)}: First dimensionality jump coincides with 2× patch merging and head doubling. Transition from local to intermediate-scale features.
    \item \textbf{Stage 3 (Blocks 4--9)}: Deepest stage with 6 blocks enables iterative refinement at fixed spatial scale $(T/4P \times F/4P)$ and capacity ($D_2 = 384$). Gradual dimensionality growth reflects progressive feature abstraction.
    \item \textbf{Stage 4 (Blocks 10--11)}: Maximum capacity ($D_3 = 768$) without further spatial reduction. Minimal dimensionality increase suggests saturation.
\end{itemize}

\begin{table}[h]
\centering
\caption{Block-wise aggregated dimensionality metrics for TinySOL dataset. Blocks 0--1 (Stage 1), 2--3 (Stage 2), 4--9 (Stage 3), 10--11 (Stage 4). L = Linear ID ($d_{\text{PCA}_{99}}$), N = Nonlinear ID (TwoNN), L/N = Linear-nonlinear ratio, EVR1 = First PC variance explained.}
\label{tab:block_detailed}
\small
\begin{tabular}{ccccccc}
\toprule
\textbf{Block} & \textbf{Stage} & \textbf{Heads} & \textbf{L} & \textbf{N} & \textbf{L/N} & \textbf{EVR1} \\
\midrule
0 & 1 & 4 & 3.75 & 3.94 & 0.95 & 0.878 \\
1 & 1 & 4 & 8.00 & 4.94 & 1.62 & 0.575 \\
2 & 2 & 8 & 12.75 & 6.15 & 2.07 & 0.403 \\
3 & 2 & 8 & 17.50 & 6.75 & 2.59 & 0.460 \\
4 & 3 & 16 & 18.00 & 6.93 & 2.60 & 0.388 \\
5 & 3 & 16 & 20.13 & 7.08 & 2.84 & 0.279 \\
6 & 3 & 16 & 21.25 & 7.40 & 2.87 & 0.250 \\
7 & 3 & 16 & 21.38 & 7.42 & 2.88 & 0.243 \\
8 & 3 & 16 & 22.25 & 8.37 & 2.66 & 0.217 \\
9 & 3 & 16 & 21.88 & 8.11 & 2.70 & 0.241 \\
10 & 4 & 32 & 22.25 & 8.49 & 2.62 & 0.262 \\
11 & 4 & 32 & 21.59 & 8.00 & 2.70 & 0.272 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Interpretation.}
\begin{itemize}
    \item Block 0 operates near the linear regime (L/N $\approx$ 1), with almost 88\% variance in the first PC, indicating extremely constrained early processing.
    \item The largest single-block jump occurs at the Stage 1→2 transition (blocks 1→2: $\Delta$L = +4.75, +59\%), corresponding to doubling of attention heads (4→8) and hidden dimension (96→192).
    \item Stage 3 exhibits gradual linear ID growth (18.00 → 22.25 over 6 blocks) despite constant architecture, suggesting intra-stage feature refinement through depth.
    \item Stage 4 shows minimal progression (blocks 10→11: $\Delta$L = -0.66), consistent with representational saturation observed in the main text.
\end{itemize}

\subsection{Extended Cross-Dataset Analysis}
\label{app:cross_dataset_extended}

We replicate the full layer-wise analysis on ESC-50 and VocalSound to validate architectural generalizability. Tables~\ref{tab:esc50_layers} and~\ref{tab:vocalsound_layers} present complete statistics.

\begin{table}[h]
\centering
\caption{Layer-wise dimensionality metrics for ESC-50 dataset (50 environmental sound classes, 1000 stratified samples).}
\label{tab:esc50_layers}
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Layer} & \textbf{$d_{\text{PCA}_{99}}$} & \textbf{TwoNN} & \textbf{PR} & \textbf{EVR1} \\
\midrule
L0 & $5.2 \pm 2.1$ & $4.2 \pm 0.7$ & $1.8 \pm 0.8$ & $0.741 \pm 0.187$ \\
L1 & $14.3 \pm 2.8$ & $6.2 \pm 0.6$ & $3.9 \pm 1.3$ & $0.449 \pm 0.109$ \\
L2 & $20.1 \pm 2.0$ & $7.4 \pm 0.8$ & $7.5 \pm 1.8$ & $0.281 \pm 0.081$ \\
L3 & $20.3 \pm 1.2$ & $7.8 \pm 1.1$ & $7.4 \pm 1.9$ & $0.279 \pm 0.074$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Layer-wise dimensionality metrics for VocalSound dataset (6 vocal sound categories, 1000 stratified samples).}
\label{tab:vocalsound_layers}
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Layer} & \textbf{$d_{\text{PCA}_{99}}$} & \textbf{TwoNN} & \textbf{PR} & \textbf{EVR1} \\
\midrule
L0 & $6.1 \pm 2.3$ & $4.6 \pm 0.8$ & $2.1 \pm 1.0$ & $0.698 \pm 0.192$ \\
L1 & $16.2 \pm 2.4$ & $6.7 \pm 0.5$ & $4.5 \pm 1.5$ & $0.421 \pm 0.117$ \\
L2 & $21.9 \pm 1.9$ & $8.0 \pm 0.9$ & $8.2 \pm 2.0$ & $0.263 \pm 0.079$ \\
L3 & $22.7 \pm 1.1$ & $8.6 \pm 0.8$ & $8.3 \pm 1.6$ & $0.254 \pm 0.071$ \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Cross-Dataset Consistency Analysis.}
Despite differing semantic granularities (ESC-50: 50 classes, VocalSound: 6 classes, TinySOL: 14 classes), layer-wise trends remain remarkably stable:
\begin{itemize}
    \item \textbf{L0 Concentration}: All datasets exhibit EVR1 > 69\% in Stage 1, confirming universal early spectral concentration.
    \item \textbf{L1 Expansion}: The L0→L1 dimensionality jump is consistent (TinySOL: +9.2, ESC-50: +9.1, VocalSound: +10.1 for $d_{\text{PCA}_{99}}$), with coefficient of variation across datasets CV = 0.06.
    \item \textbf{L2-L3 Saturation}: All datasets show similar modest L2→L3 growth ($\Delta d < 2.0$), despite L3 having 2× the heads of L2, indicating architecture-driven capacity limits.
    \item \textbf{L/N Ratio Convergence}: By Stage 4, all datasets reach L/N $\approx$ 2.6--2.7, suggesting a universal nonlinear complexity regime independent of semantic domain.
\end{itemize}

\subsection{Statistical Validation}
\label{app:statistical_validation}

\subsubsection{ANOVA Results}

One-way ANOVA tests for layer differences on TinySOL dataset:

\begin{table}[h]
\centering
\caption{Statistical significance of layer effects on dimensionality metrics (TinySOL, $n=184$ heads). All tests use $\alpha = 0.05$.}
\label{tab:anova_results}
\small
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{F-statistic} & \textbf{p-value} & \textbf{Significance} \\
\midrule
$d_{\text{PCA}_{99}}$ & 262.64 & $< 0.001$ & *** \\
TwoNN & 58.93 & $< 0.001$ & *** \\
PR & 44.74 & $< 0.001$ & *** \\
EffRank & 76.03 & $< 0.001$ & *** \\
EVR(PC1) & 118.47 & $< 0.001$ & *** \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Post-Hoc Pairwise Comparisons}

Bonferroni-corrected pairwise t-tests for $d_{\text{PCA}_{99}}$ (6 comparisons, $\alpha_{\text{corrected}} = 0.0083$):

\begin{table}[h]
\centering
\caption{Pairwise layer comparisons for linear intrinsic dimensionality (TinySOL). All comparisons significant at corrected $\alpha = 0.0083$.}
\label{tab:posthoc_pairwise}
\small
\begin{tabular}{lccc}
\toprule
\textbf{Comparison} & \textbf{$\Delta d_{\text{PCA}_{99}}$} & \textbf{Cohen's $d$} & \textbf{p-value} \\
\midrule
L0 vs L1 & 9.22 & 3.86 & $< 0.001$ \\
L0 vs L2 & 14.93 & 7.12 & $< 0.001$ \\
L0 vs L3 & 16.04 & 8.94 & $< 0.001$ \\
L1 vs L2 & 5.71 & 2.34 & $< 0.001$ \\
L1 vs L3 & 6.82 & 3.19 & $< 0.001$ \\
L2 vs L3 & 1.11 & 0.78 & $< 0.001$ \\
\bottomrule
\end{tabular}
\end{table}

All effect sizes exceed Cohen's threshold for "large" effects ($d > 0.8$), with the L0 vs L3 comparison exhibiting extremely large effects ($d > 8$), confirming substantial representational differences across layers.

\subsection{Additional Visualizations}
\label{app:additional_viz}

\subsubsection{PC1 Dominance and Boxplots}

Figure~\ref{fig:additional_panels} presents complementary views of dimensionality structure.

\begin{figure*}[t]
    \centering
    \subfloat[PC1 Variance Dominance]{\includegraphics[width=0.48\textwidth]{panel_E_pc1_dominance.png}}
    \hfill
    \subfloat[Multi-Metric Boxplots]{\includegraphics[width=0.48\textwidth]{panel_F_boxplot.png}}
    
    \caption{Extended dimensionality analysis. (a) First principal component variance explained across all 184 heads. Horizontal line at 50\% marks equal-contribution threshold. Sharp decline from L0 (mean 73\%) to L3 (mean 27\%) quantifies transition from low-rank to distributed representations. (b) Boxplot comparison of four key metrics across layers, revealing consistent monotonic trends and increasing intra-layer variance in deeper stages (note wider boxes for L2-L3).}
    \label{fig:additional_panels}
\end{figure*}

\paragraph{Observations from Panel (a):}
\begin{itemize}
    \item Only 2 heads in L0 (2.1\%) fall below the 50\% EVR1 threshold, compared to 89\% of L3 heads, demonstrating near-universal early concentration.
    \item The EVR1 distribution shifts from unimodal (L0: concentrated near 0.7--0.8) to bimodal (L3: peaks at 0.2--0.3 and 0.35--0.4), suggesting emergence of head subpopulations with distinct specialization levels.
\end{itemize}

\paragraph{Observations from Panel (b):}
\begin{itemize}
    \item PR and EffRank exhibit parallel scaling, confirming their measurement of related spectral properties.
    \item TwoNN shows compressed scale relative to PCA99, visually emphasizing the linear-nonlinear gap discussed in the main text.
    \item Outliers (marked as individual points beyond whiskers) are rare in L0-L1 but frequent in L2-L3, consistent with increased head-level heterogeneity.
\end{itemize}

\subsection{Computational Details}
\label{app:computational_details}

All analyses were performed on an NVIDIA A100 GPU (40GB) using PyTorch 2.0.1 and Python 3.10. Key implementation details:

\begin{itemize}
    \item \textbf{Head Extraction}: Forward hooks registered via \texttt{torch.nn.Module.register\_forward\_hook}. Batch size 100 for extraction to balance memory and throughput.
    \item \textbf{PCA}: Computed via \texttt{sklearn.decomposition.PCA} with full SVD solver. Eigenvalue thresholding at machine epsilon ($\sim 10^{-7}$) to remove numerical noise.
    \item \textbf{TwoNN}: Implemented using \texttt{skdim.id.TwoNN} with default parameters (no $k$ selection required).
    \item \textbf{MLE}: \texttt{skdim.id.MLE} with $k=20$ neighbors, standard Euclidean metric.
    \item \textbf{Statistical Tests}: \texttt{scipy.stats} functions (\texttt{f\_oneway}, \texttt{ttest\_ind}, \texttt{spearmanr}) with standard settings.
\end{itemize}

Total extraction time: $\sim$45 minutes per dataset (1000 samples × 184 heads). Analysis pipeline code available at \url{https://github.com/[ANONYMOUS]/residual-audio}.

\subsection{Reproducibility Checklist}
\label{app:reproducibility}

To facilitate replication:
\begin{itemize}
    \item Random seeds: 42 (Python), 42 (NumPy), 42 (PyTorch)
    \item CLAP version: \texttt{laion/clap-htsat-unfused} checkpoint from HuggingFace
    \item Audio preprocessing: CLAP default (64-band mel, 10s duration, 48kHz resampling)
    \item Dataset versions: ESC-50 v2.0, TinySOL v3.0, VocalSound official release
    \item Stratified sampling: \texttt{sklearn.model\_selection.StratifiedShuffleSplit} with 1000 samples
\end{itemize}
