\appendix

% =============================================================
%  APPENDIX A — CLAP/HTS-AT ARCHITECTURE AND NOTATION
% =============================================================

\section{CLAP and HTS-AT: Architecture, Notation, and Residual Decomposition}
\label{app:architecture}

\subsection{CLAP Overview}

CLAP (Contrastive Language--Audio Pretraining)~\cite{elizalde2023clap} is a dual-encoder model
that aligns audio and text in a shared $\mathbb{R}^{1024}$ embedding space via contrastive
learning. In our configuration, the audio encoder is HTS-AT and the text encoder is GPT-2~\cite{radford2019gpt2} with
embedding dimension 768, whose weights are frozen during training. Both encoders are independently
projected to the joint space of dimension $d_{\mathrm{proj}} = 1024$ via linear layers, and
similarity is measured by temperature-scaled cosine similarity (InfoNCE loss, temperature $\tau =
0.003$). Zero-shot audio classification is performed by comparing the audio embedding against
embeddings of textual class prompts.

\paragraph{Configuration.}
The CLAP instance analyzed in this work uses the following configuration:

\begin{table}[h]
\centering
\caption{CLAP configuration parameters.}
\label{tab:clap_config}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Text encoder & GPT-2 \\
Text encoder embedding dim & 768 \\
Audio encoder & HTS-AT \\
Audio encoder output dim ($D_3$) & 768 \\
Joint projection dim ($d_{\mathrm{proj}}$) & 1024 \\
Contrastive temperature $\tau$ & 0.003 \\
Sampling rate & 44{,}100\,Hz \\
Audio duration & 7\,s \\
Mel bands & 64 \\
FFT window & 1024 \\
Hop size & 320 \\
$f_{\min}$ / $f_{\max}$ & 50\,Hz / 8{,}000\,Hz \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Formal Notation}
In Table \ref{tab:notation}, we provide a summary of notation used in the paper.
\begin{table*}[h]
\centering
\caption{Summary of notation used throughout the paper.}
\label{tab:notation}
\begin{tabularx}{\textwidth}{llX}
\toprule
\textbf{Symbol} & \textbf{Definition} & \textbf{Values} \\
\midrule
$\ell = 0,1,2,3$ & Stage index & \\
$b \in \{1,\ldots,B_\ell\}$ & Block index within stage $\ell$ & $B_\ell = (2,2,6,2)$ \\
$h \in \{1,\ldots,H_\ell\}$ & Head index within a block & $H_\ell = 4 \cdot 2^\ell$ \\
$N_w^\ell = \frac{H_\ell \times W_\ell}{w^2} = \frac{H_\ell \times W_\ell}{64}$ & Number of attention windows at stage $\ell$ & \\
$M = w^2 = 64$ & Tokens per window & $w=8$ \\
$d_h = 24$ & Per-head dimension (constant across stages) & \\
$D_\ell = H_\ell \cdot d_h$ & Total embedding dimension at stage $\ell$ & \\
$d_{\mathrm{proj}} = 1024$ & CLAP joint embedding dimension & \\
$\mathbf{H}_{\ell,b,h} \in \mathbb{R}^{N_w^\ell \times M \times d_h}$ & Raw head output (pre-$W^O$) & \\
$W^O \in \mathbb{R}^{D_\ell \times D_\ell}$ & W-MSA output projection (\texttt{self.proj.weight}) & \\
$W^O_\ell \in \mathbb{R}^{D_\ell \times D_\ell}$ & W-MSA output projection at stage $\ell$ & \\
$W^O_{\ell,h} \in \mathbb{R}^{d_h \times D_\ell}$ & Row slice of $W^O_\ell$ for head $h$ & \\
$\widehat{\mathbf{H}}_{\ell,b,h} \in \mathbb{R}^{N_w^\ell \times M \times D_\ell}$ & Projected head contribution (post-$W^O$) & \\
$P: \mathbb{R}^{768} \to \mathbb{R}^{1024}$ & CLAP audio projection head (two-layer MLP with GELU) & \\
$\mathbf{r}_{\ell,b,h} \in \mathbb{R}^{d_h}$ & Spatially aggregated raw head output (pre-$W^O$) & \\
$\widehat{\mathbf{r}}_{\ell,b,h} \in \mathbb{R}^{D_\ell}$ & Spatially aggregated projected head output (post-$W^O$) & \\
$n$ & Number of audio samples & \\
$\mathbf{R}_{\ell,b,h} \in \mathbb{R}^{n \times d_h}$ & Matrix stacking all $\mathbf{r}_{\ell,b,h}$ & \\
$\widehat{\mathbf{R}}_{\ell,b,h} \in \mathbb{R}^{n \times D_\ell}$ & Matrix stacking all $\widehat{\mathbf{r}}_{\ell,b,h}$ & \\
\bottomrule
\end{tabularx}
\end{table*}

\subsection{HTS-AT Architecture}

HTS-AT~\cite{chen2022hts} is a Swin-Transformer variant adapted for audio spectrograms. It
organises computation into four hierarchical \emph{stages}, each composed of several
\emph{blocks}, as illustrated in Figure~\ref{fig:htsat_architecture}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{images/htsat_architecture.png}
    \caption{HTS-AT hierarchical architecture. The model consists of four basic layers (stages) with increasing complexity: Stage 0 (2 blocks × 4 heads), Stage 1 (2 blocks × 8 heads), Stage 2 (6 blocks × 16 heads), and Stage 3 (2 blocks × 32 heads). Patch merging between stages reduces spatial resolution while doubling the feature dimension. Input spectrogram dimensions are $T/P \times F/P = 64 \times 64 = 4096$ patches with $D=96$ channels. The final output has spatial size $(\frac{T}{8P}) \times (\frac{F}{8P}) = 64 \times 768$ before global pooling. Note that Stage 3 omits patch merging to preserve spatial resolution for fine-grained modeling.}
    \label{fig:htsat_architecture}
\end{figure*}

\paragraph{Input representation.}
Each audio clip is converted into a 64-band log-mel spectrogram
($f_{\min}=50$\,Hz, $f_{\max}=8{,}000$\,Hz, STFT window 1024, hop 320)
and normalised per mel-band. The spectrogram is then rearranged into a
square image $\mathbf{x}_{\mathrm{img}} \in \mathbb{R}^{1 \times 256 \times 256}$
by folding the time axis into 4 segments of 256 frames, stacked vertically
over the 64 mel bands ($4 \times 64 = 256$ rows, 256 columns).
This image is then divided into $4\times4$ non-overlapping patches via a
\texttt{Conv2d} layer (stride 4), producing a sequence of
$64 \times 64 = 4{,}096$ tokens each of dimension 96, followed by
\texttt{LayerNorm}. The resulting token sequence
$\mathbf{X}_{in} \in \mathbb{R}^{4096 \times 96}$ is the actual input
to the HTS-AT transformer stack.

\paragraph{Stage structure and patch merging.}
The four stages of HTS-AT have block depths $[2,2,6,2]$. Starting from
$\mathbf{X}_{in} \in \mathbb{R}^{4096 \times 96}$, each stage operates at a
progressively coarser spatial resolution, as summarised in
Table~\ref{tab:architecture}. Between stages 1--2 and 2--3, a \emph{PatchMerging} layer
concatenates each $2\times2$ neighbourhood of spatially adjacent tokens
into a single vector of dimension $4D_\ell$, then projects it down to
$2D_\ell$ via a linear layer, halving the token sequence length and
doubling the embedding dimension:
\begin{equation}
    \mathbb{R}^{\frac{H}{2^{\ell}} \cdot \frac{W}{2^{\ell}} \times D_\ell}
    \;\xrightarrow{\text{PatchMerging}}\;
    \mathbb{R}^{\frac{H}{2^{\ell+1}} \cdot \frac{W}{2^{\ell+1}} \times 2D_\ell},
\end{equation}
where $H = W = 64$ are the initial spatial dimensions of $\mathbf{X}_{in}$.
The last transition (Stage\,$3\to4$) omits patch merging, keeping the
sequence length fixed at 256 tokens while $D_3 = 768$
(see Table~\ref{tab:architecture}).

\begin{table}[H]
\centering
\setlength{\tabcolsep}{3pt}
\caption{HTS-AT stage-level architectural parameters. $d_h = D_\ell / H_\ell = 24$ is constant.}
\label{tab:architecture}
\begin{tabular}{lcccc}
\toprule
\textbf{Stage} $\ell$ & \textbf{Blocks} $B_\ell$ & \textbf{Heads} $H_\ell$ &
\textbf{Dim} $D_\ell$ & \textbf{Spatial res.} \\
\midrule
0 & 2 & 4  & 96  & $64 \times 64$ \\
1 & 2 & 8  & 192 & $32 \times 32$ \\
2 & 6 & 16 & 384 & $16 \times 16$ \\
3 & 2 & 32 & 768 & $16 \times 16$ \\
\midrule
\multicolumn{2}{l}{Total heads $H_{\mathrm{tot}}$} & \multicolumn{3}{l}{$2\cdot4 + 2\cdot8 + 6\cdot16 + 2\cdot32 = 184$} \\
\bottomrule
\end{tabular}
\end{table}

For a detailed view of the complete HTS-AT pipeline, see \url{https://github.com/lorenzo-arcioni/ResiDual-CLAP/blob/main/README.md}.

\paragraph{WindowAttention and output projection.}
Each \texttt{WindowAttention} module computes queries, keys, and values via a single fused
projection \texttt{self.qkv}: $\mathbb{R}^{D_\ell} \to \mathbb{R}^{3 D_\ell}$. The $H_\ell$
heads share this projection, each operating on a $d_h = 24$-dimensional slice. After computing
attention, all heads are concatenated and passed through \texttt{self.proj} ($W^O \in
\mathbb{R}^{D_\ell \times D_\ell}$). Relative position biases $\mathbf{B}_h$ are added to the
attention logits per head. The resulting per-block computation in code is:

\begin{verbatim}
qkv = self.qkv(x)
q, k, v = qkv.split(...)
attn = softmax(q @ k.T / sqrt(dh) + bias)
x = (attn @ v)
x = x.reshape(B*Nw, M, D)
x = self.proj(x)
\end{verbatim}

The forward hook for extracting $\widehat{\mathbf{H}}_{\ell,b,h}$ is
registered \emph{after} \texttt{self.proj} (post-$W^O$), as described
in the main analysis. 

For the pre-projection analysis (Appendix \ref{app:preprojection}), the hook
is instead registered \emph{before} \texttt{self.proj}, directly
capturing $\mathbf{H}_{\ell,b,h}$ (pre-$W^O$), which is then aggregated
in the native $d_h$-dimensional space to yield 

\begin{equation}
\mathbf{r}_{\ell,b,h} = \frac{1}{N_w^\ell M} \sum_{i=1}^{N_w^\ell} \sum_{j=1}^{M} \mathbf{H}_{\ell,b,h}[i, j, :] \in \mathbb{R}^{d_h}.
\label{eq:spatial_pooling_corrected}
\end{equation}

without applying $W^O$ or $P$.

\subsection{Residual Decomposition: Derivation}
The pre-norm architecture ensures that at each block $b$ of stage $\ell$,
attention and MLP sub-layers write directly to the residual stream
$\mathbf{Z}^{(\ell)}$:
\begin{align}
    \mathbf{Z}^{(\ell)} &\leftarrow \mathbf{Z}^{(\ell)} + 
                  \overbrace{\mathrm{W\text{-}MSA}(\mathrm{LN}(\mathbf{Z}^{(\ell)}))}^{\mathbf{A}_{\ell,b}}, \\
    \mathbf{Z}^{(\ell)} &\leftarrow \mathbf{Z}^{(\ell)} + 
                  \mathrm{MLP}(\mathrm{LN}(\mathbf{Z}^{(\ell)})).
\end{align}
The attention output $\mathbf{A}_{\ell,b}$ can be decomposed over heads by
distributing $W^O_\ell$ via its block-diagonal structure:
\begin{align}
    \mathbf{A}_{\ell,b}
    &= \mathrm{cat}(\mathbf{H}_{\ell,b,1},\ldots,\mathbf{H}_{\ell,b,H_\ell})\,W^O_\ell
    + \mathbf{b}^O_\ell \notag \\
    &= \sum_{h=1}^{H_\ell} \left(\mathbf{H}_{\ell,b,h}\,W^O_{\ell,h}
    + \frac{\mathbf{b}^O_\ell}{H_\ell}\right)
    = \sum_{h=1}^{H_\ell}\widehat{\mathbf{H}}_{\ell,b,h},
\end{align}
where $\widehat{\mathbf{H}}_{\ell,b,h} = \mathbf{H}_{\ell,b,h}\,W^O_{\ell,h} +
\mathbf{b}^O_\ell / H_\ell \in \mathbb{R}^{N_w^\ell \times M \times D_\ell}$
is the per-head projected contribution. This decomposition holds
\emph{within} each stage, where $D_\ell$ is constant. Across stage
boundaries, \emph{PatchMerging} changes both spatial resolution and
embedding dimension, breaking any global additive structure across stages.

\paragraph{Cross-stage analysis.}
Since $D_\ell \in \{96, 192, 384, 768\}$ varies across stages,
$\widehat{\mathbf{H}}_{\ell,b,h}$ from different stages live in spaces of
different ambient dimension and are not directly comparable. Our analysis
therefore operates on the spatially aggregated representations
$\widehat{\mathbf{r}}_{\ell,b,h} \in \mathbb{R}^{D_\ell}$ (see
Eq.~\eqref{eq:aggregation}), and cross-stage comparisons of dimensionality
metrics must account for this varying ambient ceiling.

% =============================================================
%  APPENDIX B — PRE-PROJECTION HEAD ANALYSIS
% =============================================================

\section{Pre-Projection Head Analysis}
\label{app:preprojection}

The main analysis operates on \emph{projected} head contributions
$\widehat{\mathbf{H}}_{\ell,b,h}$, which reflect each head's influence on the residual stream.
Here we describe a complementary analysis of \emph{raw pre-projection} outputs
$\mathbf{H}_{\ell,b,h}$, which characterise the intrinsic computational geometry of each head
independently of $W^O$.

\subsection{Motivation and Methodological Differences}

The raw head output $\mathbf{H}_{\ell,b,h} \in \mathbb{R}^{N_w \times M \times d_h}$ is produced
by the attention mechanism---specifically the weighted sum of value vectors---before any mixing
across heads. It lives in a constant $d_h = 24$-dimensional native space, offering two analytical
advantages:

\begin{enumerate}
    \item \textbf{Cross-stage comparability without projection.} $d_h = 24$ is identical for all
    184 heads regardless of stage, so dimensionality metrics are directly comparable without
    mapping to an external space. This isolates head-internal geometry from the CLAP projection.

    \item \textbf{Absence of $W^O$ distortion.} $W^O$ is a linear map that mixes head
    contributions and can rotate or rescale the geometry. The pre-projection space reflects what
    the attention mechanism \emph{computes}; the post-projection space reflects what it
    \emph{communicates} to the residual stream.
\end{enumerate}

\begin{table}[h]
\centering
\caption{Comparison of the two analysis pipelines.}
\label{tab:pipeline_comparison}
\begin{tabular}{lll}
\toprule
 & \textbf{Main (post-$W^O$)} & \textbf{Appendix (pre-$W^O$)} \\
\midrule
Representation & $\widehat{\mathbf{H}}_{\ell,b,h}$ & $\mathbf{H}_{\ell,b,h}$ \\
Ambient dim & $D_\ell \in \{96,192,384,768\}$ & $d_h = 24$ (constant) \\
Analysis space & $\mathbb{R}^{1024}$ (after $P$) & $\mathbb{R}^{24}$ (native) \\
Hook location & Before \texttt{.reshape}, then $\times W^O_h$ & Before \texttt{.reshape} only \\
$W^O$ applied & Yes (via $W^O_h$) & No \\
$P$ applied & Yes & No \\
Captures & Contribution to residual stream & Internal attention computation \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Extraction Procedure}

The same forward hooks are reused. Instead of applying $W^O_h$, we directly aggregate
$\mathbf{H}_{\ell,b,h}$ in the native $d_h$-dimensional space:
\begin{equation}
    \tilde{\mathbf{r}}_{\ell,b,h}
    = \frac{1}{N_w M}\sum_{i=1}^{N_w}\sum_{j=1}^{M}
      \mathbf{H}_{\ell,b,h}[i,j,:]
    \in \mathbb{R}^{24}.
    \label{eq:app_aggregation}
\end{equation}
For each head we collect $\widetilde{\mathbf{R}}_{\ell,b,h} \in \mathbb{R}^{n \times 24}$ and
apply the same estimators of Section~\ref{sec:dimensionality_method}, with 24 as the maximum
possible PCA dimension. No $W^O$ or CLAP projection is applied.

\subsection{Dimensionality Estimators in the Pre-Projection Space}

All estimators from Section~\ref{sec:dimensionality_method} apply with
$\widetilde{\mathbf{R}}_{\ell,b,h}$ replacing $\mathbf{R}_{\ell,b,h}$ and $d_h = 24$ as the
ambient dimension ceiling. Key consequences:

\paragraph{PCA.} The covariance $\widetilde{\mathbf{C}} \in \mathbb{R}^{24 \times 24}$ has at
most 24 non-zero eigenvalues. $d_{\mathrm{PCA}}(\alpha) \leq 24$ for all stages, making the
metric a direct measure of what fraction of the 24-dimensional native capacity each head exploits.

\paragraph{L/N Ratio.} With a fixed ambient dimension of 24, the ratio
$d_{\mathrm{PCA}_{99}} / d_{\mathrm{TwoNN}}$ isolates genuine manifold curvature from any ambient
dimension effect, providing a cleaner nonlinearity diagnostic than in the post-projection case.

\subsection{Interpretation and Relationship to Main Results}

A head with low intrinsic dimensionality in the pre-projection space computes a low-rank attention
pattern: the weighted combinations of value vectors collapse onto a small subspace of
$\mathbb{R}^{24}$. Comparing pre- and post-projection results reveals the role of $W^O$: if
dimensionality increases substantially after projection, $W^O$ expands the representational
geometry of that head in the residual stream; if it decreases, $W^O$ compresses or mixes it.

Concretely, pre-projection analysis is best suited to studying \emph{individual head
specialisation} in isolation; post-projection analysis---the perspective adopted in the main
body---is best suited to studying \emph{how heads collectively shape the residual stream} and,
ultimately, the CLAP embedding used for zero-shot classification.

% =============================================================
%  APPENDIX C — Extended Dimensionality Analysis
% =============================================================

\section{Extended Dimensionality Analysis}
\label{app:extended_analysis}

This appendix provides comprehensive quantitative details and additional visualizations complementing the main results in Section~\ref{sec:dimensionality_results}.

\subsection{Detailed Block-Level Statistics}
\label{app:block_stats}

Table~\ref{tab:block_detailed} reports complete block-wise metrics across all 12 transformer blocks in HTS-AT, aggregating over the heads within each block as described in Section~\ref{sec:dimensionality_method}.

\paragraph{Relationship to Architecture.}
As illustrated in Figure~\ref{fig:htsat_architecture}, the spatial resolution decreases progressively through the network due to patch merging between stages. While this affects the number of tokens $N$ processed by each attention head, our analysis focuses on the intrinsic dimensionality of the \emph{head dimension} $d_h = 24$ after spatial aggregation (Eq.~\ref{eq:spatial_pooling}). Thus, the reported metrics characterize the semantic complexity of head representations independent of spatial resolution effects.

The hierarchical structure creates natural breakpoints for dimensionality analysis:
\begin{itemize}
    \item \textbf{Stage 1 (Blocks 0--1)}: High spatial resolution $(T/2P \times F/2P)$ but limited capacity ($D_0 = 96$). Early fusion of local spectral-temporal patterns.
    \item \textbf{Stage 2 (Blocks 2--3)}: First dimensionality jump coincides with 2× patch merging and head doubling. Transition from local to intermediate-scale features.
    \item \textbf{Stage 3 (Blocks 4--9)}: Deepest stage with 6 blocks enables iterative refinement at fixed spatial scale $(T/4P \times F/4P)$ and capacity ($D_2 = 384$). Gradual dimensionality growth reflects progressive feature abstraction.
    \item \textbf{Stage 4 (Blocks 10--11)}: Maximum capacity ($D_3 = 768$) without further spatial reduction. Minimal dimensionality increase suggests saturation.
\end{itemize}

\begin{table}[h]
\centering
\caption{Block-wise aggregated dimensionality metrics for TinySOL dataset. Blocks 0--1 (Stage 1), 2--3 (Stage 2), 4--9 (Stage 3), 10--11 (Stage 4). L = Linear ID ($d_{\text{PCA}_{99}}$), N = Nonlinear ID (TwoNN), L/N = Linear-nonlinear ratio, EVR1 = First PC variance explained.}
\label{tab:block_detailed}
\small
\begin{tabular}{ccccccc}
\toprule
\textbf{Block} & \textbf{Stage} & \textbf{Heads} & \textbf{L} & \textbf{N} & \textbf{L/N} & \textbf{EVR1} \\
\midrule
0 & 1 & 4 & 3.75 & 3.94 & 0.95 & 0.878 \\
1 & 1 & 4 & 8.00 & 4.94 & 1.62 & 0.575 \\
2 & 2 & 8 & 12.75 & 6.15 & 2.07 & 0.403 \\
3 & 2 & 8 & 17.50 & 6.75 & 2.59 & 0.460 \\
4 & 3 & 16 & 18.00 & 6.93 & 2.60 & 0.388 \\
5 & 3 & 16 & 20.13 & 7.08 & 2.84 & 0.279 \\
6 & 3 & 16 & 21.25 & 7.40 & 2.87 & 0.250 \\
7 & 3 & 16 & 21.38 & 7.42 & 2.88 & 0.243 \\
8 & 3 & 16 & 22.25 & 8.37 & 2.66 & 0.217 \\
9 & 3 & 16 & 21.88 & 8.11 & 2.70 & 0.241 \\
10 & 4 & 32 & 22.25 & 8.49 & 2.62 & 0.262 \\
11 & 4 & 32 & 21.59 & 8.00 & 2.70 & 0.272 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Interpretation.}
\begin{itemize}
    \item Block 0 operates near the linear regime (L/N $\approx$ 1), with almost 88\% variance in the first PC, indicating extremely constrained early processing.
    \item The largest single-block jump occurs at the Stage 1→2 transition (blocks 1→2: $\Delta$L = +4.75, +59\%), corresponding to doubling of attention heads (4→8) and hidden dimension (96→192).
    \item Stage 3 exhibits gradual linear ID growth (18.00 → 22.25 over 6 blocks) despite constant architecture, suggesting intra-stage feature refinement through depth.
    \item Stage 4 shows minimal progression (blocks 10→11: $\Delta$L = -0.66), consistent with representational saturation observed in the main text.
\end{itemize}

\subsection{Extended Cross-Dataset Analysis}
\label{app:cross_dataset_extended}

We replicate the full layer-wise analysis on ESC-50 and VocalSound to validate architectural generalizability. Tables~\ref{tab:esc50_layers} and~\ref{tab:vocalsound_layers} present complete statistics.

\begin{table}[h]
\centering
\caption{Layer-wise dimensionality metrics for ESC-50 dataset (50 environmental sound classes, 1000 stratified samples).}
\label{tab:esc50_layers}
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Layer} & \textbf{$d_{\text{PCA}_{99}}$} & \textbf{TwoNN} & \textbf{PR} & \textbf{EVR1} \\
\midrule
L0 & $5.2 \pm 2.1$ & $4.2 \pm 0.7$ & $1.8 \pm 0.8$ & $0.741 \pm 0.187$ \\
L1 & $14.3 \pm 2.8$ & $6.2 \pm 0.6$ & $3.9 \pm 1.3$ & $0.449 \pm 0.109$ \\
L2 & $20.1 \pm 2.0$ & $7.4 \pm 0.8$ & $7.5 \pm 1.8$ & $0.281 \pm 0.081$ \\
L3 & $20.3 \pm 1.2$ & $7.8 \pm 1.1$ & $7.4 \pm 1.9$ & $0.279 \pm 0.074$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Layer-wise dimensionality metrics for VocalSound dataset (6 vocal sound categories, 1000 stratified samples).}
\label{tab:vocalsound_layers}
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Layer} & \textbf{$d_{\text{PCA}_{99}}$} & \textbf{TwoNN} & \textbf{PR} & \textbf{EVR1} \\
\midrule
L0 & $6.1 \pm 2.3$ & $4.6 \pm 0.8$ & $2.1 \pm 1.0$ & $0.698 \pm 0.192$ \\
L1 & $16.2 \pm 2.4$ & $6.7 \pm 0.5$ & $4.5 \pm 1.5$ & $0.421 \pm 0.117$ \\
L2 & $21.9 \pm 1.9$ & $8.0 \pm 0.9$ & $8.2 \pm 2.0$ & $0.263 \pm 0.079$ \\
L3 & $22.7 \pm 1.1$ & $8.6 \pm 0.8$ & $8.3 \pm 1.6$ & $0.254 \pm 0.071$ \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Cross-Dataset Consistency Analysis.}
Despite differing semantic granularities (ESC-50: 50 classes, VocalSound: 6 classes, TinySOL: 14 classes), layer-wise trends remain remarkably stable:
\begin{itemize}
    \item \textbf{L0 Concentration}: All datasets exhibit EVR1 > 69\% in Stage 1, confirming universal early spectral concentration.
    \item \textbf{L1 Expansion}: The L0→L1 dimensionality jump is consistent (TinySOL: +9.2, ESC-50: +9.1, VocalSound: +10.1 for $d_{\text{PCA}_{99}}$), with coefficient of variation across datasets CV = 0.06.
    \item \textbf{L2-L3 Saturation}: All datasets show similar modest L2→L3 growth ($\Delta d < 2.0$), despite L3 having 2× the heads of L2, indicating architecture-driven capacity limits.
    \item \textbf{L/N Ratio Convergence}: By Stage 4, all datasets reach L/N $\approx$ 2.6--2.7, suggesting a universal nonlinear complexity regime independent of semantic domain.
\end{itemize}

\subsection{Statistical Validation}
\label{app:statistical_validation}

\subsubsection{ANOVA Results}

One-way ANOVA tests for layer differences on TinySOL dataset:

\begin{table}[h]
\centering
\caption{Statistical significance of layer effects on dimensionality metrics (TinySOL, $n=184$ heads). All tests use $\alpha = 0.05$.}
\label{tab:anova_results}
\small
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{F-statistic} & \textbf{p-value} & \textbf{Significance} \\
\midrule
$d_{\text{PCA}_{99}}$ & 262.64 & $< 0.001$ & *** \\
TwoNN & 58.93 & $< 0.001$ & *** \\
PR & 44.74 & $< 0.001$ & *** \\
EffRank & 76.03 & $< 0.001$ & *** \\
EVR(PC1) & 118.47 & $< 0.001$ & *** \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Post-Hoc Pairwise Comparisons}

Bonferroni-corrected pairwise t-tests for $d_{\text{PCA}_{99}}$ (6 comparisons, $\alpha_{\text{corrected}} = 0.0083$):

\begin{table}[h]
\centering
\caption{Pairwise layer comparisons for linear intrinsic dimensionality (TinySOL). All comparisons significant at corrected $\alpha = 0.0083$.}
\label{tab:posthoc_pairwise}
\small
\begin{tabular}{lccc}
\toprule
\textbf{Comparison} & \textbf{$\Delta d_{\text{PCA}_{99}}$} & \textbf{Cohen's $d$} & \textbf{p-value} \\
\midrule
L0 vs L1 & 9.22 & 3.86 & $< 0.001$ \\
L0 vs L2 & 14.93 & 7.12 & $< 0.001$ \\
L0 vs L3 & 16.04 & 8.94 & $< 0.001$ \\
L1 vs L2 & 5.71 & 2.34 & $< 0.001$ \\
L1 vs L3 & 6.82 & 3.19 & $< 0.001$ \\
L2 vs L3 & 1.11 & 0.78 & $< 0.001$ \\
\bottomrule
\end{tabular}
\end{table}

All effect sizes exceed Cohen's threshold for "large" effects ($d > 0.8$), with the L0 vs L3 comparison exhibiting extremely large effects ($d > 8$), confirming substantial representational differences across layers.

\subsection{Additional Visualizations}
\label{app:additional_viz}

\subsubsection{PC1 Dominance and Boxplots}

Figure~\ref{fig:additional_panels} presents complementary views of dimensionality structure.

\begin{figure*}[t]
    \centering
    \subfloat[PC1 Variance Dominance]{\includegraphics[width=0.48\textwidth]{panel_E_pc1_dominance.png}}
    \hfill
    \subfloat[Multi-Metric Boxplots]{\includegraphics[width=0.48\textwidth]{panel_F_boxplot.png}}
    
    \caption{Extended dimensionality analysis. (a) First principal component variance explained across all 184 heads. Horizontal line at 50\% marks equal-contribution threshold. Sharp decline from L0 (mean 73\%) to L3 (mean 27\%) quantifies transition from low-rank to distributed representations. (b) Boxplot comparison of four key metrics across layers, revealing consistent monotonic trends and increasing intra-layer variance in deeper stages (note wider boxes for L2-L3).}
    \label{fig:additional_panels}
\end{figure*}

\paragraph{Observations from Panel (a):}
\begin{itemize}
    \item Only 2 heads in L0 (2.1\%) fall below the 50\% EVR1 threshold, compared to 89\% of L3 heads, demonstrating near-universal early concentration.
    \item The EVR1 distribution shifts from unimodal (L0: concentrated near 0.7--0.8) to bimodal (L3: peaks at 0.2--0.3 and 0.35--0.4), suggesting emergence of head subpopulations with distinct specialization levels.
\end{itemize}

\paragraph{Observations from Panel (b):}
\begin{itemize}
    \item PR and EffRank exhibit parallel scaling, confirming their measurement of related spectral properties.
    \item TwoNN shows compressed scale relative to PCA99, visually emphasizing the linear-nonlinear gap discussed in the main text.
    \item Outliers (marked as individual points beyond whiskers) are rare in L0-L1 but frequent in L2-L3, consistent with increased head-level heterogeneity.
\end{itemize}

\subsection{Computational Details}
\label{app:computational_details}

All analyses were performed on an NVIDIA A100 GPU (40GB) using PyTorch 2.0.1 and Python 3.10. Key implementation details:

\begin{itemize}
    \item \textbf{Head Extraction}: Forward hooks registered via \texttt{torch.nn.Module.register\_forward\_hook}. Batch size 100 for extraction to balance memory and throughput.
    \item \textbf{PCA}: Computed via \texttt{sklearn.decomposition.PCA} with full SVD solver. Eigenvalue thresholding at machine epsilon ($\sim 10^{-7}$) to remove numerical noise.
    \item \textbf{TwoNN}: Implemented using \texttt{skdim.id.TwoNN} with default parameters (no $k$ selection required).
    \item \textbf{MLE}: \texttt{skdim.id.MLE} with $k=20$ neighbors, standard Euclidean metric.
    \item \textbf{Statistical Tests}: \texttt{scipy.stats} functions (\texttt{f\_oneway}, \texttt{ttest\_ind}, \texttt{spearmanr}) with standard settings.
\end{itemize}

Total extraction time: $\sim$45 minutes per dataset (1000 samples × 184 heads). Analysis pipeline code available at \url{https://github.com/[ANONYMOUS]/residual-audio}.

\subsection{Reproducibility Checklist}
\label{app:reproducibility}

To facilitate replication:
\begin{itemize}
    \item Random seeds: 42 (Python), 42 (NumPy), 42 (PyTorch)
    \item CLAP version: \texttt{laion/clap-htsat-unfused} checkpoint from HuggingFace
    \item Audio preprocessing: CLAP default (64-band mel, 10s duration, 48kHz resampling)
    \item Dataset versions: ESC-50 v2.0, TinySOL v3.0, VocalSound official release
    \item Stratified sampling: \texttt{sklearn.model\_selection.StratifiedShuffleSplit} with 1000 samples
\end{itemize}
